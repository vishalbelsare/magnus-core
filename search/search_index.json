{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Magnus \u00b6 Lets do great things together!! Magnus provides four capabilities for data teams: Compute execution plan : A DAG representation of work that you want to get done. Individual nodes of the DAG could be simple python or shell tasks or complex deeply nested parallel branches or embedded DAGs themselves. Run log store : A place to store run logs for reporting or re-running older runs. Along with capturing the status of execution, the run logs also capture code identifiers (commits, docker image digests etc), data hashes and configuration settings for reproducibility and audit. Data Catalogs : A way to pass data between nodes of the graph during execution and also serves the purpose of versioning the data used by a particular run. Secrets : A framework to provide secrets/credentials at run time to the nodes of the graph. Design decisions: \u00b6 Easy to extend : All the four capabilities are just definitions and can be implemented in many flavors. Compute execution plan : You can choose to run the DAG on your local computer, in containers of local computer or off load the work to cloud providers or translate the DAG to AWS step functions or Argo workflows. Run log Store : The actual implementation of storing the run logs could be in-memory, file system, S3, database etc. Data Catalogs : The data files generated as part of a run could be stored on file-systems, S3 or could be extended to fit your needs. Secrets : The secrets needed for your code to work could be in dotenv, AWS or extended to fit your needs. Pipeline as contract : Once a DAG is defined and proven to work in local or some environment, there is absolutely no code change needed to deploy it to other environments. This enables the data teams to prove the correctness of the dag in dev environments while infrastructure teams to find the suitable way to deploy it. Reproducibility : Run log store and data catalogs hold the version, code commits, data files used for a run making it easy to re-run an older run or debug a failed run. Debug environment need not be the same as original environment. Easy switch : Your infrastructure landscape changes over time. With magnus, you can switch infrastructure by just changing a config and not code. Magnus does not aim to replace existing and well constructed orchestrators like AWS Step functions or argo but complements them in a unified, simple and intuitive way. Please find the github project here .","title":"Home"},{"location":"#welcome_to_magnus","text":"Lets do great things together!! Magnus provides four capabilities for data teams: Compute execution plan : A DAG representation of work that you want to get done. Individual nodes of the DAG could be simple python or shell tasks or complex deeply nested parallel branches or embedded DAGs themselves. Run log store : A place to store run logs for reporting or re-running older runs. Along with capturing the status of execution, the run logs also capture code identifiers (commits, docker image digests etc), data hashes and configuration settings for reproducibility and audit. Data Catalogs : A way to pass data between nodes of the graph during execution and also serves the purpose of versioning the data used by a particular run. Secrets : A framework to provide secrets/credentials at run time to the nodes of the graph.","title":"Welcome to Magnus"},{"location":"RELEASES/","text":"Release history \u00b6 v0.2.2 (2022-02-23) \u00b6 Enabling configurations to have placeholders that individual nodes can use to over-ride. Provided API in the executor to resolve the effective config by using global and local config v0.2.1 (2022-02-22) \u00b6 Updated docs to clarify the extension capabilities of the CLI and nodes Removed demo-renderer argument parsing block as parameters come from parameters v0.2 (2022-02-22) \u00b6 Moved magnus CLI to click. magnus command group can be extended to add more commands by other packages. Breaking changes: Contextualized config parameters for executors Parameters to be sent in via parameters file v0.1.2 (2022-02-08) \u00b6 Command config provided for all command types to pass in additional configuration. Moved to plugin based model using stevedore for nodes and tasks. Added support for notebooks as command types with optional install of papermill. v0.1.1 (2022-02-03) \u00b6 Bug fix with demo-renderer and as-is Moved to plugin based model using stevedore for executor, run log store, catalog, secrets and integrations v0.1.0 (2022-01-21) \u00b6 First release to open source. Compute: local, local-container, demo-renderer Run log store: local, buffered. Catalog: local, do-nothing. Secrets: dotenv, do-nothing.","title":"Releases"},{"location":"RELEASES/#release_history","text":"","title":"Release history"},{"location":"RELEASES/#v022_2022-02-23","text":"Enabling configurations to have placeholders that individual nodes can use to over-ride. Provided API in the executor to resolve the effective config by using global and local config","title":"v0.2.2 (2022-02-23)"},{"location":"RELEASES/#v021_2022-02-22","text":"Updated docs to clarify the extension capabilities of the CLI and nodes Removed demo-renderer argument parsing block as parameters come from parameters","title":"v0.2.1 (2022-02-22)"},{"location":"RELEASES/#v02_2022-02-22","text":"Moved magnus CLI to click. magnus command group can be extended to add more commands by other packages. Breaking changes: Contextualized config parameters for executors Parameters to be sent in via parameters file","title":"v0.2 (2022-02-22)"},{"location":"RELEASES/#v012_2022-02-08","text":"Command config provided for all command types to pass in additional configuration. Moved to plugin based model using stevedore for nodes and tasks. Added support for notebooks as command types with optional install of papermill.","title":"v0.1.2 (2022-02-08)"},{"location":"RELEASES/#v011_2022-02-03","text":"Bug fix with demo-renderer and as-is Moved to plugin based model using stevedore for executor, run log store, catalog, secrets and integrations","title":"v0.1.1 (2022-02-03)"},{"location":"RELEASES/#v010_2022-01-21","text":"First release to open source. Compute: local, local-container, demo-renderer Run log store: local, buffered. Catalog: local, do-nothing. Secrets: dotenv, do-nothing.","title":"v0.1.0 (2022-01-21)"},{"location":"command-line/","text":"Command line options \u00b6 You can execute a pipeline by the following command: magnus execute Note For the above command to work, make sure you are in the environment where magnus was installed. If you are using poetry, you can also invoke magnus by poetry run magnus execute Dag definition/config \u00b6 The file containing the dag definition and the config to be used. Provided by -f , --file option on magnus cli. Defaults to pipeline.yaml if nothing is provided. Variables file \u00b6 The yaml file containing the variables or placeholder values in the dag definition file. Provided by -v , --var-file option on magnus cli. Defaults to None, if nothing is provided. Read more about parameterized definitions . Configurations file \u00b6 The yaml file containing the configurations used to run magnus. The configurations provided here would over-ride any configuration variables. Provided by -c , --config-file option on magnus cli. Defaults to None, if nothing is provided. Read more about different ways you can configure magnus runs here. Changed in v0.2 Parameters file \u00b6 The yaml file containing the initial set of parameters that the application can access. Individual steps of the pipeline can still add/update parameters as required. Provided by -p , --parameters-file option to magnus cli. Defaults to None, if nothing is provided. You can also pass parameters by environmental variables prefixed by MAGNUS_PRM_ Log level \u00b6 To control the logging level of magnus only. This does not affect your application logs what so ever. Provided by --log-level option on magnus cli. Available options are: DEBUG, INFO, WARNING, ERROR, CRITICAL. Defaults to INFO if nothing is provided. Tag \u00b6 A friendly way to tag experiments or runs together. Provided by --tag option on magnus cli. Defaults to None if nothing is provided. Run id \u00b6 An unique run identifier for the run. Provided by --run-id on magnus cli. We generate one based on Timestamp if one is not provided. Use cached \u00b6 Enables you to re-run a previous run provided by the run-id. Example: magnus execute --file example.yaml --run-id 20210506051758 --use-cached old_run_id Extensions \u00b6 Magnus internally uses click to perform CLI operations and base command is given below. @with_plugins ( iter_entry_points ( 'magnus.cli_plugins' )) @click . group () @click . version_option () def cli (): \"\"\" Welcome to magnus. Please provide the command that you want to use. All commands have options that you can see by magnus <command> --help \"\"\" pass You can provide custom extensions to the command line capabilities by extending the namespace magnus.cli_plugins # For example, as part of your pyproject.toml [tool.poetry.plugins.\"magnus.cli_plugins\"] \"aws-ecr = \" YOUR_PACKAGE : push_to_ecr \" This extension than can be used as magnus aws-ecr <parameters and options>","title":"Options"},{"location":"command-line/#command_line_options","text":"You can execute a pipeline by the following command: magnus execute Note For the above command to work, make sure you are in the environment where magnus was installed. If you are using poetry, you can also invoke magnus by poetry run magnus execute","title":"Command line options"},{"location":"command-line/#dag_definitionconfig","text":"The file containing the dag definition and the config to be used. Provided by -f , --file option on magnus cli. Defaults to pipeline.yaml if nothing is provided.","title":"Dag definition/config"},{"location":"command-line/#variables_file","text":"The yaml file containing the variables or placeholder values in the dag definition file. Provided by -v , --var-file option on magnus cli. Defaults to None, if nothing is provided. Read more about parameterized definitions .","title":"Variables file"},{"location":"command-line/#configurations_file","text":"The yaml file containing the configurations used to run magnus. The configurations provided here would over-ride any configuration variables. Provided by -c , --config-file option on magnus cli. Defaults to None, if nothing is provided. Read more about different ways you can configure magnus runs here. Changed in v0.2","title":"Configurations file"},{"location":"command-line/#parameters_file","text":"The yaml file containing the initial set of parameters that the application can access. Individual steps of the pipeline can still add/update parameters as required. Provided by -p , --parameters-file option to magnus cli. Defaults to None, if nothing is provided. You can also pass parameters by environmental variables prefixed by MAGNUS_PRM_","title":"Parameters file"},{"location":"command-line/#log_level","text":"To control the logging level of magnus only. This does not affect your application logs what so ever. Provided by --log-level option on magnus cli. Available options are: DEBUG, INFO, WARNING, ERROR, CRITICAL. Defaults to INFO if nothing is provided.","title":"Log level"},{"location":"command-line/#tag","text":"A friendly way to tag experiments or runs together. Provided by --tag option on magnus cli. Defaults to None if nothing is provided.","title":"Tag"},{"location":"command-line/#run_id","text":"An unique run identifier for the run. Provided by --run-id on magnus cli. We generate one based on Timestamp if one is not provided.","title":"Run id"},{"location":"command-line/#use_cached","text":"Enables you to re-run a previous run provided by the run-id. Example: magnus execute --file example.yaml --run-id 20210506051758 --use-cached old_run_id","title":"Use cached"},{"location":"command-line/#extensions","text":"Magnus internally uses click to perform CLI operations and base command is given below. @with_plugins ( iter_entry_points ( 'magnus.cli_plugins' )) @click . group () @click . version_option () def cli (): \"\"\" Welcome to magnus. Please provide the command that you want to use. All commands have options that you can see by magnus <command> --help \"\"\" pass You can provide custom extensions to the command line capabilities by extending the namespace magnus.cli_plugins # For example, as part of your pyproject.toml [tool.poetry.plugins.\"magnus.cli_plugins\"] \"aws-ecr = \" YOUR_PACKAGE : push_to_ecr \" This extension than can be used as magnus aws-ecr <parameters and options>","title":"Extensions"},{"location":"examples/","text":"Examples \u00b6 A single node pipeline \u00b6 Every pipeline in magnus should have a success node and fail node. The starting node of the pipeline is denoted by start_at and every node needs to define the next node to traverse during successful execution of the current node using next . Nodes can optionally mention the node to traverse during failure using on_failure . Example: # In my_module.py def my_function (): print ( 'In the function, my_function of my_module' ) The pipeline which contains one node to call the above function. dag : description : A single node pipeline start_at : step 1 steps : step 1 : type : task next : success command : my_module.my_function command_type : python success : type : success failure : type : fail Mocking a node in pipeline \u00b6 In magnus, you can skip execution of a node or mock using a node of type as-is . This functionality is useful when you want to focus on designing the flow of code but not the specific implementation. Example: dag : description : A single node pipeline with mock start_at : step 1 steps : step 1 : type : as-is # The function would not execute as this is as-is node next : success command : my_module.my_function command_type : python success : type : success failure : type : fail Using shell commands as part of the pipeline \u00b6 In magnus, a pipeline can have shell commands as part of the pipeline. The only caveat in doing so is magnus would not be able to support returning parameters , secrets or any of the built-in functions. The cataloging functionality of magnus still would work via the configuration file. Parameters can be accessed by looking for environment variables with a prefix of MAGNUS_PRM_ . Example: Step 1 of the below pipeline would Get all the files from the catalog to the compute_data_folder . Execute the command python my_module.my_function in the shell. Put all the files from the compute_data_folder to the catalog. dag : description : A single node pipeline with shell start_at : step 1 steps : step 1 : type : task next : success command : python -m my_module.my_function # You can use this to call some executable in the PATH command_type : shell catalog : get : - \"*\" put : - \"*\" success : type : success failure : type : fail Using python lambda expressions in pipeline \u00b6 You can use python lambda expressions as a task type. Please note that you cannot have _ or __ as part of the expression. This is to prevent any malicious code to be passed into the expression. In the example below, step 1 takes in a parameter x and returns the integer x + 1 . Example: dag : description : A single node pipeline with python lambda start_at : step 1 steps : step 1 : command_type : python-lambda command : \"lambda x: {'x': int(x) + 1}\" next : success success : type : success failure : type : fail Using notebook in pipeline \u00b6 You can use notebooks as a command_type of a step in the pipeline. The only caveat in doing so is magnus would not be able to support returning parameters , secrets or any of the built-in functions. The cataloging functionality of magnus still would work via the configuration file. We use papermill to inspect the parameters and send them dynamically from the parameter space. The command refers to the notebook that you want to use as a task and it should point to the notebook. The output notebook naming could be provided by using the command_config section or would be defaulted to the notebook mentioned in command section post-fixed with _out . dag : description : A single node pipeline with notebook start_at : step 1 steps : step 1 : command_type : notebook command : pre_processing.iypnb next : success command_config : notebook_output_path : notebooks/output.ipynb success : type : success failure : type : fail A multi node pipeline \u00b6 A pipeline can have many nodes as part of its execution. Example: # In my_module.py def first_function (): print ( 'In the function, first_function of my_module' ) def second_function (): print ( 'In the function, second_function of my_module' ) The pipeline which calls first_function of the above module and then to the call the second_function is given below. dag : description : A multi node pipeline start_at : step 1 steps : step 1 : type : task next : step 2 command : my_module.first_function command_type : python step 2 : type : task next : success command : my_module.second_function command_type : python success : type : success failure : type : fail Using on-failure to handle errors \u00b6 You can instruct magnus to traverse to a different node of the dag if the current node fails to execute. A non-zero exit status of the python function or shell command is considered a failure. The default behavior in case of a failure of a node is, if no on_failure is defined, is to traverse to the fail node of the graph and mark the execution of the dag as failure. The execution of a dag is considered failure if and only if the fail node of the graph is reached. # In my_module.py def first_function (): print ( 'In the function, first_function of my_module' ) def second_function (): print ( 'In the function, second_function of my_module' ) def handle_error (): print ( 'Send an email notification' ) ## Some logic to send error notification ... The pipeline definition to call my_module.handle_error in case of a failure of any node is defined below. dag : description : A multi node pipeline with on_failure start_at : step 1 steps : step 1 : type : task next : step 2 command : my_module.first_function command_type : python on_failure : graceful exit step 2 : type : task next : success command : my_module.second_function command_type : python on_failure : graceful exit graceful exit : type : task next : fail command : my_module.handle_error command_type : python success : type : success failure : type : fail Passing parameters between nodes \u00b6 There are several ways we can pass parameters between nodes. Please note that this functionality is only for simple python data types which can be JSON serializable. Use the catalog functionality to pass files across to different nodes of the graph. You can choose any of the methods to pass the parameters from below. All are compatible with each other. The example pipeline to call all the below functions is given here: dag : description : A multi node pipeline to pass parameters start_at : step 1 steps : step 1 : type : task next : step 2 command : my_module.first_function command_type : python step 2 : type : task next : success command : my_module.second_function command_type : python success : type : success failure : type : fail Pythonically \u00b6 # In my_module.py def first_function (): print ( 'In the function, first_function of my_module' ) return { 'a' : 4 } def second_function ( a ): print ( 'In the function, second_function of my_module' ) print ( a ) In the above code, first_function is returning a dictionary setting a to be 4. If the function was called as a step in the magnus pipeline, magnus adds the key-value pair of a=4 to the parameter space. Note that first_function can return a dictionary containing as many key-value pairs as needed, magnus would add all of them to the parameter space. second_function is expecting a named argument a . If the function was called as a step in the magnus pipeline, magnus would look for a parameter a in the parameter space and assign it. Very loosely, the whole process can be thought of as: second_function(**first_function()) . Since magnus holds parameter space, the functions need not be consecutive and magnus handles the passing only the required arguments into the function. Using in-built functions \u00b6 You can also use the built-in functions that magnus provides to store and get parameters. # In my_module.py from magnus import store_parameter , get_parameter def first_function (): print ( 'In the function, first_function of my_module' ) store_parameter ( a = 4 ) def second_function (): print ( 'In the function, second_function of my_module' ) a = get_parameter ( 'a' ) # Get parameter with name provides only the named parameter. parameters = get_parameter () # Returns a dictionary of all the parameters print ( a ) # prints 4 print ( parameters ) # prints {'a': 4} Using environment variables \u00b6 The parameters can also be accessed by using environment variables. All magnus specific parameters would be prefixed by MAGNUS_PRM_ . Any environment variable that is prefixed by MAGNUS_PRM_ is also added to the parameter space. # In my_module.py import os def first_function (): print ( 'In the function, first_function of my_module' ) os . environ [ 'MAGNUS_PRM_a' ] = 4 def second_function (): print ( 'In the function, second_function of my_module' ) a = os . environ [ 'MAGNUS_PRM_a' ] print ( a ) Passing parameters to the first node of the pipeline \u00b6 There are several ways to set parameters at the start of the execution of the pipeline. Please choose one that fits your situation. During execution of pipeline by magnus \u00b6 The step step parameters of the below pipeline expects a parameter x in the lambda expression. # in getting-started.yaml dag : description : Getting started start_at : step parameters steps : step parameters : type : task command_type : python-lambda command : \"lambda x: {'x': int(x) + 1}\" next : success success : type : success fail : type : fail Changed in v0.2 You can pass the parameter during the execution of the run like below. magnus execute --file getting-started.yaml --parameters-file parameters.yaml # in parameters.yaml x : 3 Using environment variables \u00b6 For the same pipeline defined in getting-started.yaml , you can also pass the parameters as environment variables prefixed by MAGNUS_PRM_x . The below command does the same job of passing x as 3. MAGNUS_PRM_x = 3 ; magnus execute --file getting-started.yaml You can pass in as many parameters as you want by prefixing them with MAGNUS_PRM_ . All parameters would be read as string and have to casted appropriately by the code. This method of sending parameters by environmental variables is independent of who does the pipeline execution. Using the catalog to pass artifacts between nodes \u00b6 While parameters are used to transfer simple and JSON serializable data types, catalog can be used to make larger files or artifacts available to down stream nodes. A typical configuration of catalog provider would be: catalog : type : #defaults to file-system config : compute_data_folder : # defaults to data/ If no config is provided, magnus defaults to file-system . Logically magnus does the following: get files from the catalog before the execution to a specific compute data folder execute the command put any files from the compute data folder back to the catalog. Using the configuration. \u00b6 dag : description : Getting started start_at : step shell make data steps : step shell make data : type : task command_type : shell command : mkdir data ; env >> data/data.txt next : step shell ls data catalog : put : - \"*\" step shell ls data : type : task command_type : shell command : ls data/ next : success catalog : compute_data_folder : data/ # This is the default value too. get : - \"*\" success : type : success fail : type : fail In the above dag definition, step shell make data makes a data folder and dumps the environmental variables into data.txt file and instructs the catalog to put all (i.e '*') files into the catalog for downstream nodes. While the step step shell ls data instructs the catalog to get (i.e '*') files from the catalog and put them in compute_data_folder which is data and executes the command to see the contents of the directory. You can over-ride the compute_data_folder of a single step to any folder that you want as shown. Glob patterns are perfectly allowed and you can it to selectively get or put files in the catalog. Using the in-built functions \u00b6 You can interact with the catalog from the python code too if that is convenient. # In my_module.py from pathlib import Path from magnus import put_in_catalog , get_from_catalog def first_function (): print ( 'In the function, first_function of my_module' ) Path ( 'data' ) . mkdir ( parents = True , exist_ok = True ) with open ( 'data/data.txt' , 'w' ) as fw : fw . write ( 'something interesting) # filepath is required and can be a glob pattern put_in_catalog ( filepath = 'data/data.txt' ) def second_function (): print ( 'In the function, second_function of my_module' ) # name is required and can be a glob pattern. # destination_folder is defaulted to the compute_data_folder as defined in the config get_from_catalog ( name = 'data.txt' , destination_folder = 'data/' ) The python function first_function makes the compute_data_folder and instructs the catalog to put it the catalog. The python function second_function instructs the catalog to get the file by name data.txt from the catalog and put it in the folder data/ . You can use glob patterns both in put_in_catalog or get_from_catalog . The corresponding pipeline definition need not even aware of the cataloging happening by the functions. dag : description : A multi node pipeline start_at : step 1 steps : step 1 : type : task next : step 2 command : my_module.first_function command_type : python step 2 : type : task next : success command : my_module.second_function command_type : python success : type : success failure : type : fail Using the catalog to source external data \u00b6 In magnus, you can only get from catalog if the catalog location already exists. Calling put in catalog, which safely makes the catalog location if it does not exist, before you are trying to get from the catalog ensures that the catalog location is always present. But there are situations where you want to call get before you put data in the catalog location by the steps of the pipeline. For example, you want to source a data file generated by external processes and transform them in your pipeline. You can achieve that by the fact all catalog providers (eg. file-system and extensions) use run_id as the directory (or partition) of the catalog. To source data from external sources for a particular run, Create a run_id that you want to use for pipeline execution. Create the directory (or partition) in the catalog location by that run_id Copy the contents that you want the pipeline steps to access in the catalog location. Run the magnus pipeline by providing the run_id i.e magnus execute --run-id run_id --file <> Since the catalog location already exists, get from the catalog will source the external data. Accessing secrets within code. \u00b6 Secrets are the only service that magnus provides where you need to import magnus in your source code. This is to ensure that the integrity of the secrets are held and handled safely. A typical configuration of the secrets is: secrets : type : #defaults to do-nothing config : By default, magnus chooses a do-nothing secrets provider which holds no secrets. For local development, dotenv secrets manager is useful and the config is as below. secrets : type : dotenv config : location : # defaults to .env Example: #Inside .env file secret_name=secret_value#Any comment that you want to pass Any content after # is ignored and the format is key=value pairs. # In my_module.py from magnus import get_secret def first_function (): print ( 'In the function, first_function of my_module' ) secret_value = get_secret ( 'secret_name' ) print ( secret_value ) # Should print secret_value secrets = get_secret () print ( secrets ) # Should print {'secret_name': 'secret_value'} The pipeline to run the above function as a step of the pipeline. secrets : type : dotenv config : location : # defaults to .env dag : description : Demo of secrets start_at : step 1 steps : step 1 : type : task next : success command : my_module.first_function command_type : python success : type : success failure : type : fail Parallel node \u00b6 We will be using as-is nodes as part of the examples to keep it simple but the concepts of nesting/branching remain the same even in the case of actual tasks. Example of a parallel node: # in the yaml example-parallel.yaml run_log_store : type : file-system dag : description : DAG for testing with as-is and parallel start_at : step1 steps : step1 : type : as-is next : step2 step2 : type : parallel next : success branches : branch_1 : start_at : step_1 steps : step_1 : type : as-is next : success success : type : success fail : type : fail branch_2 : start_at : step_1 steps : step_1 : type : as-is next : success success : type : success fail : type : fail success : type : success fail : type : fail You can execute the above dag by: magnus execute --file example-parallel.yaml The above run should produce a run_log in the .run_log_store directory with the run_id as filename. The contents of the log should be similar to this: Click to show the run log { \"run_id\" : \"20220120131257\" , \"dag_hash\" : \"cf5cc7df88d4af3bc0936a9a8a3c4572ce4e11bc\" , \"use_cached\" : false , \"tag\" : null , \"original_run_id\" : \"\" , \"status\" : \"SUCCESS\" , \"steps\" : { \"step1\" : { \"name\" : \"step1\" , \"internal_name\" : \"step1\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"as-is\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : <INTENTIONALLY_REMOVED> , \"code_identifier_message\" : <INTENTIONALLY_REMOVED> } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 13:12:57.999265\" , \"end_time\" : \"2022-01-20 13:12:57.999287\" , \"duration\" : \"0:00:00.000022\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] }, \"step2\" : { \"name\" : \"step2\" , \"internal_name\" : \"step2\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"parallel\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : <INTENTIONALLY_REMOVED> , \"code_identifier_message\" : <INTENTIONALLY_REMOVED> } ], \"attempts\" : [], \"user_defined_metrics\" : {}, \"branches\" : { \"step2.branch_1\" : { \"internal_name\" : \"step2.branch_1\" , \"status\" : \"SUCCESS\" , \"steps\" : { \"step2.branch_1.step_1\" : { \"name\" : \"step_1\" , \"internal_name\" : \"step2.branch_1.step_1\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"as-is\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : <INTENTIONALLY_REMOVED> , \"code_identifier_message\" : <INTENTIONALLY_REMOVED> } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 13:12:58.090461\" , \"end_time\" : \"2022-01-20 13:12:58.090476\" , \"duration\" : \"0:00:00.000015\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] }, \"step2.branch_1.success\" : { \"name\" : \"success\" , \"internal_name\" : \"step2.branch_1.success\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"success\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : <INTENTIONALLY_REMOVED> , \"code_identifier_message\" : <INTENTIONALLY_REMOVED> } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 13:12:58.135551\" , \"end_time\" : \"2022-01-20 13:12:58.135732\" , \"duration\" : \"0:00:00.000181\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] } } }, \"step2.branch_2\" : { \"internal_name\" : \"step2.branch_2\" , \"status\" : \"SUCCESS\" , \"steps\" : { \"step2.branch_2.step_1\" : { \"name\" : \"step_1\" , \"internal_name\" : \"step2.branch_2.step_1\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"as-is\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : <INTENTIONALLY_REMOVED> , \"code_identifier_message\" : <INTENTIONALLY_REMOVED> } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 13:12:58.187648\" , \"end_time\" : \"2022-01-20 13:12:58.187661\" , \"duration\" : \"0:00:00.000013\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] }, \"step2.branch_2.success\" : { \"name\" : \"success\" , \"internal_name\" : \"step2.branch_2.success\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"success\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : <INTENTIONALLY_REMOVED> , \"code_identifier_message\" : <INTENTIONALLY_REMOVED> } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 13:12:58.233479\" , \"end_time\" : \"2022-01-20 13:12:58.233681\" , \"duration\" : \"0:00:00.000202\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] } } } }, \"data_catalog\" : [] }, \"success\" : { \"name\" : \"success\" , \"internal_name\" : \"success\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"success\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : <INTENTIONALLY_REMOVED> , \"code_identifier_message\" : <INTENTIONALLY_REMOVED> } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 13:12:58.280538\" , \"end_time\" : \"2022-01-20 13:12:58.280597\" , \"duration\" : \"0:00:00.000059\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] } }, \"parameters\" : {}, \"run_config\" : { \"executor\" : { \"type\" : \"local\" , \"config\" : {} }, \"run_log_store\" : { \"type\" : \"file-system\" , \"config\" : {} }, \"catalog\" : { \"type\" : \"file-system\" , \"config\" : {} }, \"secrets\" : { \"type\" : \"do-nothing\" , \"config\" : {} } } } The individual steps of the dag are named in dot path convention You can nest a parallel node, dag or a map node within parallel node to enable modular dag designs. Enabling parallel execution \u00b6 Though the dag definition defines a parallel node, the execution of the dag and the parallelism is actually controlled by the executor. In local mode, you can enable parallel branch execution by modifying the config. mode : type : local config : enable_parallel : \"true\" Points to note: The enable_parallel flag in the config is a string \"true\" Run log stores which use a single file as their log source (eg. file-system) cannot reliably run parallel executions as race conditions to modify the same file can happen leaving the run log in inconsistent state. The logs of the execution would also warn the same. Partitioned run log stores (eg. db) can be reliable run log stores. Embedding dag within dag \u00b6 You can embed dag's defined elsewhere into your dag. For example, we can define a dag which works all by itself in sub-dag.yaml # in sub-dag.yaml dag : description : sub dag start_at : step1 steps : step1 : type : as-is next : step2 step2 : type : as-is next : success success : type : success fail : type : fail We can embed this dag into another dag as a node like below. dag : description : DAG for nested dag start_at : step_dag_within_dag steps : step_dag_within_dag : type : dag dag_definition : sub-dag.yaml # Should be the filepath to the dag you want to embed. next : success success : type : success fail : type : fail Nested dag's should allow for a very modular design where individual dag's do well defined tasks but the nested dag can stitch them to complete the whole task. As with parallel execution, the individual steps of the dag are named in dot path convention Looping a branch over an iterable parameter \u00b6 Often, you would need to do the same repetitive tasks over a list and magnus allows you to do that. Example of dynamic branch looping is below. # in map-state.yaml dag : description : DAG for map start_at : step1 steps : step1 : type : task command : \"lambda : {'variables' : ['a', 'b', 'c']}\" command_type : python-lambda next : step2 step2 : type : map iterate_on : variables iterate_as : x next : success branch : start_at : step_1 steps : step_1 : type : task command : \"lambda x : {'state_' + str(x) : 5}\" command_type : python-lambda next : success success : type : success fail : type : fail success : type : success fail : type : fail In the above dag, step1 sets the parameters variables as list ['a', 'b', 'c'] . step2 is a node of type map which will iterate on variables and execute the branch defined as part of the definition of step2 for every value in the iterable variables . The branch definition of the step2 basically creates one more parameter state_<variable>=5 by the lambda expression. You can see these parameters as part of the run log show below. Click to show the run log { \"run_id\" : \"20220120150813\" , \"dag_hash\" : \"c0492a644b4f28f8441d669d9f0efb0f6d6be3d3\" , \"use_cached\" : false , \"tag\" : null , \"original_run_id\" : \"\" , \"status\" : \"SUCCESS\" , \"steps\" : { \"step1\" : { \"name\" : \"step1\" , \"internal_name\" : \"step1\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"task\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"INTENTIONALLY_REMOVED\" , \"code_identifier_message\" : \"INTENTIONALLY_REMOVED\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 15:08:14.069919\" , \"end_time\" : \"2022-01-20 15:08:14.070484\" , \"duration\" : \"0:00:00.000565\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] }, \"step2\" : { \"name\" : \"step2\" , \"internal_name\" : \"step2\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"map\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"INTENTIONALLY_REMOVED\" , \"code_identifier_message\" : \"INTENTIONALLY_REMOVED\" } ], \"attempts\" : [], \"user_defined_metrics\" : {}, \"branches\" : { \"step2.a\" : { \"internal_name\" : \"step2.a\" , \"status\" : \"SUCCESS\" , \"steps\" : { \"step2.a.step_1\" : { \"name\" : \"step_1\" , \"internal_name\" : \"step2.a.step_1\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"task\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"INTENTIONALLY_REMOVED\" , \"code_identifier_message\" : \"INTENTIONALLY_REMOVED\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 15:08:14.162440\" , \"end_time\" : \"2022-01-20 15:08:14.162882\" , \"duration\" : \"0:00:00.000442\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] }, \"step2.a.success\" : { \"name\" : \"success\" , \"internal_name\" : \"step2.a.success\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"success\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"INTENTIONALLY_REMOVED\" , \"code_identifier_message\" : \"INTENTIONALLY_REMOVED\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 15:08:14.209895\" , \"end_time\" : \"2022-01-20 15:08:14.210106\" , \"duration\" : \"0:00:00.000211\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] } } }, \"step2.b\" : { \"internal_name\" : \"step2.b\" , \"status\" : \"SUCCESS\" , \"steps\" : { \"step2.b.step_1\" : { \"name\" : \"step_1\" , \"internal_name\" : \"step2.b.step_1\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"task\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"INTENTIONALLY_REMOVED\" , \"code_identifier_message\" : \"INTENTIONALLY_REMOVED\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 15:08:14.258519\" , \"end_time\" : \"2022-01-20 15:08:14.258982\" , \"duration\" : \"0:00:00.000463\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] }, \"step2.b.success\" : { \"name\" : \"success\" , \"internal_name\" : \"step2.b.success\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"success\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"INTENTIONALLY_REMOVED\" , \"code_identifier_message\" : \"INTENTIONALLY_REMOVED\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 15:08:14.305524\" , \"end_time\" : \"2022-01-20 15:08:14.305754\" , \"duration\" : \"0:00:00.000230\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] } } }, \"step2.c\" : { \"internal_name\" : \"step2.c\" , \"status\" : \"SUCCESS\" , \"steps\" : { \"step2.c.step_1\" : { \"name\" : \"step_1\" , \"internal_name\" : \"step2.c.step_1\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"task\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"INTENTIONALLY_REMOVED\" , \"code_identifier_message\" : \"INTENTIONALLY_REMOVED\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 15:08:14.353182\" , \"end_time\" : \"2022-01-20 15:08:14.353603\" , \"duration\" : \"0:00:00.000421\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] }, \"step2.c.success\" : { \"name\" : \"success\" , \"internal_name\" : \"step2.c.success\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"success\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"INTENTIONALLY_REMOVED\" , \"code_identifier_message\" : \"INTENTIONALLY_REMOVED\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 15:08:14.401043\" , \"end_time\" : \"2022-01-20 15:08:14.401304\" , \"duration\" : \"0:00:00.000261\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] } } } }, \"data_catalog\" : [] }, \"success\" : { \"name\" : \"success\" , \"internal_name\" : \"success\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"success\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"INTENTIONALLY_REMOVED\" , \"code_identifier_message\" : ` \"INTENTIONALLY_REMOVED\" ` } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 15:08:14.449759\" , \"end_time\" : \"2022-01-20 15:08:14.449826\" , \"duration\" : \"0:00:00.000067\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] } }, \"parameters\" : { \"variables\" : [ \"a\" , \"b\" , \"c\" ], \"state_a\" : 5 , \"state_b\" : 5 , \"state_c\" : 5 }, \"run_config\" : { \"executor\" : { \"type\" : \"local\" , \"config\" : {} }, \"run_log_store\" : { \"type\" : \"buffered\" , \"config\" : {} }, \"catalog\" : { \"type\" : \"file-system\" , \"config\" : {} }, \"secrets\" : { \"type\" : \"do-nothing\" , \"config\" : {} } } } The individual steps of the dag are named in dot path convention . Enabling parallel execution \u00b6 Though the dag definition defines a map node where the branches can be executed in parallel, the execution of the dag and the parallelism is actually controlled by the executor. In local mode, you can enable parallel branch execution by modifying the config. mode : type : local config : enable_parallel : \"true\" Points to note: The enable_parallel flag in the config is a string \"true\" Run log stores which use a single file as their log source (eg. file-system) cannot reliably run parallel executions as race conditions to modify the same file can happen leaving the run log in inconsistent state. The logs of the execution would also warn the same. Partitioned run log stores (eg. db) can be reliable run log stores. Nesting and complex dags \u00b6 Magnus does not limit you at all in nesting at any level. You have construct deep nesting levels easily and magnus would execute them as you designed. As a general coding practice, having deeply nested branches could be hard to read and maintain. NOTE : There is a possibility that you can nest the same dag within the dag definition resulting in a infinite loop. We are actively finding ways to detect these situations and warn you. Advanced use as-is \u00b6 Node type as-is defined in magnus can be a very powerful tool in some deployment patterns. For example in the below dag definition, the step step echo does nothing as part of local execution. mode : type : demo-renderer run_log_store : type : file-system dag : description : Getting started start_at : step parameters steps : step parameters : type : task command_type : python-lambda command : \"lambda x: {'x': int(x) + 1}\" next : step shell step shell : type : task command_type : shell command : mkdir data ; env >> data/data.txt next : step echo catalog : put : - \"*\" step echo : type : as-is command_type : shell command_config : render_string : echo hello next : success success : type : success fail : type : fail But a deployment pattern, like demo-renderer , can use it to inject a command into the bash script. To test it out, uncomment the config to change to mode to demo-renderer and the run log store to be file-system and execute it like below. magnus execute --file getting-started.yaml should generate a bash script as show below in demo-bash.sh . for ARGUMENT in \" ${ @: 2 } \" do KEY = $( echo $ARGUMENT | cut -f1 -d = ) VALUE = $( echo $ARGUMENT | cut -f2 -d = ) export \"MAGNUS_PRM_ $KEY \" = $VALUE done magnus execute_single_node $1 step%parameters --file getting-started.yaml exit_code = $? echo $exit_code if [ $exit_code -ne 0 ] ; then $( magnus execute_single_node $1 fail --file getting-started.yaml ) exit 1 fi magnus execute_single_node $1 step%shell --file getting-started.yaml exit_code = $? echo $exit_code if [ $exit_code -ne 0 ] ; then $( magnus execute_single_node $1 fail --file getting-started.yaml ) exit 1 fi echo hello exit_code = $? echo $exit_code if [ $exit_code -ne 0 ] ; then $( magnus execute_single_node $1 fail --file getting-started.yaml ) exit 1 fi magnus execute_single_node $1 success --file getting-started.yaml The shell script is translation of the dag into a series of bash commands but notice the command echo hello as part of the script. While the local mode interpreted that node as a stub or a mock node, the demo-renderer mode used the render_string variable of the node config to inject a script. This feature is very useful when you want certain few steps (may be email notifications) to be only possible in production like environments but want to mock the during dev/experimental set up. NOTE : When trying to locally re-run a dag definition with as-is node used to inject scripts, the run would start from as-is step onwards independent of the source of failure. You can change this behavior by writing extensions which skip over as-is nodes during re-run. Controlling the log level of magnus \u00b6 The default log level of magnus is WARNING but you can change it at the point of execution to one of ['CRITICAL', 'ERROR', 'WARNING', 'INFO', 'DEBUG', 'NOTSET] by using the command line argument --log-level. For example: magnus execute --file <dag definition file> --log-level DEBUG would set the magnus log level to DEBUG. This setting only affects magnus logs and will not alter your application log levels. Order of configurations \u00b6 Magnus supports many ways of providing configurations but there is a order of preference. Magnus defaults to the following if no config is provided. mode : type : local config : enable_parallel : \"false\" run_log_store : type : buffered catalog : type : file-system config : compute_data_folder : data/ catalog_location : .catalog secrets : type : do-nothing But you can over-ride these defaults by providing a magnus-config.yaml in the source directory. For example, if the magnus-config.yaml file has the following contents, even if you do not provide a config in the dag definition file, these would taken as default service providers. mode : type : local config : enable_parallel : \"true\" # false is the default run_log_store : type : file-system catalog : type : file-system config : compute_data_folder : data/ # default catalog_location : .catalog # default secrets : type : dotenv config : location : .env # default You can over-ride the defaults either set by magnus or magnus-config.yaml by providing the config in the dag definition file. For example, in the dag definition below only the secrets providers config is over-ridden by the config present in the dag definition file. Compute mode, catalog and run log store configurations remain the same to defaults. secrets : type : do-nothing dag : description : Demo of secrets start_at : step 1 steps : step 1 : type : task next : success command : my_module.first_function command_type : python success : type : success failure : type : fail Finally, you can also over-ride the configurations set in the dag definition file by providing a custom configuration file containing only the configurations. For example, you can provide a dag definition file as above with do-nothing secrets handler but by providing the below configurations file at the run time, you can over-ride it to dotenv . #in prod-configuration.yaml secrets : type : dotenv The command to execute while providing the configuration file. magnus execute --file <dag definition file> --config-file prod-configuration.yaml The design thought is enable switching between different configurations by different actors involved in the data science workflow. The engineering team could provide magnus-config.yaml that should be default to the team or project for dev/experimental phase of the work but can over-ride the configuration during production deployment. Custom local extensions \u00b6 Magnus was built with extensions in mind. For example, there could be catalog extension using s3 or object storage that are generic enough to be open sourced back to the community. But there is always a chance where the extension is only specific to your team or project. You can implement custom extensions to either compute mode, run log store, catalog or secrets as part of your source folder and let magnus know to use them. For example, consider the use case of a custom secrets handler that only serves your team needs, called CustomSecrets which extends BaseSecrets provided by magnus like below. The secrets manager does nothing special and always returns 'always the same' as the secret value. # Present in the src.custom_secrets folder of your project from magnus.secrets import BaseSecrets class CustomSecrets ( BaseSecrets ): \"\"\" Does the same thing \"\"\" service_name = 'custom-secrets' def __init__ ( self , config , ** kwargs ): super () . __init__ ( config , ** kwargs ) self . secrets = {} def get ( self , name : str = None , ** kwargs ) -> Union [ str , dict ]: \"\"\" If a name is provided, return None else return empty dict. Args: name (str): The name of the secret to retrieve Raises: Exception: If the secret by the name is not found. Returns: [type]: [description] \"\"\" if name : return 'always the same' return { 'secret_key' : 'always the same' } You can instruct magnus to detect and use the CustomSecrets by providing it in the magnus-config.yaml like below. # in magnus-config.yaml extensions : - src.custom_secrets Magnus would import the contents of the module defined in extensions and would delegate the responsibility of secrets to CustomSecrets . We would love it if you share your custom extension code or the design aspect as it builds the community.","title":"Overview"},{"location":"examples/#examples","text":"","title":"Examples"},{"location":"examples/#a_single_node_pipeline","text":"Every pipeline in magnus should have a success node and fail node. The starting node of the pipeline is denoted by start_at and every node needs to define the next node to traverse during successful execution of the current node using next . Nodes can optionally mention the node to traverse during failure using on_failure . Example: # In my_module.py def my_function (): print ( 'In the function, my_function of my_module' ) The pipeline which contains one node to call the above function. dag : description : A single node pipeline start_at : step 1 steps : step 1 : type : task next : success command : my_module.my_function command_type : python success : type : success failure : type : fail","title":"A single node pipeline"},{"location":"examples/#mocking_a_node_in_pipeline","text":"In magnus, you can skip execution of a node or mock using a node of type as-is . This functionality is useful when you want to focus on designing the flow of code but not the specific implementation. Example: dag : description : A single node pipeline with mock start_at : step 1 steps : step 1 : type : as-is # The function would not execute as this is as-is node next : success command : my_module.my_function command_type : python success : type : success failure : type : fail","title":"Mocking a node in pipeline"},{"location":"examples/#using_shell_commands_as_part_of_the_pipeline","text":"In magnus, a pipeline can have shell commands as part of the pipeline. The only caveat in doing so is magnus would not be able to support returning parameters , secrets or any of the built-in functions. The cataloging functionality of magnus still would work via the configuration file. Parameters can be accessed by looking for environment variables with a prefix of MAGNUS_PRM_ . Example: Step 1 of the below pipeline would Get all the files from the catalog to the compute_data_folder . Execute the command python my_module.my_function in the shell. Put all the files from the compute_data_folder to the catalog. dag : description : A single node pipeline with shell start_at : step 1 steps : step 1 : type : task next : success command : python -m my_module.my_function # You can use this to call some executable in the PATH command_type : shell catalog : get : - \"*\" put : - \"*\" success : type : success failure : type : fail","title":"Using shell commands as part of the pipeline"},{"location":"examples/#using_python_lambda_expressions_in_pipeline","text":"You can use python lambda expressions as a task type. Please note that you cannot have _ or __ as part of the expression. This is to prevent any malicious code to be passed into the expression. In the example below, step 1 takes in a parameter x and returns the integer x + 1 . Example: dag : description : A single node pipeline with python lambda start_at : step 1 steps : step 1 : command_type : python-lambda command : \"lambda x: {'x': int(x) + 1}\" next : success success : type : success failure : type : fail","title":"Using python lambda expressions in pipeline"},{"location":"examples/#using_notebook_in_pipeline","text":"You can use notebooks as a command_type of a step in the pipeline. The only caveat in doing so is magnus would not be able to support returning parameters , secrets or any of the built-in functions. The cataloging functionality of magnus still would work via the configuration file. We use papermill to inspect the parameters and send them dynamically from the parameter space. The command refers to the notebook that you want to use as a task and it should point to the notebook. The output notebook naming could be provided by using the command_config section or would be defaulted to the notebook mentioned in command section post-fixed with _out . dag : description : A single node pipeline with notebook start_at : step 1 steps : step 1 : command_type : notebook command : pre_processing.iypnb next : success command_config : notebook_output_path : notebooks/output.ipynb success : type : success failure : type : fail","title":"Using notebook in pipeline"},{"location":"examples/#a_multi_node_pipeline","text":"A pipeline can have many nodes as part of its execution. Example: # In my_module.py def first_function (): print ( 'In the function, first_function of my_module' ) def second_function (): print ( 'In the function, second_function of my_module' ) The pipeline which calls first_function of the above module and then to the call the second_function is given below. dag : description : A multi node pipeline start_at : step 1 steps : step 1 : type : task next : step 2 command : my_module.first_function command_type : python step 2 : type : task next : success command : my_module.second_function command_type : python success : type : success failure : type : fail","title":"A multi node pipeline"},{"location":"examples/#using_on-failure_to_handle_errors","text":"You can instruct magnus to traverse to a different node of the dag if the current node fails to execute. A non-zero exit status of the python function or shell command is considered a failure. The default behavior in case of a failure of a node is, if no on_failure is defined, is to traverse to the fail node of the graph and mark the execution of the dag as failure. The execution of a dag is considered failure if and only if the fail node of the graph is reached. # In my_module.py def first_function (): print ( 'In the function, first_function of my_module' ) def second_function (): print ( 'In the function, second_function of my_module' ) def handle_error (): print ( 'Send an email notification' ) ## Some logic to send error notification ... The pipeline definition to call my_module.handle_error in case of a failure of any node is defined below. dag : description : A multi node pipeline with on_failure start_at : step 1 steps : step 1 : type : task next : step 2 command : my_module.first_function command_type : python on_failure : graceful exit step 2 : type : task next : success command : my_module.second_function command_type : python on_failure : graceful exit graceful exit : type : task next : fail command : my_module.handle_error command_type : python success : type : success failure : type : fail","title":"Using on-failure to handle errors"},{"location":"examples/#passing_parameters_between_nodes","text":"There are several ways we can pass parameters between nodes. Please note that this functionality is only for simple python data types which can be JSON serializable. Use the catalog functionality to pass files across to different nodes of the graph. You can choose any of the methods to pass the parameters from below. All are compatible with each other. The example pipeline to call all the below functions is given here: dag : description : A multi node pipeline to pass parameters start_at : step 1 steps : step 1 : type : task next : step 2 command : my_module.first_function command_type : python step 2 : type : task next : success command : my_module.second_function command_type : python success : type : success failure : type : fail","title":"Passing parameters between nodes"},{"location":"examples/#passing_parameters_to_the_first_node_of_the_pipeline","text":"There are several ways to set parameters at the start of the execution of the pipeline. Please choose one that fits your situation.","title":"Passing parameters to the first node of the pipeline"},{"location":"examples/#using_the_catalog_to_pass_artifacts_between_nodes","text":"While parameters are used to transfer simple and JSON serializable data types, catalog can be used to make larger files or artifacts available to down stream nodes. A typical configuration of catalog provider would be: catalog : type : #defaults to file-system config : compute_data_folder : # defaults to data/ If no config is provided, magnus defaults to file-system . Logically magnus does the following: get files from the catalog before the execution to a specific compute data folder execute the command put any files from the compute data folder back to the catalog.","title":"Using the catalog to pass artifacts between nodes"},{"location":"examples/#using_the_catalog_to_source_external_data","text":"In magnus, you can only get from catalog if the catalog location already exists. Calling put in catalog, which safely makes the catalog location if it does not exist, before you are trying to get from the catalog ensures that the catalog location is always present. But there are situations where you want to call get before you put data in the catalog location by the steps of the pipeline. For example, you want to source a data file generated by external processes and transform them in your pipeline. You can achieve that by the fact all catalog providers (eg. file-system and extensions) use run_id as the directory (or partition) of the catalog. To source data from external sources for a particular run, Create a run_id that you want to use for pipeline execution. Create the directory (or partition) in the catalog location by that run_id Copy the contents that you want the pipeline steps to access in the catalog location. Run the magnus pipeline by providing the run_id i.e magnus execute --run-id run_id --file <> Since the catalog location already exists, get from the catalog will source the external data.","title":"Using the catalog to source external data"},{"location":"examples/#accessing_secrets_within_code","text":"Secrets are the only service that magnus provides where you need to import magnus in your source code. This is to ensure that the integrity of the secrets are held and handled safely. A typical configuration of the secrets is: secrets : type : #defaults to do-nothing config : By default, magnus chooses a do-nothing secrets provider which holds no secrets. For local development, dotenv secrets manager is useful and the config is as below. secrets : type : dotenv config : location : # defaults to .env Example: #Inside .env file secret_name=secret_value#Any comment that you want to pass Any content after # is ignored and the format is key=value pairs. # In my_module.py from magnus import get_secret def first_function (): print ( 'In the function, first_function of my_module' ) secret_value = get_secret ( 'secret_name' ) print ( secret_value ) # Should print secret_value secrets = get_secret () print ( secrets ) # Should print {'secret_name': 'secret_value'} The pipeline to run the above function as a step of the pipeline. secrets : type : dotenv config : location : # defaults to .env dag : description : Demo of secrets start_at : step 1 steps : step 1 : type : task next : success command : my_module.first_function command_type : python success : type : success failure : type : fail","title":"Accessing secrets within code."},{"location":"examples/#parallel_node","text":"We will be using as-is nodes as part of the examples to keep it simple but the concepts of nesting/branching remain the same even in the case of actual tasks. Example of a parallel node: # in the yaml example-parallel.yaml run_log_store : type : file-system dag : description : DAG for testing with as-is and parallel start_at : step1 steps : step1 : type : as-is next : step2 step2 : type : parallel next : success branches : branch_1 : start_at : step_1 steps : step_1 : type : as-is next : success success : type : success fail : type : fail branch_2 : start_at : step_1 steps : step_1 : type : as-is next : success success : type : success fail : type : fail success : type : success fail : type : fail You can execute the above dag by: magnus execute --file example-parallel.yaml The above run should produce a run_log in the .run_log_store directory with the run_id as filename. The contents of the log should be similar to this: Click to show the run log { \"run_id\" : \"20220120131257\" , \"dag_hash\" : \"cf5cc7df88d4af3bc0936a9a8a3c4572ce4e11bc\" , \"use_cached\" : false , \"tag\" : null , \"original_run_id\" : \"\" , \"status\" : \"SUCCESS\" , \"steps\" : { \"step1\" : { \"name\" : \"step1\" , \"internal_name\" : \"step1\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"as-is\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : <INTENTIONALLY_REMOVED> , \"code_identifier_message\" : <INTENTIONALLY_REMOVED> } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 13:12:57.999265\" , \"end_time\" : \"2022-01-20 13:12:57.999287\" , \"duration\" : \"0:00:00.000022\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] }, \"step2\" : { \"name\" : \"step2\" , \"internal_name\" : \"step2\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"parallel\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : <INTENTIONALLY_REMOVED> , \"code_identifier_message\" : <INTENTIONALLY_REMOVED> } ], \"attempts\" : [], \"user_defined_metrics\" : {}, \"branches\" : { \"step2.branch_1\" : { \"internal_name\" : \"step2.branch_1\" , \"status\" : \"SUCCESS\" , \"steps\" : { \"step2.branch_1.step_1\" : { \"name\" : \"step_1\" , \"internal_name\" : \"step2.branch_1.step_1\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"as-is\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : <INTENTIONALLY_REMOVED> , \"code_identifier_message\" : <INTENTIONALLY_REMOVED> } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 13:12:58.090461\" , \"end_time\" : \"2022-01-20 13:12:58.090476\" , \"duration\" : \"0:00:00.000015\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] }, \"step2.branch_1.success\" : { \"name\" : \"success\" , \"internal_name\" : \"step2.branch_1.success\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"success\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : <INTENTIONALLY_REMOVED> , \"code_identifier_message\" : <INTENTIONALLY_REMOVED> } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 13:12:58.135551\" , \"end_time\" : \"2022-01-20 13:12:58.135732\" , \"duration\" : \"0:00:00.000181\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] } } }, \"step2.branch_2\" : { \"internal_name\" : \"step2.branch_2\" , \"status\" : \"SUCCESS\" , \"steps\" : { \"step2.branch_2.step_1\" : { \"name\" : \"step_1\" , \"internal_name\" : \"step2.branch_2.step_1\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"as-is\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : <INTENTIONALLY_REMOVED> , \"code_identifier_message\" : <INTENTIONALLY_REMOVED> } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 13:12:58.187648\" , \"end_time\" : \"2022-01-20 13:12:58.187661\" , \"duration\" : \"0:00:00.000013\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] }, \"step2.branch_2.success\" : { \"name\" : \"success\" , \"internal_name\" : \"step2.branch_2.success\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"success\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : <INTENTIONALLY_REMOVED> , \"code_identifier_message\" : <INTENTIONALLY_REMOVED> } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 13:12:58.233479\" , \"end_time\" : \"2022-01-20 13:12:58.233681\" , \"duration\" : \"0:00:00.000202\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] } } } }, \"data_catalog\" : [] }, \"success\" : { \"name\" : \"success\" , \"internal_name\" : \"success\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"success\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : <INTENTIONALLY_REMOVED> , \"code_identifier_message\" : <INTENTIONALLY_REMOVED> } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 13:12:58.280538\" , \"end_time\" : \"2022-01-20 13:12:58.280597\" , \"duration\" : \"0:00:00.000059\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] } }, \"parameters\" : {}, \"run_config\" : { \"executor\" : { \"type\" : \"local\" , \"config\" : {} }, \"run_log_store\" : { \"type\" : \"file-system\" , \"config\" : {} }, \"catalog\" : { \"type\" : \"file-system\" , \"config\" : {} }, \"secrets\" : { \"type\" : \"do-nothing\" , \"config\" : {} } } } The individual steps of the dag are named in dot path convention You can nest a parallel node, dag or a map node within parallel node to enable modular dag designs.","title":"Parallel node"},{"location":"examples/#embedding_dag_within_dag","text":"You can embed dag's defined elsewhere into your dag. For example, we can define a dag which works all by itself in sub-dag.yaml # in sub-dag.yaml dag : description : sub dag start_at : step1 steps : step1 : type : as-is next : step2 step2 : type : as-is next : success success : type : success fail : type : fail We can embed this dag into another dag as a node like below. dag : description : DAG for nested dag start_at : step_dag_within_dag steps : step_dag_within_dag : type : dag dag_definition : sub-dag.yaml # Should be the filepath to the dag you want to embed. next : success success : type : success fail : type : fail Nested dag's should allow for a very modular design where individual dag's do well defined tasks but the nested dag can stitch them to complete the whole task. As with parallel execution, the individual steps of the dag are named in dot path convention","title":"Embedding dag within dag"},{"location":"examples/#looping_a_branch_over_an_iterable_parameter","text":"Often, you would need to do the same repetitive tasks over a list and magnus allows you to do that. Example of dynamic branch looping is below. # in map-state.yaml dag : description : DAG for map start_at : step1 steps : step1 : type : task command : \"lambda : {'variables' : ['a', 'b', 'c']}\" command_type : python-lambda next : step2 step2 : type : map iterate_on : variables iterate_as : x next : success branch : start_at : step_1 steps : step_1 : type : task command : \"lambda x : {'state_' + str(x) : 5}\" command_type : python-lambda next : success success : type : success fail : type : fail success : type : success fail : type : fail In the above dag, step1 sets the parameters variables as list ['a', 'b', 'c'] . step2 is a node of type map which will iterate on variables and execute the branch defined as part of the definition of step2 for every value in the iterable variables . The branch definition of the step2 basically creates one more parameter state_<variable>=5 by the lambda expression. You can see these parameters as part of the run log show below. Click to show the run log { \"run_id\" : \"20220120150813\" , \"dag_hash\" : \"c0492a644b4f28f8441d669d9f0efb0f6d6be3d3\" , \"use_cached\" : false , \"tag\" : null , \"original_run_id\" : \"\" , \"status\" : \"SUCCESS\" , \"steps\" : { \"step1\" : { \"name\" : \"step1\" , \"internal_name\" : \"step1\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"task\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"INTENTIONALLY_REMOVED\" , \"code_identifier_message\" : \"INTENTIONALLY_REMOVED\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 15:08:14.069919\" , \"end_time\" : \"2022-01-20 15:08:14.070484\" , \"duration\" : \"0:00:00.000565\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] }, \"step2\" : { \"name\" : \"step2\" , \"internal_name\" : \"step2\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"map\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"INTENTIONALLY_REMOVED\" , \"code_identifier_message\" : \"INTENTIONALLY_REMOVED\" } ], \"attempts\" : [], \"user_defined_metrics\" : {}, \"branches\" : { \"step2.a\" : { \"internal_name\" : \"step2.a\" , \"status\" : \"SUCCESS\" , \"steps\" : { \"step2.a.step_1\" : { \"name\" : \"step_1\" , \"internal_name\" : \"step2.a.step_1\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"task\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"INTENTIONALLY_REMOVED\" , \"code_identifier_message\" : \"INTENTIONALLY_REMOVED\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 15:08:14.162440\" , \"end_time\" : \"2022-01-20 15:08:14.162882\" , \"duration\" : \"0:00:00.000442\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] }, \"step2.a.success\" : { \"name\" : \"success\" , \"internal_name\" : \"step2.a.success\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"success\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"INTENTIONALLY_REMOVED\" , \"code_identifier_message\" : \"INTENTIONALLY_REMOVED\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 15:08:14.209895\" , \"end_time\" : \"2022-01-20 15:08:14.210106\" , \"duration\" : \"0:00:00.000211\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] } } }, \"step2.b\" : { \"internal_name\" : \"step2.b\" , \"status\" : \"SUCCESS\" , \"steps\" : { \"step2.b.step_1\" : { \"name\" : \"step_1\" , \"internal_name\" : \"step2.b.step_1\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"task\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"INTENTIONALLY_REMOVED\" , \"code_identifier_message\" : \"INTENTIONALLY_REMOVED\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 15:08:14.258519\" , \"end_time\" : \"2022-01-20 15:08:14.258982\" , \"duration\" : \"0:00:00.000463\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] }, \"step2.b.success\" : { \"name\" : \"success\" , \"internal_name\" : \"step2.b.success\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"success\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"INTENTIONALLY_REMOVED\" , \"code_identifier_message\" : \"INTENTIONALLY_REMOVED\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 15:08:14.305524\" , \"end_time\" : \"2022-01-20 15:08:14.305754\" , \"duration\" : \"0:00:00.000230\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] } } }, \"step2.c\" : { \"internal_name\" : \"step2.c\" , \"status\" : \"SUCCESS\" , \"steps\" : { \"step2.c.step_1\" : { \"name\" : \"step_1\" , \"internal_name\" : \"step2.c.step_1\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"task\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"INTENTIONALLY_REMOVED\" , \"code_identifier_message\" : \"INTENTIONALLY_REMOVED\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 15:08:14.353182\" , \"end_time\" : \"2022-01-20 15:08:14.353603\" , \"duration\" : \"0:00:00.000421\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] }, \"step2.c.success\" : { \"name\" : \"success\" , \"internal_name\" : \"step2.c.success\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"success\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"INTENTIONALLY_REMOVED\" , \"code_identifier_message\" : \"INTENTIONALLY_REMOVED\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 15:08:14.401043\" , \"end_time\" : \"2022-01-20 15:08:14.401304\" , \"duration\" : \"0:00:00.000261\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] } } } }, \"data_catalog\" : [] }, \"success\" : { \"name\" : \"success\" , \"internal_name\" : \"success\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"success\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"2a5b33bdf60c4f0d38cae04ab3f988b3d1c6ed59\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"INTENTIONALLY_REMOVED\" , \"code_identifier_message\" : ` \"INTENTIONALLY_REMOVED\" ` } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-20 15:08:14.449759\" , \"end_time\" : \"2022-01-20 15:08:14.449826\" , \"duration\" : \"0:00:00.000067\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] } }, \"parameters\" : { \"variables\" : [ \"a\" , \"b\" , \"c\" ], \"state_a\" : 5 , \"state_b\" : 5 , \"state_c\" : 5 }, \"run_config\" : { \"executor\" : { \"type\" : \"local\" , \"config\" : {} }, \"run_log_store\" : { \"type\" : \"buffered\" , \"config\" : {} }, \"catalog\" : { \"type\" : \"file-system\" , \"config\" : {} }, \"secrets\" : { \"type\" : \"do-nothing\" , \"config\" : {} } } } The individual steps of the dag are named in dot path convention .","title":"Looping a branch over an iterable parameter"},{"location":"examples/#nesting_and_complex_dags","text":"Magnus does not limit you at all in nesting at any level. You have construct deep nesting levels easily and magnus would execute them as you designed. As a general coding practice, having deeply nested branches could be hard to read and maintain. NOTE : There is a possibility that you can nest the same dag within the dag definition resulting in a infinite loop. We are actively finding ways to detect these situations and warn you.","title":"Nesting and complex dags"},{"location":"examples/#advanced_use_as-is","text":"Node type as-is defined in magnus can be a very powerful tool in some deployment patterns. For example in the below dag definition, the step step echo does nothing as part of local execution. mode : type : demo-renderer run_log_store : type : file-system dag : description : Getting started start_at : step parameters steps : step parameters : type : task command_type : python-lambda command : \"lambda x: {'x': int(x) + 1}\" next : step shell step shell : type : task command_type : shell command : mkdir data ; env >> data/data.txt next : step echo catalog : put : - \"*\" step echo : type : as-is command_type : shell command_config : render_string : echo hello next : success success : type : success fail : type : fail But a deployment pattern, like demo-renderer , can use it to inject a command into the bash script. To test it out, uncomment the config to change to mode to demo-renderer and the run log store to be file-system and execute it like below. magnus execute --file getting-started.yaml should generate a bash script as show below in demo-bash.sh . for ARGUMENT in \" ${ @: 2 } \" do KEY = $( echo $ARGUMENT | cut -f1 -d = ) VALUE = $( echo $ARGUMENT | cut -f2 -d = ) export \"MAGNUS_PRM_ $KEY \" = $VALUE done magnus execute_single_node $1 step%parameters --file getting-started.yaml exit_code = $? echo $exit_code if [ $exit_code -ne 0 ] ; then $( magnus execute_single_node $1 fail --file getting-started.yaml ) exit 1 fi magnus execute_single_node $1 step%shell --file getting-started.yaml exit_code = $? echo $exit_code if [ $exit_code -ne 0 ] ; then $( magnus execute_single_node $1 fail --file getting-started.yaml ) exit 1 fi echo hello exit_code = $? echo $exit_code if [ $exit_code -ne 0 ] ; then $( magnus execute_single_node $1 fail --file getting-started.yaml ) exit 1 fi magnus execute_single_node $1 success --file getting-started.yaml The shell script is translation of the dag into a series of bash commands but notice the command echo hello as part of the script. While the local mode interpreted that node as a stub or a mock node, the demo-renderer mode used the render_string variable of the node config to inject a script. This feature is very useful when you want certain few steps (may be email notifications) to be only possible in production like environments but want to mock the during dev/experimental set up. NOTE : When trying to locally re-run a dag definition with as-is node used to inject scripts, the run would start from as-is step onwards independent of the source of failure. You can change this behavior by writing extensions which skip over as-is nodes during re-run.","title":"Advanced use as-is"},{"location":"examples/#controlling_the_log_level_of_magnus","text":"The default log level of magnus is WARNING but you can change it at the point of execution to one of ['CRITICAL', 'ERROR', 'WARNING', 'INFO', 'DEBUG', 'NOTSET] by using the command line argument --log-level. For example: magnus execute --file <dag definition file> --log-level DEBUG would set the magnus log level to DEBUG. This setting only affects magnus logs and will not alter your application log levels.","title":"Controlling the log level of magnus"},{"location":"examples/#order_of_configurations","text":"Magnus supports many ways of providing configurations but there is a order of preference. Magnus defaults to the following if no config is provided. mode : type : local config : enable_parallel : \"false\" run_log_store : type : buffered catalog : type : file-system config : compute_data_folder : data/ catalog_location : .catalog secrets : type : do-nothing But you can over-ride these defaults by providing a magnus-config.yaml in the source directory. For example, if the magnus-config.yaml file has the following contents, even if you do not provide a config in the dag definition file, these would taken as default service providers. mode : type : local config : enable_parallel : \"true\" # false is the default run_log_store : type : file-system catalog : type : file-system config : compute_data_folder : data/ # default catalog_location : .catalog # default secrets : type : dotenv config : location : .env # default You can over-ride the defaults either set by magnus or magnus-config.yaml by providing the config in the dag definition file. For example, in the dag definition below only the secrets providers config is over-ridden by the config present in the dag definition file. Compute mode, catalog and run log store configurations remain the same to defaults. secrets : type : do-nothing dag : description : Demo of secrets start_at : step 1 steps : step 1 : type : task next : success command : my_module.first_function command_type : python success : type : success failure : type : fail Finally, you can also over-ride the configurations set in the dag definition file by providing a custom configuration file containing only the configurations. For example, you can provide a dag definition file as above with do-nothing secrets handler but by providing the below configurations file at the run time, you can over-ride it to dotenv . #in prod-configuration.yaml secrets : type : dotenv The command to execute while providing the configuration file. magnus execute --file <dag definition file> --config-file prod-configuration.yaml The design thought is enable switching between different configurations by different actors involved in the data science workflow. The engineering team could provide magnus-config.yaml that should be default to the team or project for dev/experimental phase of the work but can over-ride the configuration during production deployment.","title":"Order of configurations"},{"location":"examples/#custom_local_extensions","text":"Magnus was built with extensions in mind. For example, there could be catalog extension using s3 or object storage that are generic enough to be open sourced back to the community. But there is always a chance where the extension is only specific to your team or project. You can implement custom extensions to either compute mode, run log store, catalog or secrets as part of your source folder and let magnus know to use them. For example, consider the use case of a custom secrets handler that only serves your team needs, called CustomSecrets which extends BaseSecrets provided by magnus like below. The secrets manager does nothing special and always returns 'always the same' as the secret value. # Present in the src.custom_secrets folder of your project from magnus.secrets import BaseSecrets class CustomSecrets ( BaseSecrets ): \"\"\" Does the same thing \"\"\" service_name = 'custom-secrets' def __init__ ( self , config , ** kwargs ): super () . __init__ ( config , ** kwargs ) self . secrets = {} def get ( self , name : str = None , ** kwargs ) -> Union [ str , dict ]: \"\"\" If a name is provided, return None else return empty dict. Args: name (str): The name of the secret to retrieve Raises: Exception: If the secret by the name is not found. Returns: [type]: [description] \"\"\" if name : return 'always the same' return { 'secret_key' : 'always the same' } You can instruct magnus to detect and use the CustomSecrets by providing it in the magnus-config.yaml like below. # in magnus-config.yaml extensions : - src.custom_secrets Magnus would import the contents of the module defined in extensions and would delegate the responsibility of secrets to CustomSecrets . We would love it if you share your custom extension code or the design aspect as it builds the community.","title":"Custom local extensions"},{"location":"concepts/catalog/","text":"Overview \u00b6 Catalog provides a way to store and retrieve data generated by the individual steps of the dag either to downstream steps of the dag. Catalog also provides a way to reproduce a historic magnus run on any other machine. Along with the actual file, we also store the SHA id of the data catalog object in the logs to enable diagnostics. Magnus stores the data generated for every run in the catalog indexed by the unique run_id of the run. This enables you to re-run an older run and debug in case of any errors with the actual datasets used. Note Since the data is stored per-run, it might cause the catalog to inflate a lot. Please consider some clean up mechanisms to regularly prune runs that are no longer relevant. As with all services of magnus, there are several providers of catalog and you can easily extend to create your own cataloging system and use it in your runs. Configuration \u00b6 Configuring the catalog can be done as follows. catalog : type : config : type \u00b6 The type of catalog you want. This should be one of the catalog types already available. By default FileSystem Catalog is given if no config is provided. config \u00b6 Any configuration variables accepted by the catalog provider. Configuration within Step \u00b6 Within a step of the dag, the catalog can be configured by catalog : ... dag : steps : step name : ... catalog : compute_data_folder : # optional get : - list put : - list ... compute_data_folder \u00b6 The compute_data_folder for a single step could be different from the global compute_data_folder and you can provide it by using the catalog settings for that step. The actual cataloging is done in two stages: get: Get the data mentioned in the get from the catalog to compute_data_folder before executing the node. put: Store all the data mentioned in put from the compute_data_folder to catalog after executing the node. Both get and put can accept glob patterns. Internally we use Pathlib match function to match the name to pattern. Note The put stage of the cataloging checks if the data source has been obtained from get phase and only puts a new record if there were changes observed during the execution of the node. Interaction within code \u00b6 You can also interact with the catalog within your python programs if it is convenient than providing it in yaml. Get from catalog \u00b6 To get a file from the catalog, use get_from_catalog from magnus. For example, the below code gets the file interesting_data.csv from the catalog into data/ folder. from magnus import get_from_catalog def my_function (): get_from_catalog ( 'interesting.csv' , destination_folder = 'data/' ) Put in catalog \u00b6 To put a file into the catalog, use put_in_catalog from magnus. For example, the below code puts the file data/interesting_data.csv from the data folder into catalog. from magnus import put_in_catalog def my_function (): put_in_catalog ( 'data/interesting.csv' ) Note Unlike put phase of the cataloging process, put_in_catalog does not check if the cataloging object has changed and does a blind update. Parameterized definition \u00b6 As with any part of the magnus configuration, you can parameterize the configuration of catalog to switch between catalog providers without changing the base definition. Please follow the example provided here for more information. Extensions \u00b6 You can easily extend magnus to bring in your custom provider, if a default implementation does not exist or you are not happy with the implementation.","title":"Overview"},{"location":"concepts/catalog/#overview","text":"Catalog provides a way to store and retrieve data generated by the individual steps of the dag either to downstream steps of the dag. Catalog also provides a way to reproduce a historic magnus run on any other machine. Along with the actual file, we also store the SHA id of the data catalog object in the logs to enable diagnostics. Magnus stores the data generated for every run in the catalog indexed by the unique run_id of the run. This enables you to re-run an older run and debug in case of any errors with the actual datasets used. Note Since the data is stored per-run, it might cause the catalog to inflate a lot. Please consider some clean up mechanisms to regularly prune runs that are no longer relevant. As with all services of magnus, there are several providers of catalog and you can easily extend to create your own cataloging system and use it in your runs.","title":"Overview"},{"location":"concepts/catalog/#configuration","text":"Configuring the catalog can be done as follows. catalog : type : config :","title":"Configuration"},{"location":"concepts/catalog/#configuration_within_step","text":"Within a step of the dag, the catalog can be configured by catalog : ... dag : steps : step name : ... catalog : compute_data_folder : # optional get : - list put : - list ...","title":"Configuration within Step"},{"location":"concepts/catalog/#interaction_within_code","text":"You can also interact with the catalog within your python programs if it is convenient than providing it in yaml.","title":"Interaction within code"},{"location":"concepts/catalog/#parameterized_definition","text":"As with any part of the magnus configuration, you can parameterize the configuration of catalog to switch between catalog providers without changing the base definition. Please follow the example provided here for more information.","title":"Parameterized definition"},{"location":"concepts/catalog/#extensions","text":"You can easily extend magnus to bring in your custom provider, if a default implementation does not exist or you are not happy with the implementation.","title":"Extensions"},{"location":"concepts/command-types/","text":"Command types \u00b6 Python \u00b6 By default, python is the command type. You can mention the python function that you want to invoke using the command section. For example, in the dag definition below, the command type is defaulted to python and magnus invokes my_module.my_function as part of the step. dag : steps : step1 : command : my_module.my_function ... The function arguments are dynamically introspected from the parameter space. The return value of the function should always be a dictionary and are added as key-value pairs into the parameter space. More examples Shell \u00b6 You can have shell commands as part of magnus dag definition. The command provided in the config is invoked as part of the step. For example, in the dag definition below, step invokes the ls command as part of the pipeline. You can use this command_type to have non-python executables as part of your pipeline. dag : steps : step1 : command : ls command_type : shell ... Please note that, magnus will be able to send in the existing parameters using environmental variables prefixed with MAGNUS_PRM_ but would not be able to collect any return parameters. Similarly, the functionality of secrets should be handled by the script and would not be done by magnus. The cataloging functionality works as designed and can be used to sync data in and out the compute_data_folder . More examples Python lambda expression \u00b6 Using command_type: python-lambda , you can provide a lambda expression as command . For example: lambda x : int(x) + 1 is a valid lambda expression. Note that, you cannot have _ or __ as part of your string. This is just a security feature to avoid malicious code injections . The parameters used as part of the lambda expression are introspected and provided dynamically from the parameter space. This command type is designed to provide simpler ways to manipulate parameter space. Notebook \u00b6 In magnus, you can execute Jupyter notebooks by command_type: notebook . The command should be the path to the notebook you want to execute. Note For command_type: notebook to work, you need to install optional packages by: pip install magnus[notebook] Internally, we use papermill for inspection and execution of the notebook. Any parameters defined in the notebook would be introspected and dynamically provided at runtime from the parameter space. The path of the output of execution is obtained by post-fixing _out to the input notebook but can be configured by command_config as shown below. dag : steps : step1 : command : notebooks/input.ipynb command_type : notebook command_config : notebook_output_path : notebooks/output.ipynb ... You can also control the kernel used for execution by using, notebook_kernel as part of command_config . The default kernel used is the current kernel of the execution environment. You can also provide additional arguments to papermill by providing a mapping optional_papermill_args as part of command_config . Please note that, magnus will not be able to collect any return parameters. Similarly, the functionality of secrets should be handled by the notebook and would not be done by magnus. The cataloging functionality works as designed. Extensions \u00b6 You can extend and implement your command_types by extending the base class of the command type. #Example implementations can be found in magnus/tasks.py class BaseTaskType : # pylint: disable=too-few-public-methods \"\"\" A base task class which does the execution of command defined by the user \"\"\" task_type = '' def __init__ ( self , command : str , config : dict = None ): self . command = command self . config = config or {} def get_parameters ( self , map_variable : dict = None , ** kwargs ) -> dict : \"\"\" Return the parameters in scope for the execution Args: map_variable (dict, optional): If the command is part of map node, the value of map. Defaults to None. Returns: dict: The parameters dictionary in-scope for the task execution \"\"\" return utils . get_user_set_parameters ( remove = False ) def execute_command ( self , map_variable : dict = None , ** kwargs ): \"\"\" The function to execute the command. And map_variable is sent in as an argument into the function. Args: map_variable (dict, optional): If the command is part of map node, the value of map. Defaults to None. Raises: NotImplementedError: Base class, not implemented \"\"\" raise NotImplementedError () def set_parameters ( self , parameters : dict = None , ** kwargs ): \"\"\" Set the parameters back to the environment variables. Args: parameters (dict, optional): The parameters to set back as env variables. Defaults to None. \"\"\" # Nothing to do if not parameters : return if not isinstance ( parameters , dict ): msg = ( f 'call to function { self . command } returns of type: { type ( parameters ) } . ' 'Only dictionaries are supported as return values for functions as part part of magnus pipeline.' ) raise Exception ( msg ) for key , value in parameters . items (): logger . info ( f 'Setting User defined parameter { key } with value: { value } ' ) os . environ [ defaults . PARAMETER_PREFIX + key ] = json . dumps ( value ) The custom extensions should be registered as part of the namespace: magnus.tasks.BaseTaskType for it to be loaded. # For example, as part of your pyproject.toml [tool.poetry.plugins.\"magnus.tasks.BaseTaskType\"] \"sql\" = \"YOUR_PACKAGE:SQLtaskType\"","title":"Command types"},{"location":"concepts/command-types/#command_types","text":"","title":"Command types"},{"location":"concepts/command-types/#python","text":"By default, python is the command type. You can mention the python function that you want to invoke using the command section. For example, in the dag definition below, the command type is defaulted to python and magnus invokes my_module.my_function as part of the step. dag : steps : step1 : command : my_module.my_function ... The function arguments are dynamically introspected from the parameter space. The return value of the function should always be a dictionary and are added as key-value pairs into the parameter space. More examples","title":"Python"},{"location":"concepts/command-types/#shell","text":"You can have shell commands as part of magnus dag definition. The command provided in the config is invoked as part of the step. For example, in the dag definition below, step invokes the ls command as part of the pipeline. You can use this command_type to have non-python executables as part of your pipeline. dag : steps : step1 : command : ls command_type : shell ... Please note that, magnus will be able to send in the existing parameters using environmental variables prefixed with MAGNUS_PRM_ but would not be able to collect any return parameters. Similarly, the functionality of secrets should be handled by the script and would not be done by magnus. The cataloging functionality works as designed and can be used to sync data in and out the compute_data_folder . More examples","title":"Shell"},{"location":"concepts/command-types/#python_lambda_expression","text":"Using command_type: python-lambda , you can provide a lambda expression as command . For example: lambda x : int(x) + 1 is a valid lambda expression. Note that, you cannot have _ or __ as part of your string. This is just a security feature to avoid malicious code injections . The parameters used as part of the lambda expression are introspected and provided dynamically from the parameter space. This command type is designed to provide simpler ways to manipulate parameter space.","title":"Python lambda expression"},{"location":"concepts/command-types/#notebook","text":"In magnus, you can execute Jupyter notebooks by command_type: notebook . The command should be the path to the notebook you want to execute. Note For command_type: notebook to work, you need to install optional packages by: pip install magnus[notebook] Internally, we use papermill for inspection and execution of the notebook. Any parameters defined in the notebook would be introspected and dynamically provided at runtime from the parameter space. The path of the output of execution is obtained by post-fixing _out to the input notebook but can be configured by command_config as shown below. dag : steps : step1 : command : notebooks/input.ipynb command_type : notebook command_config : notebook_output_path : notebooks/output.ipynb ... You can also control the kernel used for execution by using, notebook_kernel as part of command_config . The default kernel used is the current kernel of the execution environment. You can also provide additional arguments to papermill by providing a mapping optional_papermill_args as part of command_config . Please note that, magnus will not be able to collect any return parameters. Similarly, the functionality of secrets should be handled by the notebook and would not be done by magnus. The cataloging functionality works as designed.","title":"Notebook"},{"location":"concepts/command-types/#extensions","text":"You can extend and implement your command_types by extending the base class of the command type. #Example implementations can be found in magnus/tasks.py class BaseTaskType : # pylint: disable=too-few-public-methods \"\"\" A base task class which does the execution of command defined by the user \"\"\" task_type = '' def __init__ ( self , command : str , config : dict = None ): self . command = command self . config = config or {} def get_parameters ( self , map_variable : dict = None , ** kwargs ) -> dict : \"\"\" Return the parameters in scope for the execution Args: map_variable (dict, optional): If the command is part of map node, the value of map. Defaults to None. Returns: dict: The parameters dictionary in-scope for the task execution \"\"\" return utils . get_user_set_parameters ( remove = False ) def execute_command ( self , map_variable : dict = None , ** kwargs ): \"\"\" The function to execute the command. And map_variable is sent in as an argument into the function. Args: map_variable (dict, optional): If the command is part of map node, the value of map. Defaults to None. Raises: NotImplementedError: Base class, not implemented \"\"\" raise NotImplementedError () def set_parameters ( self , parameters : dict = None , ** kwargs ): \"\"\" Set the parameters back to the environment variables. Args: parameters (dict, optional): The parameters to set back as env variables. Defaults to None. \"\"\" # Nothing to do if not parameters : return if not isinstance ( parameters , dict ): msg = ( f 'call to function { self . command } returns of type: { type ( parameters ) } . ' 'Only dictionaries are supported as return values for functions as part part of magnus pipeline.' ) raise Exception ( msg ) for key , value in parameters . items (): logger . info ( f 'Setting User defined parameter { key } with value: { value } ' ) os . environ [ defaults . PARAMETER_PREFIX + key ] = json . dumps ( value ) The custom extensions should be registered as part of the namespace: magnus.tasks.BaseTaskType for it to be loaded. # For example, as part of your pyproject.toml [tool.poetry.plugins.\"magnus.tasks.BaseTaskType\"] \"sql\" = \"YOUR_PACKAGE:SQLtaskType\"","title":"Extensions"},{"location":"concepts/dag/","text":"Dag \u00b6 Dag or directed acyclic graphs are a way to define your work flows. Its a graph representation of the series of tasks you want to perform and the order of it. In magnus, a lot of design emphasis was on making sure that a dag once defined should not change for deployment purposes. The dag is also version controlled and as part of your code repositories to promote good software engineering practices. These design decisions should enable experimentation to happen in interactive modes while engineering teams can use their preferred Continuos Integration tools to operationalize experiments once they are mature without changing code or the dag. We see the dag as a contract between the engineering teams and data science teams. While the data science teams can focus on what should be the part of the pipeline, the engineering teams can focus on the how to operationalize it. The configuration of a dag: dag : start_at : description : #optional max_time : # Optional steps : description (optional) \u00b6 A short description of the dag max_time (optional) \u00b6 The maximum compute time that this dag could run in seconds. Note Maximum run time is just a definition in the dag and the actual implementation depends upon the mode of execution. For example, interactive modes can completely ignore the maximum run time. Orchestration modes might have a default if one is not provided. For example: AWS step functions defaults maximum run time for a a state machine to be 86400 seconds. start_at \u00b6 The node/step in the steps to start the traversal of the graph. A node of this name should be present in the steps. steps \u00b6 A mapping of steps with each step belonging to one of the defined types . Example \u00b6 Assuming this is in dag-concepts.yaml # in dag-concepts.yaml dag : start_at : Hello steps : Hello : type : task command : my_module.say_hello next : Success Success : type : success Fail : type : fail And the following code in my_module.py # in my_module.py def say_hello ( name = world ): print ( f 'Hello { name } ' ) We can execute the dag by: magnus execute --file dag-concepts.yaml --name universe You should be able to see Hello universe in the logs. Parameterized Definition \u00b6 Magnus allows dag definitions to be parameterized by using placeholders. We use python String templates to enable parameter substitution. As we use, safe_substitution it means that we silently ignore any parameter that is not found. You should make sure that the parameters are properly defined. Example of variables \u00b6 Assuming this is in dag-variable.yaml dag : start_at : Hello steps : Hello : type : task command : ${module_name} next : Success Success : type : success Fail : type : fail and we have defined our variables in variables.yaml as # in variables.yaml module_name : my_module.say_hello and with the same python code as before , we can achieve the same result by: magnus execute --file dag-variable.yaml --name universe --var-file variables.yaml Magnus would resolve the placeholders at the load of the dag definition. Design thought behind variables \u00b6 Parameters are a great way to have a generalized definition of the dag and the config parameters. Internally, we use variables to switch between different configs for testing different implementations of executor, run log, catalog and secrets without changing the pipeline definition file.","title":"Dag"},{"location":"concepts/dag/#dag","text":"Dag or directed acyclic graphs are a way to define your work flows. Its a graph representation of the series of tasks you want to perform and the order of it. In magnus, a lot of design emphasis was on making sure that a dag once defined should not change for deployment purposes. The dag is also version controlled and as part of your code repositories to promote good software engineering practices. These design decisions should enable experimentation to happen in interactive modes while engineering teams can use their preferred Continuos Integration tools to operationalize experiments once they are mature without changing code or the dag. We see the dag as a contract between the engineering teams and data science teams. While the data science teams can focus on what should be the part of the pipeline, the engineering teams can focus on the how to operationalize it. The configuration of a dag: dag : start_at : description : #optional max_time : # Optional steps :","title":"Dag"},{"location":"concepts/dag/#example","text":"Assuming this is in dag-concepts.yaml # in dag-concepts.yaml dag : start_at : Hello steps : Hello : type : task command : my_module.say_hello next : Success Success : type : success Fail : type : fail And the following code in my_module.py # in my_module.py def say_hello ( name = world ): print ( f 'Hello { name } ' ) We can execute the dag by: magnus execute --file dag-concepts.yaml --name universe You should be able to see Hello universe in the logs.","title":"Example"},{"location":"concepts/dag/#parameterized_definition","text":"Magnus allows dag definitions to be parameterized by using placeholders. We use python String templates to enable parameter substitution. As we use, safe_substitution it means that we silently ignore any parameter that is not found. You should make sure that the parameters are properly defined.","title":"Parameterized Definition"},{"location":"concepts/integration/","text":"Integration \u00b6 Magnus at the core provides 4 services A computational execution plan or an Executor. A run log store to store metadata and run logs. A cataloging functionality to pass data between steps and audibility trace. A framework to handle secrets. The executor plays the role of talking to other 3 service providers to process the graph, keep track of the status of the run, pass data between steps and provide secrets. Depending upon the stage of execution, the executor might do one of the two actions traversing the graph : For compute modes that just render instructions for other engines, the executor first traverses the graph to understand the plan but does not actually execute. For interactive modes, the executor traverses to set up the right environment for execution but defers the execution for later stage. executing the node : The executor is actually in the compute environment that it has to be and executes the task. Magnus is designed to make the executor talk to the service providers at both these stages to understand the changes needed for the config to make it happen via the BaseIntegration pattern. class BaseIntegration : \"\"\" Base class for handling integration between Executor and one of Catalog, Secrets, RunLogStore. \"\"\" mode_type = None service_type = None # One of secret, catalog, datastore service_provider = None # The actual implementation of the service def __init__ ( self , executor , integration_service ): self . executor = executor self . service = integration_service def validate ( self , ** kwargs ): \"\"\" Raise an exception if the mode_type is not compatible with service provider. By default, it is considered as compatible. \"\"\" def configure_for_traversal ( self , ** kwargs ): \"\"\" Do any changes needed to both executor and service provider during traversal of the graph. You are in the compute environment traversing the graph by this time. By default, no change is required. \"\"\" def configure_for_execution ( self , ** kwargs ): \"\"\" Do any changes needed to both executor and service provider during execution of a node. You are in the compute environment by this time. By default, no change is required. \"\"\" The custom extensions should be registered as part of the namespace: magnus.integration.BaseIntegration for it to be loaded. # For example, as part of your pyproject.toml [tool.poetry.plugins.\"magnus.integration.BaseIntegration\"] # {executor.name}-{service}-{service.name} \"local-secrets-vault\" = \"YOUR_PACKAGE:LocalComputeSecretsVault\" All extensions need to be unique given a mode_type , service_type and service_provider . Duplicate integrations will be raised as an exception. Example \u00b6 Consider the example of S3 Run log store. For the execution engine of local , the aws credentials file is available on the local machine and we can store the run logs in the S3 bucket. But for the mode local-container , the aws credentials file has to be mounted in the container for the container to have access to S3. This could be achieved by writing an integration pattern between S3 and local-container to do the same. class LocalContainerComputeS3Store ( BaseIntegration ): \"\"\" Integration between local container and S3 run log store \"\"\" mode_type = 'local-container' service_type = 'run-log-store' # One of secret, catalog, datastore service_provider = 's3' # The actual implementation of the service def configure_for_traversal ( self , ** kwargs ): write_to = self . service . get_aws_credentials_file () self . executor . volumes [ str ( Path ( write_to ) . resolve ())] = { 'bind' : '/root/.aws/credentials' , 'mode' : 'ro' } We instruct the executor to mount the volumes containing the AWS credentials file as part of spinning the container to make the credentials available to the running container.","title":"Overview"},{"location":"concepts/integration/#integration","text":"Magnus at the core provides 4 services A computational execution plan or an Executor. A run log store to store metadata and run logs. A cataloging functionality to pass data between steps and audibility trace. A framework to handle secrets. The executor plays the role of talking to other 3 service providers to process the graph, keep track of the status of the run, pass data between steps and provide secrets. Depending upon the stage of execution, the executor might do one of the two actions traversing the graph : For compute modes that just render instructions for other engines, the executor first traverses the graph to understand the plan but does not actually execute. For interactive modes, the executor traverses to set up the right environment for execution but defers the execution for later stage. executing the node : The executor is actually in the compute environment that it has to be and executes the task. Magnus is designed to make the executor talk to the service providers at both these stages to understand the changes needed for the config to make it happen via the BaseIntegration pattern. class BaseIntegration : \"\"\" Base class for handling integration between Executor and one of Catalog, Secrets, RunLogStore. \"\"\" mode_type = None service_type = None # One of secret, catalog, datastore service_provider = None # The actual implementation of the service def __init__ ( self , executor , integration_service ): self . executor = executor self . service = integration_service def validate ( self , ** kwargs ): \"\"\" Raise an exception if the mode_type is not compatible with service provider. By default, it is considered as compatible. \"\"\" def configure_for_traversal ( self , ** kwargs ): \"\"\" Do any changes needed to both executor and service provider during traversal of the graph. You are in the compute environment traversing the graph by this time. By default, no change is required. \"\"\" def configure_for_execution ( self , ** kwargs ): \"\"\" Do any changes needed to both executor and service provider during execution of a node. You are in the compute environment by this time. By default, no change is required. \"\"\" The custom extensions should be registered as part of the namespace: magnus.integration.BaseIntegration for it to be loaded. # For example, as part of your pyproject.toml [tool.poetry.plugins.\"magnus.integration.BaseIntegration\"] # {executor.name}-{service}-{service.name} \"local-secrets-vault\" = \"YOUR_PACKAGE:LocalComputeSecretsVault\" All extensions need to be unique given a mode_type , service_type and service_provider . Duplicate integrations will be raised as an exception.","title":"Integration"},{"location":"concepts/integration/#example","text":"Consider the example of S3 Run log store. For the execution engine of local , the aws credentials file is available on the local machine and we can store the run logs in the S3 bucket. But for the mode local-container , the aws credentials file has to be mounted in the container for the container to have access to S3. This could be achieved by writing an integration pattern between S3 and local-container to do the same. class LocalContainerComputeS3Store ( BaseIntegration ): \"\"\" Integration between local container and S3 run log store \"\"\" mode_type = 'local-container' service_type = 'run-log-store' # One of secret, catalog, datastore service_provider = 's3' # The actual implementation of the service def configure_for_traversal ( self , ** kwargs ): write_to = self . service . get_aws_credentials_file () self . executor . volumes [ str ( Path ( write_to ) . resolve ())] = { 'bind' : '/root/.aws/credentials' , 'mode' : 'ro' } We instruct the executor to mount the volumes containing the AWS credentials file as part of spinning the container to make the credentials available to the running container.","title":"Example"},{"location":"concepts/modes/","text":"Compute Modes \u00b6 In magnus, a compute mode controls the way a dag is interpreted. In some modes, we do the actual execution of the dag while in some modes it only renders a dag definition language for a specific orchestrator. Conceptually, a mode can be one of two types: Interactive mode : In this mode, the dag definition is actually executed by magnus and usually it is invoked as magnus execute --file my-project.yaml --var-file variables.yaml Magnus takes care of traversal of the dag and execution of the graph in the compute you requested. Examples of this mode or local, local container, local aws batch etc. Orchestration mode : In this mode, the dag definition is translated to your preferred orchestration language of dag definition. To still achieve the capabilities of interactive mode, the orchestration language is directed to call an internal method instead of your actual function. Specifically, the orchestration is asked to call magnus execute_single_node --file my-project.yaml --var-file variables.yaml --step-name step-to-call The branches of the original dag are also translated to the orchestrators language if its supported. If the orchestration mode does not support a feature that magnus supports, you could still make it work by a mixed model. Examples of orchestration modes are aws step functions, kubeflow job specification, argo job specification etc. Configuration \u00b6 As with any system within magnus, configuration of a mode can be done by: mode : type : config : type \u00b6 The type of mode provider you want. This should be one of the mode types already available. Local mode is provided by default if nothing is provided. config \u00b6 Any configuration parameters the mode provider accepts. Parameterized definition \u00b6 As with any part of the magnus configuration, you can parameterize the configuration of Mode to switch between Mode providers without changing the base definition. Please follow the example provided here for more information. Extensions \u00b6 You can easily extend magnus to interpret the dag definition to a orchestration language of choice, if a default implementation does not exist or you are not happy with the implementation.","title":"Overview"},{"location":"concepts/modes/#compute_modes","text":"In magnus, a compute mode controls the way a dag is interpreted. In some modes, we do the actual execution of the dag while in some modes it only renders a dag definition language for a specific orchestrator. Conceptually, a mode can be one of two types: Interactive mode : In this mode, the dag definition is actually executed by magnus and usually it is invoked as magnus execute --file my-project.yaml --var-file variables.yaml Magnus takes care of traversal of the dag and execution of the graph in the compute you requested. Examples of this mode or local, local container, local aws batch etc. Orchestration mode : In this mode, the dag definition is translated to your preferred orchestration language of dag definition. To still achieve the capabilities of interactive mode, the orchestration language is directed to call an internal method instead of your actual function. Specifically, the orchestration is asked to call magnus execute_single_node --file my-project.yaml --var-file variables.yaml --step-name step-to-call The branches of the original dag are also translated to the orchestrators language if its supported. If the orchestration mode does not support a feature that magnus supports, you could still make it work by a mixed model. Examples of orchestration modes are aws step functions, kubeflow job specification, argo job specification etc.","title":"Compute Modes"},{"location":"concepts/modes/#configuration","text":"As with any system within magnus, configuration of a mode can be done by: mode : type : config :","title":"Configuration"},{"location":"concepts/modes/#parameterized_definition","text":"As with any part of the magnus configuration, you can parameterize the configuration of Mode to switch between Mode providers without changing the base definition. Please follow the example provided here for more information.","title":"Parameterized definition"},{"location":"concepts/modes/#extensions","text":"You can easily extend magnus to interpret the dag definition to a orchestration language of choice, if a default implementation does not exist or you are not happy with the implementation.","title":"Extensions"},{"location":"concepts/nodes/","text":"Nodes \u00b6 Nodes are fundamentally the smallest logical unit of work that you want to execute. Though there is no explicit guidelines on how big or small a node should be, we advice that the node becomes a part of narrative of the whole project. For example, lets take a scenario where you perform some data cleaning task before you are ready to transform/train a machine learning model. The data cleaning task could be one single task node or single dag node (which internally is a graph) if you have too many steps. The choice is completely yours to make and depends on the narrative of the project. Nodes in magnus can be logically split into 3 types: Execution : fundamentally this is a python function call or Shell command that you want to call as part of the pipeline. Task and As-Is node is the only nodes of this type. Status : nodes that denote the eventual status of a graph/sub-graph. Success or Fail nodes are examples of this type. All dag definitions should have one and exactly one node of this type and the status of the dag is basically the type of status node it hits at the end. Composite : nodes that are sub-graphs by itself. Parallel, Dag and Map are examples of this type and all three have different use cases. Nesting of composite nodes is possible, though we advise to keep the nesting simple to promote readability. Note Node names cannot have . or % in them. Any valid python string is acceptable as a name of the step. Task \u00b6 The smallest executable of the pipeline or in python language, the function call that you want to call as part of the the pipeline. In magnus, a task node has the following configuration. step name : retry : 1 # Defaults to 1 type : task next : command : command_type : # Defaults to python on_failure : # Defaults to None mode_config : # Defaults to None catalog : # Defaults to None compute_data_folder : get : put : command (required) \u00b6 The name of the actual function/shell executable you want to call as part of the pipeline. For example, for the following function, the command would be my_module.my_cool_function . # In my_module.py def my_cool_function (): pass command_type (optional) \u00b6 Defaults to python if nothing is provided. For more information, please refer command types retry (optional) \u00b6 The number of attempts to make before failing the node. Default to 1. next (required) \u00b6 The name of the node in the graph to go if the node succeeds. on_failure (optional) \u00b6 The name of the node in the graph to go if the node fails. This is optional as we would move to the fail node of the graph if one is not provided. On_failure could be an use case where you want to send a failure notification before marking the run as failure. mode_config (optional) \u00b6 Use this section to pass instructions to the executor. For example, we can instruct the local-container mode to use a different docker image to run this step of the pipeline. Example usage of mode_config: mode : type : local-container config : docker_image : python:3.7 dag : start_at : Cool function steps : Cool function : type : task command : my_module.my_cool_function next : Clean Up Clean Up : type : task command : clean_up.sh command_type : shell mode_config : docker_image : ubuntu:latest next : Success Success : type : success Fail : type : fail In the above example, while all the steps except for Clean Up happen in python3.7 docker image, the Clean Up happens in Ubuntu. mode_config provides a way for dag to have customizable instructions to the executor. catalog (optional) \u00b6 compute_data_folder: The folder where we need to sync-in or sync-out the data to the catalog . If it is not provided, it defaults to the global catalog settings. get: The files to sync-in from the catalog to the compute data folder, prior execution. put: The files to sync-out from the compute data folder to the catalog, post execution. Glob pattern naming in get or put are fully supported, internally we use Pathlib match function to match the name to pattern. Example catalog settings: catalog : compute_data_folder : data/ get : - '*' put : - 'cleaned*' In this, we sync-in all the files from the catalog to the compute data folder, data prior to the execution and sync-out all files started with cleaned to the catalog after the execution. Logically, magnus does the following when executing a task: Check the catalog-get list for any files that have to be synced to compute data folder. Inspect the function call to determine the arguments required to make the function call. Retrieve them from the parameters or fail if not present. Check if the function call has to executed in case of re-runs. If the previous re-run of the step was successful, we skip it. Make the actual function call, if we need to, and determine the result. Check the catalog-put list for any files that have to be synced back to catalog from the compute data folder. Example of a pipeline with one task node, success node and failure node. \u00b6 dag : start_at : Cool function steps : Cool function : type : task command : my_module.my_cool_function next : Success Success : type : success Fail : type : fail Success \u00b6 A status node of the graph. There should be one and only one success node per graph. The traversal of the graph stops at this node with marking the run as success. In magnus, this node can be configured as: step name : type : success No other fields are required and should not be provided. Fail \u00b6 A status node of the graph. There should be one and only one fail node per graph. The traversal of the graph stops at this node with marking the run as fail. In magnus, this node can be configured as: step name : type : fail No other fields are required and should not be provided. Parallel \u00b6 Parallel node is a composite node that in it-self has sub-graphs. A good example is to construct independent features of a training data in machine learning experiments. The number of branches in parallel node is static and pre-determined. Each branch follows the same definition language as the graph. The configuration of a parallel node could be done as: step name : type : parallel next : on_failure : branches : branch_a : ... branch_b : ... next (required) \u00b6 The name of the node in the graph to go if the node succeeds on_failure (optional) \u00b6 The name of the node in the graph to go if the node fails. This is optional as we would move to the fail node of the graph if one is not provided. on_failure could be an use case where you want to send a failure notification before marking the run as failure. branches (required) \u00b6 The branches of the step that you want to parallelize. Each branch follows the same definition as a dag in itself. Example \u00b6 Feature generation : type : parallel next : ML training branches : One hot encoding : start_at : encoder steps : encoder : type : task next : success_state command : my_encoder.encode success_state : type : success fail_state : type : fail Scaler : start_at : scale steps : scale : type : task next : success_state command : my_scaler.scale success_state : type : success fail_state : type : fail In the example, \"One hot encoding\" and \"Scaler\" are two branches that are defined using the same definition language as a dag and both together form the Feature generation step of the parent dag. Note A parallel state in the dag is just a definition, the actual implementation depends upon the mode and the support for parallelization. Dag \u00b6 Dag is a composite node which has one branch defined elsewhere. It is used to logically separate the complex details of a pipeline into modular units. For example, a typical data science project would have a data gathering, data cleaning, data transformation, modelling, prediction as steps. And it is understandable that these individual steps could get complex and require many steps to function. Instead of complicating the parent pipeline, we can abstract the individual steps into its own dag nodes. The configuration of a dag node is: step name : type : dag dag_definition : next : on_failure : # optional dag_definition \u00b6 The yaml file containing the dag definition in \"dag\" block of the file. The dag definition should follow the same rules as any other dag in magnus. next (required) \u00b6 The name of the node in the graph to go if the node succeeds on_failure (optional) \u00b6 The name of the node in the graph to go if the node fails. This is optional as we would move to the fail node of the graph if one is not provided. Example \u00b6 # Parent dag dag : start_at : Data Cleaning steps : Data Cleaning : type : dag next : Data Transformation dag_definition : data-cleaning.yaml Data Transformation : type : dag next : Modelling dag_definition : data-transformation.yaml Modelling : type : dag next : Success dag_definition : modelling.yaml Success : type : success Fail : type : fail # data-cleaning.yaml dag : start_at : Remove numbers steps : Remove numbers : type : task next : Remove special characters command : data_cleaning.remove_numbers Remove special characters : type : dag next : Success command : data_cleaning.remove_special_characters Success : type : success Fail : type : fail In this example, the parent dag only captures the high level tasks required to perform a data science experiment while the details of how data cleaning should be done are mentioned in data-cleaning.yaml. Map \u00b6 Map is a composite node consisting of one branch that can be iterated over a parameter. A typical use case would be performing the same data cleaning operation on a bunch of files or the columns of a data frame. The parameter over which the branch is iterated over should be provided and also be available to the dag at the execution time. The configuration of the map node: step name : type : map iterate_on : iterate_as : next : on_failure : # Optional branch : iterate_on (required) \u00b6 The name of the parameter to iterate on. The parameter should be of type List in python and should be available in the parameter space. iterate_as (required) \u00b6 The name of the argument that is expected by the task. For example: Set a parameter by name x which is a list [1, 2, 3] A python task node as part of the map dag definition expects this argument as x_i as part function signature. You should set iterate_on as x and iterate_as as x_i branch (required) \u00b6 The branch to iterate over the parameter. The branch definition should follow the same rules as a dag definition. next (required) \u00b6 The name of the node in the graph to go if the node succeeds on_failure (optional) \u00b6 The name of the node in the graph to go if the node fails. This is optional as we would move to the fail node of the graph if one is not provided. Example \u00b6 dag : start_at : List files steps : List files : type : task next : Clean files command : my_module.list_files Clean files : type : map next : Success iterate_on : file_list iterate_as : file_name branch : start_at : Task Clean Files steps : Task Clean Files : type : task command : my_module.clean_file next : success success : type : success fail : type : fail Success : type : success Fail : type : fail In this example dag definition, We start with the step List files , that generates a list of files to be cleaned and sets it as a parameter The step Clean files contains a branch that would be iterated over the list of files found in the previous step. To be comprehensive, here is the stub implementations of the python code # in my_module.py def list_files (): file_list = [ 'a' , 'b' , 'c' ] # do some compute to figure out the actual list of files would be # By returning a dictionary, you can set parameters that would be available for down stream steps. return { 'file_list' : file_list } def clean_file ( file_name ): # Retrieve the file or use catalog to retrieve the file and perform the cleaning steps pass Note A map state in the dag is just a definition, the actual implementation depends upon the mode and the support for parallelization. As-Is \u00b6 As-is a convenience node or a designers node. It can be used to mock nodes while designing the overall pipeline design without implementing anything in interactive modes. The same node can be used to render required templates in orchestration modes. The configuration of as-is node: step name : type : as-is command : next : render_string : command (optional) \u00b6 The command is purely optional in as-is node and even if one is provided it is not executed. next (required) \u00b6 The name of the node in the graph to go if the node succeeds render_string (optional) \u00b6 The placeholder template you want to render during translation. Example as mock node \u00b6 A very typical data science project workflow could be mocked by: dag : description : A mocked data science pipeline start_at : Data Cleaning steps : Data Cleaning : type : as-ias next : Data Transformation Data Transformation : type : as-is next : Modelling Modelling : type : as-is next : Deploy Deploy : type : as-is next : Success Success : type : success Fail : type : fail In this example, we only wrote a skeleton of the pipeline and none of the steps are actually implemented. Example as template \u00b6 Taking the same example, we can imagine that there is an executor which can deploy the trained ML model and requires a template to be generated as part of the continuos integration. mode : type : <some mode which deploys trained models> dag : description : A mocked data science pipeline start_at : Data Cleaning steps : Data Cleaning : type : task command : my_module.data_cleaning next : Data Transformation Data Transformation : type : task command : my_module.data_transformation next : Modelling Modelling : type : task command : my_module.modelling next : Deploy Deploy : type : as-is render_string : > python -m my_module.check_accuracy_threshold cp models/trained_models to s3://<some location> next : Success Success : type : success Fail : type : fail In interactive modes the as-is does not do anything and succeeds every time but the same dag in orchestrated modes can render a template that could be part of continuos integration process. Data science and ML research teams would thrive in interactive modes, given their experimental nature of work. As-Is nodes gives a way to do experiments without changing the dag definition once it is ready to be deployed. As-is nodes also provide a way to inject scripts as steps for orchestrators that do not support all the features of magnus. For example, if an orchestrator mode of your liking does not support map state, you can use as-is to inject a script that behaves like a map state and triggers all the required jobs. Passing data \u00b6 In magnus, we classify 2 kinds of data sets that can be passed around to down stream steps. Data: Processed files by an upstream step should be available for downstream steps when required. Catalog provides the way to do this. Parameters: Any JSON serializable data can be passed to down stream steps. Parameters from command line \u00b6 Changed in v0.2 Initial parameters to the application can be sent in via a parameters file. Example: magnus execute --file getting-started.yaml --parameters-file parameters.yaml # in parameters.yaml arg1 : test arg2 : dev In this case, arg1 and arg2 are available as parameters to downstream steps. Storing parameters \u00b6 Any JSON serializable dictionary returned from a task node is available as parameters to downstream steps. Example: def my_cool_function (): return { 'arg1' : 'hello' , 'arg2' : { 'hello' , 'world' } } Or from magnus import store_parameter def my_cool_function (): store_parameter ( arg1 = 'hello' , 'arg2' = { 'hello' , 'world' }) Or import os import json def my_cool_function (): os . environ [ 'MAGNUS_PRM_' + 'arg1' ] = 'hello' os . environ [ 'MAGNUS_PRM_' + 'arg2' ] = json . dumps ({ 'hello' , 'world' }) All the three above ways store arg1 and arg2 for downstream steps. Accessing parameters \u00b6 Any parameters set either at command line or by upstream nodes can be accessed by: def my_cool_function ( arg1 , arg2 = None ): pass The function is inspected to find all named args and provided value if the key exists in the parameters. or import os def my_cool_function (): arg1 = os . environ [ 'MAGNUS_PRM_arg1' ] arg2 = os . environ [ 'MAGNUS_PRM_arg2' ] or from magnus import get_parameter def my_cool_function (): arg1 = get_parameter ( 'arg1' ) arg2 = get_parameter ( 'arg2' ) or from magnus import get_parameter def my_cool_function (): args = get_parameter () arg1 = args [ 'arg1' ] arg2 = args [ 'arg2' ] Calling get_parameter with no key returns all parameters. Extensions \u00b6 You can extend and implement your own node_types by extending the BaseNode class. The base class has the following methods with only one of the two methods to be implemented for custom implementations. If the node.is_composite is True , implement the execute_as_graph method. If the node.is_composite is False , implement the execute method. # Source code present at magnus/nodes.py class BaseNode : \"\"\" Base class with common functionality provided for a Node of a graph. A node of a graph could be a * single execution node as task, success, fail. * Could be graph in itself as parallel, dag and map. * could be a convenience function like as-is. The name is relative to the DAG. The internal name of the node, is absolute name in dot path convention. This has one to one mapping to the name in the run log The internal name of a node, should always be odd when split against dot. The internal branch name, only applies for branched nodes, is the branch it belongs to. The internal branch name should always be even when split against dot. \"\"\" node_type = '' def __init__ ( self , name , internal_name , config , execution_type , internal_branch_name = None ): # pylint: disable=R0914,R0913 self . name = name self . internal_name = internal_name # Dot notation naming of the steps self . config = config self . internal_branch_name = internal_branch_name # parallel, map, dag only have internal names self . execution_type = execution_type self . branches = None self . is_composite = False # Change this to True if the node is composite ... # Truncated base methods as they should not be changed def execute ( self , executor , mock = False , map_variable : dict = None , ** kwargs ): \"\"\" The actual function that does the execution of the command in the config. Should only be implemented for task, success, fail and as-is and never for composite nodes. Args: executor (magnus.executor.BaseExecutor): The executor mode class mock (bool, optional): Don't run, just pretend. Defaults to False. map_variable (str, optional): The value of the map iteration variable, if part of a map node. Defaults to ''. Raises: NotImplementedError: Base class, hence not implemented. \"\"\" raise NotImplementedError def execute_as_graph ( self , executor , map_variable : dict = None , ** kwargs ): \"\"\" This function would be called to set up the execution of the individual branches of a composite node. Function should only be implemented for composite nodes like dag, map, parallel. Args: executor (magnus.executor.BaseExecutor): The executor mode. Raises: NotImplementedError: Base class, hence not implemented. \"\"\" raise NotImplementedError The custom extensions should be registered as part of the namespace: magnus.nodes.BaseNode for it to be loaded. # For example, as part of your pyproject.toml [tool.poetry.plugins.\"magnus.nodes.BaseNode\"] \"mail\" = \"YOUR_PACKAGE:MailTeam\"","title":"Node"},{"location":"concepts/nodes/#nodes","text":"Nodes are fundamentally the smallest logical unit of work that you want to execute. Though there is no explicit guidelines on how big or small a node should be, we advice that the node becomes a part of narrative of the whole project. For example, lets take a scenario where you perform some data cleaning task before you are ready to transform/train a machine learning model. The data cleaning task could be one single task node or single dag node (which internally is a graph) if you have too many steps. The choice is completely yours to make and depends on the narrative of the project. Nodes in magnus can be logically split into 3 types: Execution : fundamentally this is a python function call or Shell command that you want to call as part of the pipeline. Task and As-Is node is the only nodes of this type. Status : nodes that denote the eventual status of a graph/sub-graph. Success or Fail nodes are examples of this type. All dag definitions should have one and exactly one node of this type and the status of the dag is basically the type of status node it hits at the end. Composite : nodes that are sub-graphs by itself. Parallel, Dag and Map are examples of this type and all three have different use cases. Nesting of composite nodes is possible, though we advise to keep the nesting simple to promote readability. Note Node names cannot have . or % in them. Any valid python string is acceptable as a name of the step.","title":"Nodes"},{"location":"concepts/nodes/#task","text":"The smallest executable of the pipeline or in python language, the function call that you want to call as part of the the pipeline. In magnus, a task node has the following configuration. step name : retry : 1 # Defaults to 1 type : task next : command : command_type : # Defaults to python on_failure : # Defaults to None mode_config : # Defaults to None catalog : # Defaults to None compute_data_folder : get : put :","title":"Task"},{"location":"concepts/nodes/#success","text":"A status node of the graph. There should be one and only one success node per graph. The traversal of the graph stops at this node with marking the run as success. In magnus, this node can be configured as: step name : type : success No other fields are required and should not be provided.","title":"Success"},{"location":"concepts/nodes/#fail","text":"A status node of the graph. There should be one and only one fail node per graph. The traversal of the graph stops at this node with marking the run as fail. In magnus, this node can be configured as: step name : type : fail No other fields are required and should not be provided.","title":"Fail"},{"location":"concepts/nodes/#parallel","text":"Parallel node is a composite node that in it-self has sub-graphs. A good example is to construct independent features of a training data in machine learning experiments. The number of branches in parallel node is static and pre-determined. Each branch follows the same definition language as the graph. The configuration of a parallel node could be done as: step name : type : parallel next : on_failure : branches : branch_a : ... branch_b : ...","title":"Parallel"},{"location":"concepts/nodes/#dag","text":"Dag is a composite node which has one branch defined elsewhere. It is used to logically separate the complex details of a pipeline into modular units. For example, a typical data science project would have a data gathering, data cleaning, data transformation, modelling, prediction as steps. And it is understandable that these individual steps could get complex and require many steps to function. Instead of complicating the parent pipeline, we can abstract the individual steps into its own dag nodes. The configuration of a dag node is: step name : type : dag dag_definition : next : on_failure : # optional","title":"Dag"},{"location":"concepts/nodes/#map","text":"Map is a composite node consisting of one branch that can be iterated over a parameter. A typical use case would be performing the same data cleaning operation on a bunch of files or the columns of a data frame. The parameter over which the branch is iterated over should be provided and also be available to the dag at the execution time. The configuration of the map node: step name : type : map iterate_on : iterate_as : next : on_failure : # Optional branch :","title":"Map"},{"location":"concepts/nodes/#as-is","text":"As-is a convenience node or a designers node. It can be used to mock nodes while designing the overall pipeline design without implementing anything in interactive modes. The same node can be used to render required templates in orchestration modes. The configuration of as-is node: step name : type : as-is command : next : render_string :","title":"As-Is"},{"location":"concepts/nodes/#passing_data","text":"In magnus, we classify 2 kinds of data sets that can be passed around to down stream steps. Data: Processed files by an upstream step should be available for downstream steps when required. Catalog provides the way to do this. Parameters: Any JSON serializable data can be passed to down stream steps.","title":"Passing data"},{"location":"concepts/nodes/#extensions","text":"You can extend and implement your own node_types by extending the BaseNode class. The base class has the following methods with only one of the two methods to be implemented for custom implementations. If the node.is_composite is True , implement the execute_as_graph method. If the node.is_composite is False , implement the execute method. # Source code present at magnus/nodes.py class BaseNode : \"\"\" Base class with common functionality provided for a Node of a graph. A node of a graph could be a * single execution node as task, success, fail. * Could be graph in itself as parallel, dag and map. * could be a convenience function like as-is. The name is relative to the DAG. The internal name of the node, is absolute name in dot path convention. This has one to one mapping to the name in the run log The internal name of a node, should always be odd when split against dot. The internal branch name, only applies for branched nodes, is the branch it belongs to. The internal branch name should always be even when split against dot. \"\"\" node_type = '' def __init__ ( self , name , internal_name , config , execution_type , internal_branch_name = None ): # pylint: disable=R0914,R0913 self . name = name self . internal_name = internal_name # Dot notation naming of the steps self . config = config self . internal_branch_name = internal_branch_name # parallel, map, dag only have internal names self . execution_type = execution_type self . branches = None self . is_composite = False # Change this to True if the node is composite ... # Truncated base methods as they should not be changed def execute ( self , executor , mock = False , map_variable : dict = None , ** kwargs ): \"\"\" The actual function that does the execution of the command in the config. Should only be implemented for task, success, fail and as-is and never for composite nodes. Args: executor (magnus.executor.BaseExecutor): The executor mode class mock (bool, optional): Don't run, just pretend. Defaults to False. map_variable (str, optional): The value of the map iteration variable, if part of a map node. Defaults to ''. Raises: NotImplementedError: Base class, hence not implemented. \"\"\" raise NotImplementedError def execute_as_graph ( self , executor , map_variable : dict = None , ** kwargs ): \"\"\" This function would be called to set up the execution of the individual branches of a composite node. Function should only be implemented for composite nodes like dag, map, parallel. Args: executor (magnus.executor.BaseExecutor): The executor mode. Raises: NotImplementedError: Base class, hence not implemented. \"\"\" raise NotImplementedError The custom extensions should be registered as part of the namespace: magnus.nodes.BaseNode for it to be loaded. # For example, as part of your pyproject.toml [tool.poetry.plugins.\"magnus.nodes.BaseNode\"] \"mail\" = \"YOUR_PACKAGE:MailTeam\"","title":"Extensions"},{"location":"concepts/run-log/","text":"Run Log \u00b6 In magnus, Run log captures all the information required to accurately describe a run. It should not confused with your application logs, which are project dependent. Independent of the providers of any systems (compute, secrets, run log, catalog), the structure of the run log would remain the same and should enable you to compare between runs. To accurately recreate an older run either for debugging purposes or for reproducibility, it should capture all the variables of the system and their state during the run. For the purple of data science applications, it boils down to: Data: The source of the data and the version of it. Code: The code used to run the the experiment and the version of it. Environment: The environment the code ran in with all the system installations. Configuration: The pipeline definition and the configuration. The Run Log helps in storing them systematically for every run with the best possible information on all of the above. Structure of Run Log \u00b6 A typical run log has the following structure, with a few definitions given inline. { \"run_id\" : , \"dag_hash\" : , # The SHA id o f t he dag de f i n i t io n \"use_cached\" : , # True f or a re - ru n , False o t herwise \"tag\" : , # A fr ie n dly na me give n t o a group o f ru ns \"original_run_id\" : , # The ru n id o f t he older ru n i n case o f a re - ru n \"status\" : , \"steps\" : { }, \"parameters\" : { } } run_id \u00b6 Every run in magnus is given a unique run_id . Magnus creates one based on the timestamp is one is not provided during the run time. dag_hash \u00b6 The SHA id of the pipeline itself is stored here. In the case of re-run, we check the newly run pipeline hash against the older run to ensure they are the same. You can force to re-run too if you are aware of the differences. tag \u00b6 A friendly name that could be used to group multiple runs together. You can group multiple runs by the tag to compare and track the experiments done in the group. status \u00b6 A flag to denote the status of the run. The status could be: success : If the graph or sub-graph succeeded, i.e reached the success node. fail: If the graph or sub-graph reached the fail node. Please note that a failure of a node does not imply failure of the graph as you can configure conditional traversal of the nodes. processing: A temporary status if any of the nodes are currently being processed. triggered: A temporary status if any of the nodes triggered a remote job (in cloud, for example). parameters \u00b6 A dictionary of key-value pairs available to all the nodes. Any kwargs present in the function signature, called as part of the pipeline, are resolved against this dictionary and the values are set during runtime. steps \u00b6 steps is a dictionary containing step log for every individual step of the pipeline. The structure of step log is described below. Structure of Step Log \u00b6 Every step of the dag have a corresponding step log. The general structure follows, with a few explanations given inline. \"step name\" : { \"name\" : , # The na me o f t he s te p as give n i n t he dag de f i n i t io n \"internal_name\" : , # The na me o f t he s te p log i n do t pa t h co n ve nt io n \"status\" : , \"step_type\" : , # The t ype o f s te p as per t he dag de f i n i t io n \"message\" : , # A n y message added t o s te p by t he ru n \"mock\" : , # Is True i f t he s te p was skipped i n case o f a re - ru n \"code_identities\" : [ ], \"attempts\" : [ ], \"user_defined_metrics\" : { }, \"branches\" : {}, \"data_catalog\" : [] } Naming Step Log \u00b6 The name of the step log follows a convention, we refer, to as dot path convention. All the steps of the parent dag have the same exact name as the step name provided in the dag. The naming of the steps of the nested branches like parallel, map or dag are given below. parallel step \u00b6 The steps of the parallel branch follow parent_step.branch_name.child_step name. Example The step log names are given in-line for ease of reading. dag : start_at : Simple Step steps : Simple Step : # dot path name: Simple Step type : as-is next : Parallel Parallel : # dot path name: Parallel type : parallel next : Success branches : Branch A : start_at : Child Step A steps : Child Step A : # dot path name: Parallel.Branch A.Child Step A type : as-is next : Success Success : # dot path name: Parallel.Branch A.Success type : success Fail : # dot path name: Parallel.Branch A.Fail type : fail Branch B : start_at : Child Step B steps : Child Step B : # dot path name: Parallel.Branch B. Child Step B type : as-is next : Success Success : # dot path name: Parallel.Branch B.Success type : success Fail : # dot path name: Parallel.Branch B.Fail type : fail Success : # dot path name: Success type : success Fail : # dot path name: Fail type : fail dag step \u00b6 The steps of the dag branch follow parent_step.branch.child_step_name. Here branch is a special name given to keep the naming always consistent. Example The step log names are given in-line for ease of reading. dag : start_at : Simple Step steps : Simple Step : # dot path name: Simple Step type : as-is next : Dag Dag : # dot path name: Dag type : dag next : Success branch : steps : Child Step : # dot path name: Dag.branch.Child Step type : as-is next : Success Success : # dot path name: Dag.branch.Success type : success Fail : # dot path name: Dag.branch.Fail type : fail Success : # dot path name: Success type : success Fail : # dot path name: Fail type : fail map step \u00b6 The steps of the map branch follow parent_step.{value of iter_variable}.child_step_name. Example dag : start_at : Simple Step steps : Simple Step : # dot path name: Simple Step type : as-is next : Map Map : # dot path name: Map type : map iterate_on : y next : Success branch : steps : Child Step : type : as-is next : Success Success : type : success Fail : type : fail Success : # dot path name: Success type : success Fail : # dot path name: Fail type : fail If the value of parameter y turns out to be ['A', 'B'], the step log naming convention would by dynamic and have Map.A.Child Step, Map.A.Success, Map.A.Fail and Map.B.Child Step, Map.B.Success, Map.B.Fail status \u00b6 A flag to denote the status of the step. The status could be: success : If the step succeeded. fail: If the step failed. processing: A temporary status if current step is being processed. code identity \u00b6 As part of the log, magnus captures any possible identification of the state of the code and environment. This section is only present for Execution nodes. An example code identity if the code is git controlled \"code_identities\" : [ { \"code_identifier\" : \"1486bd7fbe27d57ff4a9612e8dabe6a914bc4eb5\" , # Gi t commi t id \"code_identifier_type\" : \"git\" , # Gi t \"code_identifier_dependable\" : true , # A fla g t o tra ck i f gi t tree is clea n \"code_identifier_url\" : \"ssh://git@##################.git\" , # The remo te URL o f t he repo \"code_identifier_message\" : \"\" # Lis ts all t he f iles t ha t were f ou n d t o be u n clea n as per gi t } ] If the execution was in a container, we also track the docker identity. For example: \"code_identities\" : [ { \"code_identifier\" : \"1486bd7fbe27d57ff4a9612e8dabe6a914bc4eb5\" , # Gi t commi t id \"code_identifier_type\" : \"git\" , # Gi t \"code_identifier_dependable\" : true , # A fla g t o tra ck i f gi t tree is clea n \"code_identifier_url\" : \"ssh://git@##################.git\" , # The remo te URL o f t he repo \"code_identifier_message\" : \"\" # Lis ts all t he f iles t ha t were f ou n d t o be u n clea n as per gi t }, { \"code_identifier\" : \"\" , # Docker image diges t \"code_identifier_type\" : \"docker\" , # Gi t \"code_identifier_dependable\" : true , # Always true as docker image id is depe n dable \"code_identifier_url\" : \"\" , # The docker regis tr y URL \"code_identifier_message\" : \"\" } ] attempts \u00b6 An attempt log capturing meta data about the attempt made to execute the node. This section is only present for Execution nodes. The structure of attempt log along with inline definitions \"attempts\" : [ { \"attempt_number\" : 0 , # The seque n ce nu mber o f a tte mp t . \"start_time\" : \"\" , # The s tart t ime o f t he a tte mp t \"end_time\" : \"\" , # The e n d t ime o f t he a tte mp t \"duration\" : null , # The dura t io n o f t he t ime ta ke n f or t he comma n d t o execu te \"status\" : \"\" , \"message\" : \"\" # I f a n y excep t io n was raised , t his f ield cap tures t he message o f t he excep t io n } ] The status of an attempt log could be one of: success : If the attempt succeeded. fail: If the attempt failed. user defined metrics \u00b6 As part of the execution, there is a provision to store metrics in the run log. These metrics would be stored in this section of the log. Example of storing metrics: # in my_module.py from magnus import track_this def my_cool_function (): track_this ( number_of_files = 102 , failed_for = 10 ) track_this ( number_of_incidents = { 'mean_value' : 2 , 'variance' : 0.1 }) If this function was executed as part of the pipeline, you should see the following in the run log { ... \"steps\" : { \"step name\" : { ... , \"number_of_incidents\" : { \"mean_value\" : 2 , \"variance\" : 0.1 }, \"number_of_files\" : 102 , \"failed_for\" : 10 }, ... \" }, ... } } The same could also be acheived without import magnus by exporting environment variables with prefix of MAGNUS_TRACK_ # in my_module.py import os import json def my_cool_function (): os . environ [ 'MAGNUS_TRACK_' + 'number_of_files' ] = 102 os . environ [ 'MAGNUS_TRACK_' + 'failed_for' ] = 10 os . environ [ 'MAGNUS_TRACK_' + 'number_of_incidents' ] = json . dumps ({ 'mean_value' : 2 , 'variance' : 0.1 }) branches \u00b6 If the step was a composite node of type dag or parallel or map, this section is used to store the logs of the branch which have a structure similar to the Run Log. data catalog \u00b6 Data generated as part of individual steps of the pipeline can use the catalog to make the data available for the downstream steps or for reproducibility of the run. The catalog metadata is stored here in this section. The structure of the data catalog is as follows with inline definition. \"data_catalog\" : [ { \"name\" : \"\" , # The na me o f t he f ile \"stored_at\" : \"\" , # The loca t io n a t which i t is s t ored \"data_hash\" : \"\" , # The SHA id o f t he da ta \"stage\" : \"\" # The s ta ge a t which t he da ta is ca tal oged. } ] More information about cataloging is found here. Configuration \u00b6 Configuration of a Run Log Store is as follows: run_log : type : config : type \u00b6 The type of run log provider you want. This should be one of the run log types already available. Buffered Run Log is provided as default if nothing is given. config \u00b6 Any configuration parameters the run log provider accepts. Parameterized definition \u00b6 As with any part of the magnus configuration, you can parameterize the configuration of Run Log to switch between Run Log providers without changing the base definition. Please follow the example provided here for more information. Extensions \u00b6 You can easily extend magnus to bring in your custom provider, if a default implementation does not exist or you are not happy with the implementation.","title":"Overview"},{"location":"concepts/run-log/#run_log","text":"In magnus, Run log captures all the information required to accurately describe a run. It should not confused with your application logs, which are project dependent. Independent of the providers of any systems (compute, secrets, run log, catalog), the structure of the run log would remain the same and should enable you to compare between runs. To accurately recreate an older run either for debugging purposes or for reproducibility, it should capture all the variables of the system and their state during the run. For the purple of data science applications, it boils down to: Data: The source of the data and the version of it. Code: The code used to run the the experiment and the version of it. Environment: The environment the code ran in with all the system installations. Configuration: The pipeline definition and the configuration. The Run Log helps in storing them systematically for every run with the best possible information on all of the above.","title":"Run Log"},{"location":"concepts/run-log/#structure_of_run_log","text":"A typical run log has the following structure, with a few definitions given inline. { \"run_id\" : , \"dag_hash\" : , # The SHA id o f t he dag de f i n i t io n \"use_cached\" : , # True f or a re - ru n , False o t herwise \"tag\" : , # A fr ie n dly na me give n t o a group o f ru ns \"original_run_id\" : , # The ru n id o f t he older ru n i n case o f a re - ru n \"status\" : , \"steps\" : { }, \"parameters\" : { } }","title":"Structure of Run Log"},{"location":"concepts/run-log/#structure_of_step_log","text":"Every step of the dag have a corresponding step log. The general structure follows, with a few explanations given inline. \"step name\" : { \"name\" : , # The na me o f t he s te p as give n i n t he dag de f i n i t io n \"internal_name\" : , # The na me o f t he s te p log i n do t pa t h co n ve nt io n \"status\" : , \"step_type\" : , # The t ype o f s te p as per t he dag de f i n i t io n \"message\" : , # A n y message added t o s te p by t he ru n \"mock\" : , # Is True i f t he s te p was skipped i n case o f a re - ru n \"code_identities\" : [ ], \"attempts\" : [ ], \"user_defined_metrics\" : { }, \"branches\" : {}, \"data_catalog\" : [] }","title":"Structure of Step Log"},{"location":"concepts/run-log/#configuration","text":"Configuration of a Run Log Store is as follows: run_log : type : config :","title":"Configuration"},{"location":"concepts/run-log/#parameterized_definition","text":"As with any part of the magnus configuration, you can parameterize the configuration of Run Log to switch between Run Log providers without changing the base definition. Please follow the example provided here for more information.","title":"Parameterized definition"},{"location":"concepts/run-log/#extensions","text":"You can easily extend magnus to bring in your custom provider, if a default implementation does not exist or you are not happy with the implementation.","title":"Extensions"},{"location":"concepts/secrets/","text":"Overview \u00b6 Secrets are essential in making your data science projects secure and collaborative. They could be database credentials, API keys or any information that need to present at the run-time but invisible at all other times.Magnus provides a clean interface to access/store secrets and independent of the actual secret provider, the interface remains the same. As with all modules of magnus, there are many secrets providers and if none fit your needs, it is easier to write one of your to fit your needs. In magnus, all secrets are key value pairs. Configuration \u00b6 Configuration of a Secrets is as follows: secrets : type : config : type \u00b6 The type of secrets provider you want. This should be one of the secrets types already available. There is no default secrets provider. config \u00b6 Any configuration parameters the secret provider accepts. Interaction with other services \u00b6 Other service providers, like run log store or catalog, can access the secrets by using the global_executor.secrets_handler of pipeline module during the run time. This could be useful for constructing connection strings to database or AWS connections. Interaction within code \u00b6 Secrets is the only implementation that requires you to import magnus in the code to access secrets. This is mostly to follow the best safety guidelines. Once a secret configuration is defined as above, you can access the secret by using get_secret of magnus. If a key is provided to the API, we return only the value associated with the secret by the key. If a key is not provided, we return all the key value secret pairs provided. The API would raise an exception if a secret by the key requested does not exist. Currently, there is no provision to update/edit secrets via code. For example if the secret key-value pairs are: secret_answer : 42 secret_question : everything And for the code: # In my_module.py from magnus import get_secret def my_cool_function (): secret = get_secret ( 'secret_answer' ) all_secrets = get_secret () secret would have a value of 42 while all_secrets would be a dictionary {'secret_answer': 42, 'secret_question': 'everything'} Parameterized definition \u00b6 As with any part of the magnus configuration, you can parameterize the configuration of secrets to switch between providers without changing the base definition. Please follow the example provided here for more information. Extensions \u00b6 You can easily extend magnus to bring in your custom provider, if a default implementation does not exist or you are not happy with the implementation.","title":"Overview"},{"location":"concepts/secrets/#overview","text":"Secrets are essential in making your data science projects secure and collaborative. They could be database credentials, API keys or any information that need to present at the run-time but invisible at all other times.Magnus provides a clean interface to access/store secrets and independent of the actual secret provider, the interface remains the same. As with all modules of magnus, there are many secrets providers and if none fit your needs, it is easier to write one of your to fit your needs. In magnus, all secrets are key value pairs.","title":"Overview"},{"location":"concepts/secrets/#configuration","text":"Configuration of a Secrets is as follows: secrets : type : config :","title":"Configuration"},{"location":"concepts/secrets/#interaction_with_other_services","text":"Other service providers, like run log store or catalog, can access the secrets by using the global_executor.secrets_handler of pipeline module during the run time. This could be useful for constructing connection strings to database or AWS connections.","title":"Interaction with other services"},{"location":"concepts/secrets/#interaction_within_code","text":"Secrets is the only implementation that requires you to import magnus in the code to access secrets. This is mostly to follow the best safety guidelines. Once a secret configuration is defined as above, you can access the secret by using get_secret of magnus. If a key is provided to the API, we return only the value associated with the secret by the key. If a key is not provided, we return all the key value secret pairs provided. The API would raise an exception if a secret by the key requested does not exist. Currently, there is no provision to update/edit secrets via code. For example if the secret key-value pairs are: secret_answer : 42 secret_question : everything And for the code: # In my_module.py from magnus import get_secret def my_cool_function (): secret = get_secret ( 'secret_answer' ) all_secrets = get_secret () secret would have a value of 42 while all_secrets would be a dictionary {'secret_answer': 42, 'secret_question': 'everything'}","title":"Interaction within code"},{"location":"concepts/secrets/#parameterized_definition","text":"As with any part of the magnus configuration, you can parameterize the configuration of secrets to switch between providers without changing the base definition. Please follow the example provided here for more information.","title":"Parameterized definition"},{"location":"concepts/secrets/#extensions","text":"You can easily extend magnus to bring in your custom provider, if a default implementation does not exist or you are not happy with the implementation.","title":"Extensions"},{"location":"concepts/catalog-implementations/do-nothing/","text":"Do nothing catalog provider \u00b6 Use this catalog provider if you do not want to use the cataloging functionality. The complete configuration: catalog : type : do-nothing dag : ... The individual steps could have get and put phases but since the catalog handler does nothing, these files would not be cataloged. Design thought \u00b6 Use this catalog type to temporarily switch of cataloging in local mode for debugging purposes.","title":"Do Nothing Catalog"},{"location":"concepts/catalog-implementations/do-nothing/#do_nothing_catalog_provider","text":"Use this catalog provider if you do not want to use the cataloging functionality. The complete configuration: catalog : type : do-nothing dag : ... The individual steps could have get and put phases but since the catalog handler does nothing, these files would not be cataloged.","title":"Do nothing catalog provider"},{"location":"concepts/catalog-implementations/do-nothing/#design_thought","text":"Use this catalog type to temporarily switch of cataloging in local mode for debugging purposes.","title":"Design thought"},{"location":"concepts/catalog-implementations/extensions/","text":"To extend and implement a custom catalog, you need to over-ride the appropriate methods of the Base class. Some of the methods of the BaseCatalog have default implementations and need not be over-written. Please refer to Guide to extensions for a detailed explanation and the need for implementing a Integration pattern along with the extension. Extensions that are being actively worked on and listed to be released as part of magnus-extensions s3 : Using s3 to store a catalog objects # You can find this in the source code at: magnus/catalog.py along with a few example # implementations of do-nothing and file-system class BaseCatalog : \"\"\" Base Catalog class definition. All implementations of the catalog handler should inherit and extend this class. Note: As a general guideline, do not extract anything from the config to set class level attributes. Integration patterns modify the config after init to change behaviors. Access config properties using getters/property of the class. \"\"\" service_name = '' def __init__ ( self , config , ** kwargs ): # pylint: disable=unused-argument self . config = config or {} @property def compute_data_folder ( self ) -> str : \"\"\" Returns the compute data folder defined as per the config of the catalog. Returns: [str]: The compute data folder as defined or defaults to magnus default 'data/' \"\"\" return self . config . get ( 'compute_data_folder' , defaults . COMPUTE_DATA_FOLDER ) def get ( self , name : str , run_id : str , compute_data_folder = None , ** kwargs ) -> List [ object ]: # pylint: disable=unused-argument \"\"\" Get the catalog item by 'name' for the 'run id' and store it in compute data folder. The catalog location should have been created before you can get from it. Args: name (str): The name of the catalog item run_id (str): The run_id of the run. compute_data_folder (str, optional): The compute data folder. Defaults to magnus default (data/) Raises: NotImplementedError: Base class, hence not implemented Returns: List(object) : A list of catalog objects \"\"\" raise NotImplementedError def put ( self , name : str , run_id : str , compute_data_folder = None , synced_catalogs = None , ** kwargs ) -> List [ object ]: # pylint: disable=unused-argument \"\"\" Put the file by 'name' from the 'compute_data_folder' in the catalog for the run_id. If previous syncing has happened and the file has not been changed, we do not sync again. Args: name (str): The name of the catalog item. run_id (str): The run_id of the run. compute_data_folder (str, optional): The compute data folder. Defaults to magnus default (data/) synced_catalogs (dict, optional): Any previously synced catalogs. Defaults to None. Raises: NotImplementedError: Base class, hence not implemented Returns: List(object) : A list of catalog objects \"\"\" raise NotImplementedError def sync_between_runs ( self , previous_run_id : str , run_id : str ): \"\"\" Given run_id of a previous run, sync them to the catalog of the run given by run_id Args: previous_run_id (str): The run id of the previous run run_id (str): The run_id to which the data catalogs should be synced to. Raises: NotImplementedError: Base class, hence not implemented \"\"\" raise NotImplementedError The custom extensions should be registered as part of the namespace: magnus.catalog.BaseCatalog for it to be loaded. # For example, as part of your pyproject.toml [tool.poetry.plugins.\"magnus.catalog.BaseCatalog\"] \"nfs\" = \"YOUR_PACKAGE:NFS\"","title":"Extensions"},{"location":"concepts/catalog-implementations/file-system/","text":"FileSystem \u00b6 This catalog provider uses local file system to store/retrieve the data generated by intermediate steps of the dag. The complete configuration: catalog : type : file-system config : compute_data_folder : catalog_location : dag : ... compute_data_folder \u00b6 Catalog would need a local compute data folder to get/put the contents. This is defaulted to data folder if nothing is provided. Individual steps of the dag could over-ride this global default. Example catalog : type : file-system config : compute_data_folder : data/ dag : start_at : Cool function steps : Cool function : type : task command : my_module.my_cool_function next : Success catalog : compute_data_folder : data/processed_data/ get : - '*' put : - '*' Success : type : success Fail : type : fail In this example, while the global default of the compute_data_folder is data/ , the step Cool function stored the generated data in data/processed_data/ and this would be used for cataloging. The same can also be achieved by using a glob pattern of processed_data/* if you prefer. catalog_location \u00b6 By default, the catalog would be stored at .catalog of the root directory of the project. You can override this by providing an alternate location. Example: catalog : type : file-system config : catalog_location : /tmp/data Would start using /tmp/data/ to catalog the data. Note FileSystem catalog is only applicable if all of the steps are on the same compute. To re-run an older run on a different compute, you can simply place the files in the catalog location of the re-run if its not centrally accessible.","title":"File System"},{"location":"concepts/catalog-implementations/file-system/#filesystem","text":"This catalog provider uses local file system to store/retrieve the data generated by intermediate steps of the dag. The complete configuration: catalog : type : file-system config : compute_data_folder : catalog_location : dag : ...","title":"FileSystem"},{"location":"concepts/catalog-implementations/file-system/#compute_data_folder","text":"Catalog would need a local compute data folder to get/put the contents. This is defaulted to data folder if nothing is provided. Individual steps of the dag could over-ride this global default. Example catalog : type : file-system config : compute_data_folder : data/ dag : start_at : Cool function steps : Cool function : type : task command : my_module.my_cool_function next : Success catalog : compute_data_folder : data/processed_data/ get : - '*' put : - '*' Success : type : success Fail : type : fail In this example, while the global default of the compute_data_folder is data/ , the step Cool function stored the generated data in data/processed_data/ and this would be used for cataloging. The same can also be achieved by using a glob pattern of processed_data/* if you prefer.","title":"compute_data_folder"},{"location":"concepts/catalog-implementations/file-system/#catalog_location","text":"By default, the catalog would be stored at .catalog of the root directory of the project. You can override this by providing an alternate location. Example: catalog : type : file-system config : catalog_location : /tmp/data Would start using /tmp/data/ to catalog the data. Note FileSystem catalog is only applicable if all of the steps are on the same compute. To re-run an older run on a different compute, you can simply place the files in the catalog location of the re-run if its not centrally accessible.","title":"catalog_location"},{"location":"concepts/modes-implementations/demo-renderer/","text":"Demo Renderer \u00b6 In this compute mode, we translate the dag into a bash script to demonstrate the idea of dag translation. Composite nodes like parallel , dag and map are not allowed as part of the definition. In this set up, we ignore max run time set on the dag completely. Configuration \u00b6 The full configuration of local mode is: mode : type : demo-renderer The parameters that have to be passed could be done either via environment variables prefixed by MAGNUS_PRM_ or by the command line like the example shown here .","title":"Demo Renderer"},{"location":"concepts/modes-implementations/demo-renderer/#demo_renderer","text":"In this compute mode, we translate the dag into a bash script to demonstrate the idea of dag translation. Composite nodes like parallel , dag and map are not allowed as part of the definition. In this set up, we ignore max run time set on the dag completely.","title":"Demo Renderer"},{"location":"concepts/modes-implementations/demo-renderer/#configuration","text":"The full configuration of local mode is: mode : type : demo-renderer The parameters that have to be passed could be done either via environment variables prefixed by MAGNUS_PRM_ or by the command line like the example shown here .","title":"Configuration"},{"location":"concepts/modes-implementations/extensions/","text":"To extend and implement a custom compute mode, you need to over-ride the appropriate methods of the Base class. Most of the methods of the BaseExecutor have default implementations and need not be over-written in a few situations. Please refer to Guide to extensions for a detailed explanation and the need for implementing a Integration pattern along with the extension along with understanding the right example for your extension. In summary, the extension will fall into one of the four possible possibilities: Magnus traverses, execution environment same as traversal. eg: local Magnus traverses, execution environment not same as traversal. eg: local-container Magnus does not traverse, execution environment not same as traversal. eg: demo-renderer Magnus does not traverse, execution environment same as traversal. eg: advanced use of as-is Extensions that are being actively worked on and listed to be released as part of magnus-extensions local-aws-batch : A decentralized AWS batch compute aws-step-function: Translates the dag into a Step function. Step over-rides \u00b6 Individual steps can provide custom config specific to a mode implementation by using mode_config block in the step definition. An example of such over-ride can be seen in local-container which over-rides the default global docker image to run. # You can find this in the source code at: magnus/executor.py along with a few example # implementations of local, local-container, demo-renderer class BaseExecutor : \"\"\" The skeleton of an executor class. Any implementation of an executor should inherit this class and over-ride accordingly. The logic of any dag execution is a play between three methods of this class. execute_graph: This method is responsible for traversing A graph. The core logic is start at the start_at the graph and traverse according to the state of the execution. execute_graph hands over the actual execution of the node to self.execute_from_graph Helper method: prepare_for_graph_execution would be called prior to calling execute_graph. Use it to modify settings if needed. execute_from_graph: This method is responsible for executing a node. But given that a node itself could be a dag in cases of parallel, map and dag, this method handles the cases. If the node is of type task, success, fail: we can pretty much execute it and we call self.trigger_job. If the node is of type composite: We call the node's execute_as_graph function which internally triggers execute_graph in-turn iteratively traverses the graph. execute_node: This method is where the actual execution of the work happens. This method is already in the compute environment of the mode. Use prepare_node_for_execution to adjust settings. The base class is given an implementation and in most cases should not be touched. Helper method: prepare_for_node_execution would be called prior to calling execute_node. Use it to modify settings if needed The above logic holds good when we are in interactive compute mode i.e. local, local-container, local-aws-batch But in 3rd party orchestration mode, we might have to render the job specifications and the roles might be different Please see the implementations of local, local-container, local-aws-batch to perform interactive compute. And demo-renderer to see an example of what a 3rd party executor looks like. \"\"\" service_name = '' def __init__ ( self , config ): # pylint: disable=R0914,R0913 self . config = config # The remaining would be attached later self . pipeline_file = None self . variables_file = None self . run_id = None self . dag = None self . use_cached = None self . tag = None self . run_log_store = None self . previous_run_log = None self . dag_hash = None self . catalog_handler = None self . secrets_handler = None self . variables_file = None self . configuration_file = None self . parameters_file = None def is_parallel_execution ( self ) -> bool : # pylint: disable=R0201 \"\"\" Controls the parallelization of branches in map and parallel state. Defaults to False and left for the compute modes to decide. Returns: bool: True if the mode allows parallel execution of branches. \"\"\" return defaults . ENABLE_PARALLEL def set_up_run_log ( self ): \"\"\" Create a run log and put that in the run log store \"\"\" run_log = self . run_log_store . create_run_log ( self . run_id ) run_log . tag = self . tag run_log . use_cached = False run_log . status = defaults . PROCESSING run_log . dag_hash = self . dag_hash parameters = {} if self . parameters_file : parameters = utils . load_yaml ( self . parameters_file ) if self . previous_run_log : run_log . original_run_id = self . previous_run_log . run_id # Sync the previous run log catalog to this one. self . catalog_handler . sync_between_runs ( previous_run_id = run_log . original_run_id , run_id = self . run_id ) run_log . use_cached = True parameters . update ( self . previous_run_log . parameters ) run_log . parameters = parameters # Update run_config run_log . run_config = utils . get_run_config ( self ) self . run_log_store . put_run_log ( run_log ) def prepare_for_graph_execution ( self ): \"\"\" This method would be called prior to calling execute_graph. Perform any steps required before doing the graph execution. The most common implementation is to prepare a run log for the run if the run uses local interactive compute. But in cases of actual rendering the job specs (eg: AWS step functions, K8's) we need not do anything. \"\"\" integration . validate ( self , self . run_log_store ) integration . configure_for_traversal ( self , self . run_log_store ) integration . validate ( self , self . catalog_handler ) integration . configure_for_traversal ( self , self . catalog_handler ) integration . validate ( self , self . secrets_handler ) integration . configure_for_traversal ( self , self . secrets_handler ) self . set_up_run_log () def prepare_for_node_execution ( self , node : BaseNode , map_variable : dict = None ): \"\"\" Perform any modifications to the services prior to execution of the node. Args: node (Node): [description] map_variable (dict, optional): [description]. Defaults to None. \"\"\" integration . validate ( self , self . run_log_store ) integration . configure_for_execution ( self , self . run_log_store ) integration . validate ( self , self . catalog_handler ) integration . configure_for_execution ( self , self . catalog_handler ) integration . validate ( self , self . secrets_handler ) integration . configure_for_execution ( self , self . secrets_handler ) def sync_catalog ( self , node : BaseNode , step_log : datastore . StepLog , stage : str , synced_catalogs = None ): \"\"\" 1). Identify the catalog settings by over-riding node settings with the global settings. 2). For stage = get: Identify the catalog items that are being asked to get from the catalog And copy them to the local compute data folder 3). For stage = put: Identify the catalog items that are being asked to put into the catalog Copy the items from local compute folder to the catalog 4). Add the items onto the step log according to the stage Args: node (Node): The current node being processed step_log (datastore.StepLog): The step log corresponding to that node stage (str): One of get or put \"\"\" if stage not in [ 'get' , 'put' ]: msg = ( 'Catalog service only accepts get/put possible actions as part of node execution.' f 'Sync catalog of the executor: { self . service_name } asks for { stage } which is not accepted' ) raise Exception ( msg ) node_catalog_settings = node . get_catalog_settings () if not ( node_catalog_settings and stage in node_catalog_settings ): # Nothing to get/put from the catalog return None # Local compute data folder over rides the global one compute_data_folder = self . catalog_handler . compute_data_folder if 'compute_data_folder' in node_catalog_settings and node_catalog_settings [ 'compute_data_folder' ]: compute_data_folder = node_catalog_settings [ 'compute_data_folder' ] data_catalogs = [] for name_pattern in node_catalog_settings . get ( stage ) or []: # Assumes a list data_catalogs = getattr ( self . catalog_handler , stage )( name = name_pattern , run_id = self . run_id , compute_data_folder = compute_data_folder , synced_catalogs = synced_catalogs ) if data_catalogs : step_log . add_data_catalogs ( data_catalogs ) return data_catalogs def execute_node ( self , node : BaseNode , map_variable : dict = None , ** kwargs ): \"\"\" This is the entry point when we do the actual execution of the function. Over-ride this function to do what the executor has to do for the actual function call Most frequently, this core logic should not be touched either in interactive mode or 3rd party orchestration mode. While in interactive mode, we just compute, in 3rd party interactive mode, we call this function from the CLI In most cases, * We get the corresponding step_log of the node and the parameters. * We sync the catalog to GET any data sets that are in the catalog * We call the execute method of the node for the actual compute and retry it as many times as asked. * If the node succeeds, we get any of the user defined metrics provided by the user. * We sync the catalog to PUT any data sets that are in the catalog. Args: node (Node): The node to execute map_variable (dict, optional): If the node is of a map state, map_variable is the value of the iterable. Defaults to None. \"\"\" max_attempts = node . get_max_attempts () attempts = 0 step_log = self . run_log_store . get_step_log ( node . get_step_log_name ( map_variable ), self . run_id ) parameters = self . run_log_store . get_parameters ( run_id = self . run_id ) interaction . store_parameter ( ** parameters ) data_catalogs_get = self . sync_catalog ( node , step_log , stage = 'get' ) mock = step_log . mock logger . info ( f 'Trying to execute node: { node . internal_name } , attempt : { attempts } , max_attempts: { max_attempts } ' ) while attempts < max_attempts : try : attempt_log = node . execute ( executor = self , mock = mock , map_variable = map_variable , ** kwargs ) attempt_log . attempt_number = attempts step_log . attempts . append ( attempt_log ) if attempt_log . status == defaults . FAIL : raise Exception () step_log . status = defaults . SUCCESS step_log . user_defined_metrics = utils . get_tracked_data () self . run_log_store . set_parameters ( self . run_id , utils . get_user_set_parameters ( remove = True )) break except Exception as _e : # pylint: disable=W0703 attempts += 1 logger . exception ( f 'Node: { node } failed with exception { _e } ' ) # Remove any steps data utils . get_tracked_data () utils . get_user_set_parameters ( remove = True ) if attempts == max_attempts : step_log . status = defaults . FAIL logger . error ( f 'Node { node } failed, max retries of { max_attempts } reached' ) self . sync_catalog ( node , step_log , stage = 'put' , synced_catalogs = data_catalogs_get ) self . run_log_store . add_step_log ( step_log , self . run_id ) def add_code_identities ( self , node : BaseNode , step_log : datastore . StepLog , ** kwargs ): \"\"\" Add code identities specific to the implementation. The Base class has an implementation of adding git code identities. Args: step_log (object): The step log object node (BaseNode): The node we are adding the step log for \"\"\" step_log . code_identities . append ( utils . get_git_code_identity ( self . run_log_store )) def execute_from_graph ( self , node : BaseNode , map_variable : dict = None , ** kwargs ): \"\"\" This is the entry point to from the graph execution. While the self.execute_graph is responsible for traversing the graph, this function is responsible for actual execution of the node. If the node type is: * task : We can delegate to execute_node after checking the eligibility for re-run in cases of a re-run * success: We can delegate to execute_node * fail: We can delegate to execute_node For nodes that are internally graphs: * parallel: Delegate the responsibility of execution to the node.execute_as_graph() * dag: Delegate the responsibility of execution to the node.execute_as_graph() * map: Delegate the responsibility of execution to the node.execute_as_graph() Check the implementations of local, local-container, local-aws-batch for different examples of implementation Args: node (Node): The node to execute map_variable (dict, optional): If the node if of a map state, this corresponds to the value of iterable. Defaults to None. \"\"\" step_log = self . run_log_store . create_step_log ( node . name , node . get_step_log_name ( map_variable )) self . add_code_identities ( node = node , step_log = step_log ) step_log . step_type = node . node_type step_log . status = defaults . PROCESSING # Add the step log to the database as per the situation. # If its a terminal node, complete it now if node . node_type in [ 'success' , 'fail' ]: self . run_log_store . add_step_log ( step_log , self . run_id ) self . execute_node ( node , map_variable = map_variable , ** kwargs ) return # If previous run was successful, move on to the next step if not self . is_eligible_for_rerun ( node , map_variable = map_variable ): step_log . mock = True step_log . status = defaults . SUCCESS self . run_log_store . add_step_log ( step_log , self . run_id ) return # We call an internal function to iterate the sub graphs and execute them if node . is_composite : self . run_log_store . add_step_log ( step_log , self . run_id ) node . execute_as_graph ( self , map_variable = map_variable , ** kwargs ) return # Executor specific way to trigger a job self . run_log_store . add_step_log ( step_log , self . run_id ) self . trigger_job ( node = node , map_variable = map_variable , ** kwargs ) def trigger_job ( self , node : BaseNode , map_variable : dict = None , ** kwargs ): \"\"\" Executor specific way of triggering jobs. Args: node (BaseNode): The node to execute map_variable (str, optional): If the node if of a map state, this corresponds to the value of iterable. Defaults to ''. Raises: NotImplementedError Base class hence not implemented \"\"\" raise NotImplementedError def get_status_and_next_node_name ( self , current_node : BaseNode , dag : Graph , map_variable : dict = None ): \"\"\" Given the current node and the graph, returns the name of the next node to execute. The name is always relative the graph that the node resides in. If the current node succeeded, we return the next node as per the graph. If the current node failed, we return the on failure node of the node (if provided) or the global one. Args: current_node (BaseNode): The current node. dag (Graph): The dag we are traversing. map_variable (dict): If the node belongs to a map branch. \"\"\" step_log = self . run_log_store . get_step_log ( current_node . get_step_log_name ( map_variable ), self . run_id ) logger . info ( f 'Finished executing the node { current_node } with status { step_log . status } ' ) next_node_name = current_node . get_next_node () if step_log . status == defaults . FAIL : next_node_name = dag . get_fail_node () . name if current_node . get_on_failure_node (): next_node_name = current_node . get_on_failure_node () return step_log . status , next_node_name def execute_graph ( self , dag : Graph , map_variable : dict = None , ** kwargs ): \"\"\" The parallelization is controlled by the nodes and not by this function. Logically the method should: * Start at the dag.start_at of the dag. * Call the self.execute_from_graph(node) * depending upon the status of the execution, either move to the success node or failure node. Args: dag (Graph): The directed acyclic graph to traverse and execute. map_variable (dict, optional): If the node if of a map state, this corresponds to the value of the iterable. Defaults to None. \"\"\" current_node = dag . start_at previous_node = None logger . info ( f 'Running the execution with { current_node } ' ) while True : working_on = dag . get_node_by_name ( current_node ) if previous_node == current_node : raise Exception ( 'Potentially running in a infinite loop' ) previous_node = current_node logger . info ( f 'Creating execution log for { working_on } ' ) self . execute_from_graph ( working_on , map_variable = map_variable , ** kwargs ) status , next_node_name = self . get_status_and_next_node_name ( current_node = working_on , dag = dag , map_variable = map_variable ) if status == defaults . TRIGGERED : # Some nodes go into triggered state and self traverse logger . info ( f 'Triggered the job to execute the node { current_node } ' ) break if working_on . node_type in [ 'success' , 'fail' ]: break current_node = next_node_name run_log = self . run_log_store . get_branch_log ( working_on . get_branch_log_name ( map_variable ), self . run_id ) branch = 'graph' if working_on . internal_branch_name : branch = working_on . internal_branch_name logger . info ( f 'Finished execution of the { branch } with status { run_log . status } ' ) print ( json . dumps ( run_log . dict (), indent = 4 )) def is_eligible_for_rerun ( self , node : BaseNode , map_variable : dict = None ): \"\"\" In case of a re-run, this method checks to see if the previous run step status to determine if a re-run is necessary. * True: If its not a re-run. * True: If its a re-run and we failed in the last run or the corresponding logs do not exist. * False: If its a re-run and we succeeded in the last run. Most cases, this logic need not be touched Args: node (Node): The node to check against re-run map_variable (dict, optional): If the node if of a map state, this corresponds to the value of iterable.. Defaults to None. Returns: bool: Eligibility for re-run. True means re-run, False means skip to the next step. \"\"\" if self . previous_run_log : node_step_log_name = node . get_step_log_name ( map_variable = map_variable ) logger . info ( f 'Scanning previous run logs for node logs of: { node_step_log_name } ' ) previous_node_log = None try : previous_node_log , _ = self . previous_run_log . search_step_by_internal_name ( node_step_log_name ) except exceptions . StepLogNotFoundError : logger . warning ( f 'Did not find the node { node . name } in previous run log' ) return True # We should re-run the node. step_log = self . run_log_store . get_step_log ( node . get_step_log_name ( map_variable ), self . run_id ) logger . info ( f 'The original step status: { previous_node_log . status } ' ) if previous_node_log . status == defaults . SUCCESS : logger . info ( f 'The step { node . name } is marked success, not executing it' ) step_log . status = defaults . SUCCESS step_log . message = 'Node execution successful in previous run, skipping it' self . run_log_store . add_step_log ( step_log , self . run_id ) return False # We need not run the node # Remove previous run log to start execution from this step logger . info ( f 'The new execution should start executing graph from this node { node . name } ' ) self . previous_run_log = None return True def send_return_code ( self , stage = 'traversal' ): \"\"\" Convenience function used by pipeline to send return code to the caller of the cli Raises: Exception: If the pipeline execution failed \"\"\" run_id = self . run_id run_log = self . run_log_store . get_run_log_by_id ( run_id = run_id , full = False ) if run_log . status == defaults . FAIL : raise Exception ( 'Pipeline execution failed' ) def resolve_node_config ( self , node : BaseNode ): \"\"\" The mode_config section can contain specific over-rides to an global executor config. To avoid too much clutter in the dag definition, we allow the configuration file to have placeholders block. The nodes can over-ride the global config by referring to key in the placeholder. For example: # configuration.yaml mode: type: cloud-implementation config: k1: v1 k3: v3 placeholders: k2: v2 # Could be a mapping internally. # in pipeline definition.yaml dag: steps: step1: mode_config: cloud-implementation: k1: value_specific_to_node k2: This method should resolve the node_config to {'k1': 'value_specific_to_node', 'k2': 'v2', 'k3': 'v3'} Args: node (BaseNode): The current node being processed. \"\"\" effective_node_config = copy . deepcopy ( self . config ) ctx_node_config = node . get_mode_config ( self . service_name ) placeholders = self . config . get ( 'placeholders' , None ) for key , value in ctx_node_config . items (): if not value : if key in placeholders : # Update via placeholder only if value is None try : effective_node_config . update ( placeholders [ key ]) except TypeError : logger . error ( f 'Expected value to the { key } to be a mapping but found { type ( placeholders [ key ]) } ' ) continue logger . info ( f \"For key: { key } in the { node . name } mode_config, there is no value provided and no \\ corresponding placeholder was found\" ) effective_node_config [ key ] = value effective_node_config . pop ( 'placeholders' , None ) return effective_node_config The custom extensions should be registered as part of the namespace: magnus.executor.BaseExecutor for it to be loaded. # For example, as part of your pyproject.toml [tool.poetry.plugins.\"magnus.executor.BaseExecutor\"] \"gcp\" = \"YOUR_PACKAGE:GCP\"","title":"Extensions"},{"location":"concepts/modes-implementations/extensions/#step_over-rides","text":"Individual steps can provide custom config specific to a mode implementation by using mode_config block in the step definition. An example of such over-ride can be seen in local-container which over-rides the default global docker image to run. # You can find this in the source code at: magnus/executor.py along with a few example # implementations of local, local-container, demo-renderer class BaseExecutor : \"\"\" The skeleton of an executor class. Any implementation of an executor should inherit this class and over-ride accordingly. The logic of any dag execution is a play between three methods of this class. execute_graph: This method is responsible for traversing A graph. The core logic is start at the start_at the graph and traverse according to the state of the execution. execute_graph hands over the actual execution of the node to self.execute_from_graph Helper method: prepare_for_graph_execution would be called prior to calling execute_graph. Use it to modify settings if needed. execute_from_graph: This method is responsible for executing a node. But given that a node itself could be a dag in cases of parallel, map and dag, this method handles the cases. If the node is of type task, success, fail: we can pretty much execute it and we call self.trigger_job. If the node is of type composite: We call the node's execute_as_graph function which internally triggers execute_graph in-turn iteratively traverses the graph. execute_node: This method is where the actual execution of the work happens. This method is already in the compute environment of the mode. Use prepare_node_for_execution to adjust settings. The base class is given an implementation and in most cases should not be touched. Helper method: prepare_for_node_execution would be called prior to calling execute_node. Use it to modify settings if needed The above logic holds good when we are in interactive compute mode i.e. local, local-container, local-aws-batch But in 3rd party orchestration mode, we might have to render the job specifications and the roles might be different Please see the implementations of local, local-container, local-aws-batch to perform interactive compute. And demo-renderer to see an example of what a 3rd party executor looks like. \"\"\" service_name = '' def __init__ ( self , config ): # pylint: disable=R0914,R0913 self . config = config # The remaining would be attached later self . pipeline_file = None self . variables_file = None self . run_id = None self . dag = None self . use_cached = None self . tag = None self . run_log_store = None self . previous_run_log = None self . dag_hash = None self . catalog_handler = None self . secrets_handler = None self . variables_file = None self . configuration_file = None self . parameters_file = None def is_parallel_execution ( self ) -> bool : # pylint: disable=R0201 \"\"\" Controls the parallelization of branches in map and parallel state. Defaults to False and left for the compute modes to decide. Returns: bool: True if the mode allows parallel execution of branches. \"\"\" return defaults . ENABLE_PARALLEL def set_up_run_log ( self ): \"\"\" Create a run log and put that in the run log store \"\"\" run_log = self . run_log_store . create_run_log ( self . run_id ) run_log . tag = self . tag run_log . use_cached = False run_log . status = defaults . PROCESSING run_log . dag_hash = self . dag_hash parameters = {} if self . parameters_file : parameters = utils . load_yaml ( self . parameters_file ) if self . previous_run_log : run_log . original_run_id = self . previous_run_log . run_id # Sync the previous run log catalog to this one. self . catalog_handler . sync_between_runs ( previous_run_id = run_log . original_run_id , run_id = self . run_id ) run_log . use_cached = True parameters . update ( self . previous_run_log . parameters ) run_log . parameters = parameters # Update run_config run_log . run_config = utils . get_run_config ( self ) self . run_log_store . put_run_log ( run_log ) def prepare_for_graph_execution ( self ): \"\"\" This method would be called prior to calling execute_graph. Perform any steps required before doing the graph execution. The most common implementation is to prepare a run log for the run if the run uses local interactive compute. But in cases of actual rendering the job specs (eg: AWS step functions, K8's) we need not do anything. \"\"\" integration . validate ( self , self . run_log_store ) integration . configure_for_traversal ( self , self . run_log_store ) integration . validate ( self , self . catalog_handler ) integration . configure_for_traversal ( self , self . catalog_handler ) integration . validate ( self , self . secrets_handler ) integration . configure_for_traversal ( self , self . secrets_handler ) self . set_up_run_log () def prepare_for_node_execution ( self , node : BaseNode , map_variable : dict = None ): \"\"\" Perform any modifications to the services prior to execution of the node. Args: node (Node): [description] map_variable (dict, optional): [description]. Defaults to None. \"\"\" integration . validate ( self , self . run_log_store ) integration . configure_for_execution ( self , self . run_log_store ) integration . validate ( self , self . catalog_handler ) integration . configure_for_execution ( self , self . catalog_handler ) integration . validate ( self , self . secrets_handler ) integration . configure_for_execution ( self , self . secrets_handler ) def sync_catalog ( self , node : BaseNode , step_log : datastore . StepLog , stage : str , synced_catalogs = None ): \"\"\" 1). Identify the catalog settings by over-riding node settings with the global settings. 2). For stage = get: Identify the catalog items that are being asked to get from the catalog And copy them to the local compute data folder 3). For stage = put: Identify the catalog items that are being asked to put into the catalog Copy the items from local compute folder to the catalog 4). Add the items onto the step log according to the stage Args: node (Node): The current node being processed step_log (datastore.StepLog): The step log corresponding to that node stage (str): One of get or put \"\"\" if stage not in [ 'get' , 'put' ]: msg = ( 'Catalog service only accepts get/put possible actions as part of node execution.' f 'Sync catalog of the executor: { self . service_name } asks for { stage } which is not accepted' ) raise Exception ( msg ) node_catalog_settings = node . get_catalog_settings () if not ( node_catalog_settings and stage in node_catalog_settings ): # Nothing to get/put from the catalog return None # Local compute data folder over rides the global one compute_data_folder = self . catalog_handler . compute_data_folder if 'compute_data_folder' in node_catalog_settings and node_catalog_settings [ 'compute_data_folder' ]: compute_data_folder = node_catalog_settings [ 'compute_data_folder' ] data_catalogs = [] for name_pattern in node_catalog_settings . get ( stage ) or []: # Assumes a list data_catalogs = getattr ( self . catalog_handler , stage )( name = name_pattern , run_id = self . run_id , compute_data_folder = compute_data_folder , synced_catalogs = synced_catalogs ) if data_catalogs : step_log . add_data_catalogs ( data_catalogs ) return data_catalogs def execute_node ( self , node : BaseNode , map_variable : dict = None , ** kwargs ): \"\"\" This is the entry point when we do the actual execution of the function. Over-ride this function to do what the executor has to do for the actual function call Most frequently, this core logic should not be touched either in interactive mode or 3rd party orchestration mode. While in interactive mode, we just compute, in 3rd party interactive mode, we call this function from the CLI In most cases, * We get the corresponding step_log of the node and the parameters. * We sync the catalog to GET any data sets that are in the catalog * We call the execute method of the node for the actual compute and retry it as many times as asked. * If the node succeeds, we get any of the user defined metrics provided by the user. * We sync the catalog to PUT any data sets that are in the catalog. Args: node (Node): The node to execute map_variable (dict, optional): If the node is of a map state, map_variable is the value of the iterable. Defaults to None. \"\"\" max_attempts = node . get_max_attempts () attempts = 0 step_log = self . run_log_store . get_step_log ( node . get_step_log_name ( map_variable ), self . run_id ) parameters = self . run_log_store . get_parameters ( run_id = self . run_id ) interaction . store_parameter ( ** parameters ) data_catalogs_get = self . sync_catalog ( node , step_log , stage = 'get' ) mock = step_log . mock logger . info ( f 'Trying to execute node: { node . internal_name } , attempt : { attempts } , max_attempts: { max_attempts } ' ) while attempts < max_attempts : try : attempt_log = node . execute ( executor = self , mock = mock , map_variable = map_variable , ** kwargs ) attempt_log . attempt_number = attempts step_log . attempts . append ( attempt_log ) if attempt_log . status == defaults . FAIL : raise Exception () step_log . status = defaults . SUCCESS step_log . user_defined_metrics = utils . get_tracked_data () self . run_log_store . set_parameters ( self . run_id , utils . get_user_set_parameters ( remove = True )) break except Exception as _e : # pylint: disable=W0703 attempts += 1 logger . exception ( f 'Node: { node } failed with exception { _e } ' ) # Remove any steps data utils . get_tracked_data () utils . get_user_set_parameters ( remove = True ) if attempts == max_attempts : step_log . status = defaults . FAIL logger . error ( f 'Node { node } failed, max retries of { max_attempts } reached' ) self . sync_catalog ( node , step_log , stage = 'put' , synced_catalogs = data_catalogs_get ) self . run_log_store . add_step_log ( step_log , self . run_id ) def add_code_identities ( self , node : BaseNode , step_log : datastore . StepLog , ** kwargs ): \"\"\" Add code identities specific to the implementation. The Base class has an implementation of adding git code identities. Args: step_log (object): The step log object node (BaseNode): The node we are adding the step log for \"\"\" step_log . code_identities . append ( utils . get_git_code_identity ( self . run_log_store )) def execute_from_graph ( self , node : BaseNode , map_variable : dict = None , ** kwargs ): \"\"\" This is the entry point to from the graph execution. While the self.execute_graph is responsible for traversing the graph, this function is responsible for actual execution of the node. If the node type is: * task : We can delegate to execute_node after checking the eligibility for re-run in cases of a re-run * success: We can delegate to execute_node * fail: We can delegate to execute_node For nodes that are internally graphs: * parallel: Delegate the responsibility of execution to the node.execute_as_graph() * dag: Delegate the responsibility of execution to the node.execute_as_graph() * map: Delegate the responsibility of execution to the node.execute_as_graph() Check the implementations of local, local-container, local-aws-batch for different examples of implementation Args: node (Node): The node to execute map_variable (dict, optional): If the node if of a map state, this corresponds to the value of iterable. Defaults to None. \"\"\" step_log = self . run_log_store . create_step_log ( node . name , node . get_step_log_name ( map_variable )) self . add_code_identities ( node = node , step_log = step_log ) step_log . step_type = node . node_type step_log . status = defaults . PROCESSING # Add the step log to the database as per the situation. # If its a terminal node, complete it now if node . node_type in [ 'success' , 'fail' ]: self . run_log_store . add_step_log ( step_log , self . run_id ) self . execute_node ( node , map_variable = map_variable , ** kwargs ) return # If previous run was successful, move on to the next step if not self . is_eligible_for_rerun ( node , map_variable = map_variable ): step_log . mock = True step_log . status = defaults . SUCCESS self . run_log_store . add_step_log ( step_log , self . run_id ) return # We call an internal function to iterate the sub graphs and execute them if node . is_composite : self . run_log_store . add_step_log ( step_log , self . run_id ) node . execute_as_graph ( self , map_variable = map_variable , ** kwargs ) return # Executor specific way to trigger a job self . run_log_store . add_step_log ( step_log , self . run_id ) self . trigger_job ( node = node , map_variable = map_variable , ** kwargs ) def trigger_job ( self , node : BaseNode , map_variable : dict = None , ** kwargs ): \"\"\" Executor specific way of triggering jobs. Args: node (BaseNode): The node to execute map_variable (str, optional): If the node if of a map state, this corresponds to the value of iterable. Defaults to ''. Raises: NotImplementedError Base class hence not implemented \"\"\" raise NotImplementedError def get_status_and_next_node_name ( self , current_node : BaseNode , dag : Graph , map_variable : dict = None ): \"\"\" Given the current node and the graph, returns the name of the next node to execute. The name is always relative the graph that the node resides in. If the current node succeeded, we return the next node as per the graph. If the current node failed, we return the on failure node of the node (if provided) or the global one. Args: current_node (BaseNode): The current node. dag (Graph): The dag we are traversing. map_variable (dict): If the node belongs to a map branch. \"\"\" step_log = self . run_log_store . get_step_log ( current_node . get_step_log_name ( map_variable ), self . run_id ) logger . info ( f 'Finished executing the node { current_node } with status { step_log . status } ' ) next_node_name = current_node . get_next_node () if step_log . status == defaults . FAIL : next_node_name = dag . get_fail_node () . name if current_node . get_on_failure_node (): next_node_name = current_node . get_on_failure_node () return step_log . status , next_node_name def execute_graph ( self , dag : Graph , map_variable : dict = None , ** kwargs ): \"\"\" The parallelization is controlled by the nodes and not by this function. Logically the method should: * Start at the dag.start_at of the dag. * Call the self.execute_from_graph(node) * depending upon the status of the execution, either move to the success node or failure node. Args: dag (Graph): The directed acyclic graph to traverse and execute. map_variable (dict, optional): If the node if of a map state, this corresponds to the value of the iterable. Defaults to None. \"\"\" current_node = dag . start_at previous_node = None logger . info ( f 'Running the execution with { current_node } ' ) while True : working_on = dag . get_node_by_name ( current_node ) if previous_node == current_node : raise Exception ( 'Potentially running in a infinite loop' ) previous_node = current_node logger . info ( f 'Creating execution log for { working_on } ' ) self . execute_from_graph ( working_on , map_variable = map_variable , ** kwargs ) status , next_node_name = self . get_status_and_next_node_name ( current_node = working_on , dag = dag , map_variable = map_variable ) if status == defaults . TRIGGERED : # Some nodes go into triggered state and self traverse logger . info ( f 'Triggered the job to execute the node { current_node } ' ) break if working_on . node_type in [ 'success' , 'fail' ]: break current_node = next_node_name run_log = self . run_log_store . get_branch_log ( working_on . get_branch_log_name ( map_variable ), self . run_id ) branch = 'graph' if working_on . internal_branch_name : branch = working_on . internal_branch_name logger . info ( f 'Finished execution of the { branch } with status { run_log . status } ' ) print ( json . dumps ( run_log . dict (), indent = 4 )) def is_eligible_for_rerun ( self , node : BaseNode , map_variable : dict = None ): \"\"\" In case of a re-run, this method checks to see if the previous run step status to determine if a re-run is necessary. * True: If its not a re-run. * True: If its a re-run and we failed in the last run or the corresponding logs do not exist. * False: If its a re-run and we succeeded in the last run. Most cases, this logic need not be touched Args: node (Node): The node to check against re-run map_variable (dict, optional): If the node if of a map state, this corresponds to the value of iterable.. Defaults to None. Returns: bool: Eligibility for re-run. True means re-run, False means skip to the next step. \"\"\" if self . previous_run_log : node_step_log_name = node . get_step_log_name ( map_variable = map_variable ) logger . info ( f 'Scanning previous run logs for node logs of: { node_step_log_name } ' ) previous_node_log = None try : previous_node_log , _ = self . previous_run_log . search_step_by_internal_name ( node_step_log_name ) except exceptions . StepLogNotFoundError : logger . warning ( f 'Did not find the node { node . name } in previous run log' ) return True # We should re-run the node. step_log = self . run_log_store . get_step_log ( node . get_step_log_name ( map_variable ), self . run_id ) logger . info ( f 'The original step status: { previous_node_log . status } ' ) if previous_node_log . status == defaults . SUCCESS : logger . info ( f 'The step { node . name } is marked success, not executing it' ) step_log . status = defaults . SUCCESS step_log . message = 'Node execution successful in previous run, skipping it' self . run_log_store . add_step_log ( step_log , self . run_id ) return False # We need not run the node # Remove previous run log to start execution from this step logger . info ( f 'The new execution should start executing graph from this node { node . name } ' ) self . previous_run_log = None return True def send_return_code ( self , stage = 'traversal' ): \"\"\" Convenience function used by pipeline to send return code to the caller of the cli Raises: Exception: If the pipeline execution failed \"\"\" run_id = self . run_id run_log = self . run_log_store . get_run_log_by_id ( run_id = run_id , full = False ) if run_log . status == defaults . FAIL : raise Exception ( 'Pipeline execution failed' ) def resolve_node_config ( self , node : BaseNode ): \"\"\" The mode_config section can contain specific over-rides to an global executor config. To avoid too much clutter in the dag definition, we allow the configuration file to have placeholders block. The nodes can over-ride the global config by referring to key in the placeholder. For example: # configuration.yaml mode: type: cloud-implementation config: k1: v1 k3: v3 placeholders: k2: v2 # Could be a mapping internally. # in pipeline definition.yaml dag: steps: step1: mode_config: cloud-implementation: k1: value_specific_to_node k2: This method should resolve the node_config to {'k1': 'value_specific_to_node', 'k2': 'v2', 'k3': 'v3'} Args: node (BaseNode): The current node being processed. \"\"\" effective_node_config = copy . deepcopy ( self . config ) ctx_node_config = node . get_mode_config ( self . service_name ) placeholders = self . config . get ( 'placeholders' , None ) for key , value in ctx_node_config . items (): if not value : if key in placeholders : # Update via placeholder only if value is None try : effective_node_config . update ( placeholders [ key ]) except TypeError : logger . error ( f 'Expected value to the { key } to be a mapping but found { type ( placeholders [ key ]) } ' ) continue logger . info ( f \"For key: { key } in the { node . name } mode_config, there is no value provided and no \\ corresponding placeholder was found\" ) effective_node_config [ key ] = value effective_node_config . pop ( 'placeholders' , None ) return effective_node_config The custom extensions should be registered as part of the namespace: magnus.executor.BaseExecutor for it to be loaded. # For example, as part of your pyproject.toml [tool.poetry.plugins.\"magnus.executor.BaseExecutor\"] \"gcp\" = \"YOUR_PACKAGE:GCP\"","title":"Step over-rides"},{"location":"concepts/modes-implementations/local-container/","text":"Local Container \u00b6 Local container is an interactive mode. In this mode, the traversal of the dag is done on the local computer but the actual execution happens on a container (running on local machine). This mode should enable you to test the pipeline and execution of your code in containers. This mode could also be useful in debugging a container based cloud run. In this mode, max run time is completely ignored. Apart from Buffered Run Log store, all other run log stores are compatible. All secrets and catalog providers are compatible with this mode. Note Magnus does not build the docker image for you but uses a docker image provided. Configuration \u00b6 The full configuration of this mode is: mode : type : local-container config : enable_parallel : docker_image : Enabling parallel \u00b6 By default, none of the branches in parallel or a map node are executed parallelly. You can enable it by setting enable_parallel to 'true' (case-insensitive). Note Please note that 'enable_parallel' needs a string 'true' and not a boolean true. Docker image \u00b6 The docker_image to run the individual nodes of the graph. Requirements The docker image should have magnus installed in it and available as CMD. The docker image should also its working directory as your project root. Please use python3.7 or higher. An example docker image to start with: # Python 3.7 Image without Dependencies FROM python:3.7 LABEL maintainer = <Your Name here> # If you want git versioning ability RUN apt-get update && apt-get install -y --no-install-recommends \\ git \\ && rm -rf /var/lib/apt/lists/* RUN pip install poetry ENV VIRTUAL_ENV = /opt/venv RUN python -m virtualenv --python = /usr/local/bin/python $VIRTUAL_ENV ENV PATH = \" $VIRTUAL_ENV /bin: $PATH \" COPY . /app WORKDIR /app RUN poetry install Node over-rides \u00b6 The docker image provided at mode can be over-ridden by individual nodes of the graph by providing a mode_config as part of the definition. Since mode_config is universally used by all modes, the over-rides should be provided within the context of the executor type. For example: run_log : type : file-system mode : type : local-container config : docker_image : project_default dag : description : Getting started start_at : step1 steps : step1 : type : as-is mode_config : local-container : docker_image : step1_image command : my_function_does_all.func next : step2 step2 : type : as-is next : step3 step3 : type : success step4 : type : fail In the above example, if we assume project_default and step1_image to be 2 different images that satisfy the requirements, step1 would run in step1_image while the remaining nodes would run in project_default image.","title":"Local Container"},{"location":"concepts/modes-implementations/local-container/#local_container","text":"Local container is an interactive mode. In this mode, the traversal of the dag is done on the local computer but the actual execution happens on a container (running on local machine). This mode should enable you to test the pipeline and execution of your code in containers. This mode could also be useful in debugging a container based cloud run. In this mode, max run time is completely ignored. Apart from Buffered Run Log store, all other run log stores are compatible. All secrets and catalog providers are compatible with this mode. Note Magnus does not build the docker image for you but uses a docker image provided.","title":"Local Container"},{"location":"concepts/modes-implementations/local-container/#configuration","text":"The full configuration of this mode is: mode : type : local-container config : enable_parallel : docker_image :","title":"Configuration"},{"location":"concepts/modes-implementations/local/","text":"Local \u00b6 Local mode is an interactive mode. In this mode, magnus does the traversal of the graph and execution of nodes on the local compute from which it is executed. In this set up, we ignore max run time set on the dag completely. All types of secrets, catalog and run log store are compatible with this mode. And this compute mode is default if no mode if provided in the dag definition. Configuration \u00b6 The full configuration of local mode is: mode : type : local config : enable_parallel : Enabling parallel \u00b6 By default, none of the branches in parallel or a map node are executed parallelly. You can enable it by setting enable_parallel to 'true' (case-insensitive). Note Please note that 'enable_parallel' needs a string 'true' and not a boolean true.","title":"Local"},{"location":"concepts/modes-implementations/local/#local","text":"Local mode is an interactive mode. In this mode, magnus does the traversal of the graph and execution of nodes on the local compute from which it is executed. In this set up, we ignore max run time set on the dag completely. All types of secrets, catalog and run log store are compatible with this mode. And this compute mode is default if no mode if provided in the dag definition.","title":"Local"},{"location":"concepts/modes-implementations/local/#configuration","text":"The full configuration of local mode is: mode : type : local config : enable_parallel :","title":"Configuration"},{"location":"concepts/run-log-implementations/bufferred/","text":"Buffered Run Log store \u00b6 This Run Log store does not store the logs any where but in memory during the execution of the pipeline. When to use: During development phase of the project and there is no need to compare outputs between runs. For a quick debug of a run. When not to use: When you need to compare outputs between runs or experiments. Close to production runs or in production unless you do not want to store any run logs. Other than Local compute mode, no other compute modes accept this as a Run Log store. Configuration \u00b6 Buffered Run Log store is the default if nothing was provided in the config. The configuration is minimal and just needs: run_log : type : buffered","title":"Buffered"},{"location":"concepts/run-log-implementations/bufferred/#buffered_run_log_store","text":"This Run Log store does not store the logs any where but in memory during the execution of the pipeline. When to use: During development phase of the project and there is no need to compare outputs between runs. For a quick debug of a run. When not to use: When you need to compare outputs between runs or experiments. Close to production runs or in production unless you do not want to store any run logs. Other than Local compute mode, no other compute modes accept this as a Run Log store.","title":"Buffered Run Log store"},{"location":"concepts/run-log-implementations/bufferred/#configuration","text":"Buffered Run Log store is the default if nothing was provided in the config. The configuration is minimal and just needs: run_log : type : buffered","title":"Configuration"},{"location":"concepts/run-log-implementations/extensions/","text":"To extend and implement a custom run log store, you need to over-ride the appropriate methods of the Base class. Most of the methods of the BaseRunLogStore have default implementations and need not be over-written especially in the case of a single file as a source of run log. Please refer to Guide to extensions for a detailed explanation and the need for implementing a Integration pattern along with the extension. Extensions that are being actively worked on and listed to be released as part of magnus-extensions Database as a run log store: This is an example of a partitioned run log store that is thread safe and can handle parallel executions by the executor. s3 : Using s3 to store a single JSON file as the run log. # You can find this in the source code at: magnus/datastore.py along with a few example # implementations of buffered and file-system class BaseRunLogStore : \"\"\" The base class of a Run Log Store with many common methods implemented. Note: As a general guideline, do not extract anything from the config to set class level attributes. Integration patterns modify the config after init to change behaviors. Access config properties using getters/property of the class. \"\"\" service_name = '' def __init__ ( self , config ): self . config = config or {} def create_run_log ( self , run_id : str , ** kwargs ): \"\"\" Creates a Run Log object by using the config Logically the method should do the following: * Creates a Run log * Adds it to the db * Return the log Raises: NotImplementedError: This is a base class and therefore has no default implementation \"\"\" raise NotImplementedError def get_run_log_by_id ( self , run_id : str , full : bool = True , ** kwargs ) -> RunLog : \"\"\" Retrieves a Run log from the database using the config and the run_id Args: run_id (str): The run_id of the run full (bool): return the full run log store or only the RunLog object Returns: RunLog: The RunLog object identified by the run_id Logically the method should: * Returns the run_log defined by id from the data store defined by the config Raises: NotImplementedError: This is a base class and therefore has no default implementation RunLogNotFoundError: If the run log for run_id is not found in the datastore \"\"\" raise NotImplementedError def put_run_log ( self , run_log : RunLog , ** kwargs ): \"\"\" Puts the Run Log in the database as defined by the config Args: run_log (RunLog): The Run log of the run Logically the method should: Puts the run_log into the database Raises: NotImplementedError: This is a base class and therefore has no default implementation \"\"\" raise NotImplementedError def get_parameters ( self , run_id : str , ** kwargs ) -> dict : # pylint: disable=unused-argument \"\"\" Get the parameters from the Run log defined by the run_id Args: run_id (str): The run_id of the run The method should: * Call get_run_log_by_id(run_id) to retrieve the run_log * Return the parameters as identified in the run_log Returns: dict: A dictionary of the run_log parameters Raises: RunLogNotFoundError: If the run log for run_id is not found in the datastore \"\"\" run_log = self . get_run_log_by_id ( run_id = run_id ) return run_log . parameters def set_parameters ( self , run_id : str , parameters : dict , ** kwargs ): # pylint: disable=unused-argument \"\"\" Update the parameters of the Run log with the new parameters This method would over-write the parameters, if the parameter exists in the run log already The method should: * Call get_run_log_by_id(run_id) to retrieve the run_log * Update the parameters of the run_log * Call put_run_log(run_log) to put the run_log in the datastore Args: run_id (str): The run_id of the run parameters (dict): The parameters to update in the run log Raises: RunLogNotFoundError: If the run log for run_id is not found in the datastore \"\"\" run_log = self . get_run_log_by_id ( run_id = run_id ) run_log . parameters . update ( parameters ) self . put_run_log ( run_log = run_log ) def get_run_config ( self , run_id : str , ** kwargs ) -> dict : # pylint: disable=unused-argument \"\"\" Given a run_id, return the run_config used to perform the run. Args: run_id (str): The run_id of the run Returns: dict: The run config used for the run \"\"\" run_log = self . get_run_log_by_id ( run_id = run_id ) return run_log . run_config def set_run_config ( self , run_id : str , run_config : dict , ** kwargs ): # pylint: disable=unused-argument \"\"\" Set the run config used to run the run_id Args: run_id (str): The run_id of the run run_config (dict): The run_config of the run \"\"\" run_log = self . get_run_log_by_id ( run_id = run_id ) run_log . run_config . update ( run_config ) self . put_run_log ( run_log = run_log ) def create_step_log ( self , name : str , internal_name : str , ** kwargs ): # pylint: disable=unused-argument \"\"\" Create a step log by the name and internal name The method does not update the Run Log with the step log at this point in time. This method is just an interface for external modules to create a step log Args: name (str): The friendly name of the step log internal_name (str): The internal naming of the step log. The internal naming is a dot path convention Returns: StepLog: A uncommitted step log object \"\"\" logger . info ( f ' { self . service_name } Creating a Step Log: { name } ' ) return StepLog ( name = name , internal_name = internal_name , status = defaults . CREATED ) def get_step_log ( self , internal_name : str , run_id : str , ** kwargs ) -> StepLog : # pylint: disable=unused-argument \"\"\" Get a step log from the datastore for run_id and the internal naming of the step log The internal naming of the step log is a dot path convention. The method should: * Call get_run_log_by_id(run_id) to retrieve the run_log * Identify the step location by decoding the internal naming * Return the step log Args: internal_name (str): The internal name of the step log run_id (str): The run_id of the run Returns: StepLog: The step log object for the step defined by the internal naming and run_id Raises: RunLogNotFoundError: If the run log for run_id is not found in the datastore StepLogNotFoundError: If the step log for internal_name is not found in the datastore for run_id \"\"\" logger . info ( f ' { self . service_name } Getting the step log: { internal_name } of { run_id } ' ) run_log = self . get_run_log_by_id ( run_id = run_id ) step_log , _ = run_log . search_step_by_internal_name ( internal_name ) return step_log def add_step_log ( self , step_log : StepLog , run_id : str , ** kwargs ): # pylint: disable=unused-argument \"\"\" Add the step log in the run log as identified by the run_id in the datastore The method should: * Call get_run_log_by_id(run_id) to retrieve the run_log * Identify the branch to add the step by decoding the step_logs internal name * Add the step log to the identified branch log * Call put_run_log(run_log) to put the run_log in the datastore Args: step_log (StepLog): The Step log to add to the database run_id (str): The run id of the run Raises: RunLogNotFoundError: If the run log for run_id is not found in the datastore BranchLogNotFoundError: If the branch of the step log for internal_name is not found in the datastore for run_id \"\"\" logger . info ( f ' { self . service_name } Adding the step log to DB: { step_log . name } ' ) run_log = self . get_run_log_by_id ( run_id = run_id ) branch_to_add = '.' . join ( step_log . internal_name . split ( '.' )[: - 1 ]) branch , _ = run_log . search_branch_by_internal_name ( branch_to_add ) if branch is None : branch = run_log branch . steps [ step_log . internal_name ] = step_log self . put_run_log ( run_log = run_log ) def create_branch_log ( self , internal_branch_name : str , ** kwargs ) -> BranchLog : # pylint: disable=unused-argument \"\"\" Creates a uncommitted branch log object by the internal name given Args: internal_branch_name (str): Creates a branch log by name internal_branch_name Returns: BranchLog: Uncommitted and initialized with defaults BranchLog object \"\"\" # Create a new BranchLog logger . info ( f ' { self . service_name } Creating a Branch Log : { internal_branch_name } ' ) return BranchLog ( internal_name = internal_branch_name , status = defaults . CREATED ) def get_branch_log ( self , internal_branch_name : str , run_id : str , ** kwargs ) -> Union [ BranchLog , RunLog ]: # pylint: disable=unused-argument \"\"\" Returns the branch log by the internal branch name for the run id If the internal branch name is none, returns the run log Args: internal_branch_name (str): The internal branch name to retrieve. run_id (str): The run id of interest Returns: BranchLog: The branch log or the run log as requested. \"\"\" run_log = self . get_run_log_by_id ( run_id = run_id ) if not internal_branch_name : return run_log branch , _ = run_log . search_branch_by_internal_name ( internal_branch_name ) return branch def add_branch_log ( self , branch_log : Union [ BranchLog , RunLog ], run_id : str , ** kwargs ): # pylint: disable=unused-argument \"\"\" The method should: # Get the run log # Get the branch and step containing the branch # Add the branch to the step # Write the run_log The branch log could some times be a Run log and should be handled appropriately Args: branch_log (BranchLog): The branch log/run log to add to the database run_id (str): The run id to which the branch/run log is added \"\"\" internal_branch_name = None if isinstance ( branch_log , BranchLog ): internal_branch_name = branch_log . internal_name if not internal_branch_name : self . put_run_log ( branch_log ) # type: ignore # We are dealing with base dag here return run_log = self . get_run_log_by_id ( run_id = run_id ) step_name = '.' . join ( internal_branch_name . split ( '.' )[: - 1 ]) step , _ = run_log . search_step_by_internal_name ( step_name ) step . branches [ internal_branch_name ] = branch_log # type: ignore self . put_run_log ( run_log ) def create_attempt_log ( self , ** kwargs ) -> StepAttempt : # pylint: disable=unused-argument \"\"\" Returns an uncommitted step attempt log. Returns: StepAttempt: An uncommitted step attempt log \"\"\" logger . info ( f ' { self . service_name } Creating an attempt log' ) return StepAttempt () def create_code_identity ( self , ** kwargs ) -> CodeIdentity : # pylint: disable=unused-argument \"\"\" Creates an uncommitted Code identity class Returns: CodeIdentity: An uncommitted code identity class \"\"\" logger . info ( f ' { self . service_name } Creating Code identity' ) return CodeIdentity () def create_data_catalog ( self , name : str , ** kwargs ) -> DataCatalog : # pylint: disable=unused-argument \"\"\" Create a uncommitted data catalog object Args: name (str): The name of the data catalog item to put Returns: DataCatalog: The DataCatalog object. \"\"\" logger . info ( f ' { self . service_name } Creating Data Catalog for { name } ' ) return DataCatalog ( name = name ) The custom extensions should be registered as part of the namespace: magnus.datastore.BaseRunLogStore for it to be loaded. # For example, as part of your pyproject.toml [tool.poetry.plugins.\"magnus.datastore.BaseRunLogStore\"] \"api\" = \"YOUR_PACKAGE:API\"","title":"Extensions"},{"location":"concepts/run-log-implementations/file-system/","text":"File System Run Log store \u00b6 This Run Log store stores the run logs on the file system as one JSON file. The name of the json file is the run_id of the run. When to use: When you want to compare logs between runs. During testing/debugging in local environments. When not to use: This Run Log store is not compliant when the pipeline has parallel branches and enabled for a parallel runs. The results could be inconsistent and not reliable. Only Local and Local Container compute modes accept this as a Run Log Store. Configuration \u00b6 The configuration is as follows: run_log : type : file-system config : log_folder : log_folder \u00b6 The location of the folder where you want to write the run logs. Defaults to .run_log_store","title":"File System"},{"location":"concepts/run-log-implementations/file-system/#file_system_run_log_store","text":"This Run Log store stores the run logs on the file system as one JSON file. The name of the json file is the run_id of the run. When to use: When you want to compare logs between runs. During testing/debugging in local environments. When not to use: This Run Log store is not compliant when the pipeline has parallel branches and enabled for a parallel runs. The results could be inconsistent and not reliable. Only Local and Local Container compute modes accept this as a Run Log Store.","title":"File System Run Log store"},{"location":"concepts/run-log-implementations/file-system/#configuration","text":"The configuration is as follows: run_log : type : file-system config : log_folder :","title":"Configuration"},{"location":"concepts/secrets-implementations/dot-env/","text":"Dot Env \u00b6 This secrets provider uses a file to store secrets. The naming convention for such file is the .env file. Note This secrets provider should only be used for local modes and for development purpose only. Please be sure on NOT committing these files to your git and if possible, add them to .gitignore. The complete configuration secrets : type : dotenv config : location : dag : ... location \u00b6 The location of the file from which the secrets should be loaded. Defaults to .env file in the project root directory. Format \u00b6 The format of contents of the secrets file should be secret_name = secret_value#Any comment that you want to pass Any content after # is considered a comment and ignored. A exception would be raised if the secret naming does not follow these standards.","title":"Dot Env"},{"location":"concepts/secrets-implementations/dot-env/#dot_env","text":"This secrets provider uses a file to store secrets. The naming convention for such file is the .env file. Note This secrets provider should only be used for local modes and for development purpose only. Please be sure on NOT committing these files to your git and if possible, add them to .gitignore. The complete configuration secrets : type : dotenv config : location : dag : ...","title":"Dot Env"},{"location":"concepts/secrets-implementations/dot-env/#location","text":"The location of the file from which the secrets should be loaded. Defaults to .env file in the project root directory.","title":"location"},{"location":"concepts/secrets-implementations/dot-env/#format","text":"The format of contents of the secrets file should be secret_name = secret_value#Any comment that you want to pass Any content after # is considered a comment and ignored. A exception would be raised if the secret naming does not follow these standards.","title":"Format"},{"location":"concepts/secrets-implementations/extensions/","text":"To extend and implement a custom secrets handler, you need to over-ride the appropriate methods of the Base class. Please refer to Guide to extensions for a detailed explanation and the need for implementing a Integration pattern along with the extension. Extensions that are being actively worked on and listed to be released as part of magnus-extensions aws-secrets-manager : Using aws-secrets-manager as secret store. # You can find this in the source code at: magnus/secrets.py along with a few example # implementations of do-nothing and dotenv class BaseSecrets : \"\"\" A base class for Secrets Handler. All implementations should extend this class. Note: As a general guideline, do not extract anything from the config to set class level attributes. Integration patterns modify the config after init to change behaviors. Access config properties using getters/property of the class. Raises: NotImplementedError: Base class and not implemented \"\"\" service_name = '' def __init__ ( self , config : dict , ** kwargs ): # pylint: disable=unused-argument self . config = config or {} def get ( self , name : str = None , ** kwargs ) -> Union [ str , dict ]: \"\"\" Return the secret by name. If no name is give, return all the secrets. Args: name (str): The name of the secret to return. Raises: NotImplementedError: Base class and hence not implemented. \"\"\" raise NotImplementedError The custom extensions should be registered as part of the namespace: magnus.secrets.BaseSecrets for it to be loaded. # For example, as part of your pyproject.toml [tool.poetry.plugins.\"magnus.secrets.BaseSecrets\"] \"vault\" = \"YOUR_PACKAGE:Vault\"","title":"Extensions"},{"location":"extensions/extensions/","text":"Guide to Extensions \u00b6 The idea behind magnus, in simple terms, is to decouple what should be done to how it is implemented. So while dag only defines the what part of the equation while different compute modes (along with services) define how to make it happen. All the services (compute modes, run log store, secrets, catalog) are written to align to the principle. All the interactions with the services only happen via defined API's that all implementations of the service should implement. The Base class of all the services are given the most general implementations to make extensions as easy as possible. The quadrant of possibilities \u00b6 Any dag execution has two distinct phases Traversal of the dag: In this phase, we are only interested in traversal rules of the dag. Execution of the node: In this phase, we are only interested in executing a specific node. We can characterize a pipeline execution engine by asking two questions Who is responsible for the dag traversal? Is the compute environment same as traversal environment? Taking the example of AWS step functions, AWS Step function is responsible for traversal and the compute environment is not the same as traversal environment as most of the states relate to some kind of compute provided by AWS. The state machine or workflow engine keeps track of the jobs in the compute environment or has some event based mechanism to trigger the traversal after the node finishes execution. Asking the same questions in the context of magnus, gives us 4 possible choices. Magnus traverses, execution environment same as traversal. Magnus traverses, execution environment not same as traversal. Magnus does not traverse, execution environment not same as traversal. Magnus does not traverse, execution environment same as traversal. Magnus is designed to handle all the above 4 choices which makes the decoupling possible. Magnus traverses, execution environment same as traversal. \u00b6 There is only one possible way this can happen with magnus, i.e with local compute mode. Since both the traversal and execution are the same, there is no change in configuration of services for execution and traversal. Magnus traverses, the execution environment != traversal \u00b6 In this mode, magnus is responsible for traversal of the graph but triggers the actual execution to happen in a different environment to the traversal of the graph. For example, local-container mode, magnus is responsible for traversal of the graph but spins up a container with the instruction to execute the node. Since the traversal and execution environments are different, the configuration of services have to modified for execution and traversal. This is implemented by using an Integration pattern that can be provided to control the configuration during both phases. Nearly all the other dag execution engines fall in this space. For example, AWS step functions or argo have a central server that traverses the graph but the execution of the nodes happen in some containers or compute of the AWS. We call this as centralized executor . Interestingly, in magnus there are two ways to handle this scenario: Just like AWS Step functions or argo workflows, we can have a centralized executor which triggers the execution of nodes in the environment that the user wants. For example, local-container . Since the dag definition is part of the source code, every node of the graph is fully aware of the whole graph. This enables some compute modes to let the execution environment decide the next job to trigger based on the status of the execution of the current node. We call this as decentralized executor . Detailed use case: We have internally tested an magnus-extension , that Traverses the graph and triggers an AWS Batch job for the first node from the local computer. The AWS Batch job role is given enough privileges to trigger another AWS Batch job from within the batch job. After the execution of the first node in AWS Batch, read the dag definition to find the next node to trigger and sets up the AWS batch job accordingly. The graph traversal ends when one of success nodes or fail nodes have reached. The compute extension, local-aws-batch is planned to be released along with other magnus-extensions . In our opinion, decentralized executors are ideal for experimentation phase as there could as many dag definitions as needed by the team without blocking one another or causing merge conflicts. Since the compute can also be off-loaded to compute providers, it does not block their local computers. Magnus does not traverse, execution environment not same as traversal. \u00b6 In this mode, magnus does not traverse the graph but translates the dag definition to something that the executor of user's choice. For example, demo-renderer mode available as part of the magnus-core package translates a dag definition to a bash script, although technically the execution environment is same as the traversal in this specific example. Since the traversal and execution environments are different, the configuration of services have to modified for execution and traversal. This is implemented by using an Integration pattern that can be provided to control the configuration during both phases. The actual execution of the step is still wrapped around by magnus, like in demo renderer . The design process behind this is abstract the infrastructure or engineering processes behind production grade deployments from the data science teams. This abstraction also lets the engineering teams continuously improve/test different deployment patterns without disturbing the data science team. The compute extension, aws-step-functions is planned to be released along with other magnus-extensions . Magnus does not traverse, execution environment same as traversal \u00b6 In this mode, the dag definition is translated into something that the executor of user's choice and we use the as-is node to inject scripts that are beyond the control of magnus. An example of this behavior is shown here , where the render_string of as-is is used to inject scripts. The design process is to provide the best possible chance for the dag definition to remain the same independent upon the mode of execution. Submitting Community Extensions \u00b6 We absolutely love community extensions to magnus and would also provide support in cases of complex extensions. For all the extensions, you should also provide integration pattern between some of the magnus core compute patterns. As of this writing, we consider local , local-container and demo-renderer as core compute patterns and we would be adding more to the list as we get more mature.","title":"Guide to Extensions"},{"location":"extensions/extensions/#guide_to_extensions","text":"The idea behind magnus, in simple terms, is to decouple what should be done to how it is implemented. So while dag only defines the what part of the equation while different compute modes (along with services) define how to make it happen. All the services (compute modes, run log store, secrets, catalog) are written to align to the principle. All the interactions with the services only happen via defined API's that all implementations of the service should implement. The Base class of all the services are given the most general implementations to make extensions as easy as possible.","title":"Guide to Extensions"},{"location":"extensions/extensions/#the_quadrant_of_possibilities","text":"Any dag execution has two distinct phases Traversal of the dag: In this phase, we are only interested in traversal rules of the dag. Execution of the node: In this phase, we are only interested in executing a specific node. We can characterize a pipeline execution engine by asking two questions Who is responsible for the dag traversal? Is the compute environment same as traversal environment? Taking the example of AWS step functions, AWS Step function is responsible for traversal and the compute environment is not the same as traversal environment as most of the states relate to some kind of compute provided by AWS. The state machine or workflow engine keeps track of the jobs in the compute environment or has some event based mechanism to trigger the traversal after the node finishes execution. Asking the same questions in the context of magnus, gives us 4 possible choices. Magnus traverses, execution environment same as traversal. Magnus traverses, execution environment not same as traversal. Magnus does not traverse, execution environment not same as traversal. Magnus does not traverse, execution environment same as traversal. Magnus is designed to handle all the above 4 choices which makes the decoupling possible.","title":"The quadrant of possibilities"},{"location":"extensions/extensions/#submitting_community_extensions","text":"We absolutely love community extensions to magnus and would also provide support in cases of complex extensions. For all the extensions, you should also provide integration pattern between some of the magnus core compute patterns. As of this writing, we consider local , local-container and demo-renderer as core compute patterns and we would be adding more to the list as we get more mature.","title":"Submitting Community Extensions"},{"location":"getting_started/brief-concepts-input/","text":"Closer look at input \u00b6 dag : description : Getting started start_at : step parameters steps : step parameters : type : task command_type : python-lambda command : \"lambda x: {'x': int(x) + 1}\" next : step shell step shell : type : task command_type : shell command : mkdir data ; env >> data/data.txt # For Linux/macOS next : success catalog : put : - \"*\" success : type : success fail : type : fail dag \u00b6 A directed acyclic graph (dag) is the definition of the work you want to perform. It defines a series of nodes and the rules of traversal between them. Traversal of the dag \u00b6 In magnus, the order of steps in the dag definition is not important. The traversal is as follows: We start at start_at of the dag, which is \"step parameters\". If \"step parameters\" successfully completed we move to next of \"step parameters\", which is \"step shell\". if \"step parameters\" failed, we move to the failure node of the dag (fail). The step definition can over-ride this. We stop traversing once we reach one of success or fail nodes. All dag definitions should have a success node and fail node. A step/node in the dag defines the next node to visit in the next section of the definition. A step/node can also define the next node to visit on failure of the node, if one is not provided we default to the fail node of the dag. You can also provide the maximum run time for a step or the entire dag in the definition. More information of all the features is available here . Step/Node \u00b6 A Step/Node defines a single logical unit of work in the dag. In the example, we use three different type of nodes: task \u00b6 Is some callable/executable code. Python functions are default and fully supported tasks . As shown in the example, you can also use python lambda expressions with task type of python-lambda. Or shell with a caveat that any interactions with magnus or secret management within magnus is not possible. success \u00b6 A node that marks the graph/sub-graph as success. fail \u00b6 A node that marks the graph/sub-graph as fail. You can define more complex node types (parallel, embedded dag, map) too . Parameters \u00b6 Changed in v0.2 Initial parameters to the pipeline could be sent by sending in a parameters file during execution. The lambda expression, lambda x: {'x': int(x) + 1} , then can use the parameter and update it (in this case, x = x + 1 = 4) by returning a dictionary. The parameter space is updated with the key-value pair. Parameters can be passed to python functions using a similar fashion. Shell executions have access to the parameters too with key being prefixed by MAGNUS_PRM_. Any JSON serializable key-value pairs can be used. You can confirm this by searching for MAGNUS_PRM_ in data/data.txt . For larger content/files, please use the data catalog functionality. Note All parameter keys are case insensitive and the case is changed to lower to support Windows. Please read more information here . Catalog \u00b6 Catalog is a way to pass data files across nodes and also serves as a way to track data used/generated as part of the execution. In the following instruction: step shell : type : task command_type : shell command : mkdir data ; env >> data/data.txt # For Linux/macOS #command: mkdir data next : success catalog : put : - \"*\" we are instructing magnus to create a data folder and echo the environmental variables into data.txt in the command section while asking magnus to put the files the catalog after execution. Logically, you can instruct magnus to: get files from the catalog before the execution to a specific compute data folder execute the command put the files from the compute data folder to the catalog. By default, magnus would look into data folder but you can over-ride this by providing compute_folder in the config. Glob patterns for file searching are allowed. Please read more about the catalog here .","title":"Explanation - input"},{"location":"getting_started/brief-concepts-input/#closer_look_at_input","text":"dag : description : Getting started start_at : step parameters steps : step parameters : type : task command_type : python-lambda command : \"lambda x: {'x': int(x) + 1}\" next : step shell step shell : type : task command_type : shell command : mkdir data ; env >> data/data.txt # For Linux/macOS next : success catalog : put : - \"*\" success : type : success fail : type : fail","title":"Closer look at input"},{"location":"getting_started/brief-concepts-input/#dag","text":"A directed acyclic graph (dag) is the definition of the work you want to perform. It defines a series of nodes and the rules of traversal between them.","title":"dag"},{"location":"getting_started/brief-concepts-input/#traversal_of_the_dag","text":"In magnus, the order of steps in the dag definition is not important. The traversal is as follows: We start at start_at of the dag, which is \"step parameters\". If \"step parameters\" successfully completed we move to next of \"step parameters\", which is \"step shell\". if \"step parameters\" failed, we move to the failure node of the dag (fail). The step definition can over-ride this. We stop traversing once we reach one of success or fail nodes. All dag definitions should have a success node and fail node. A step/node in the dag defines the next node to visit in the next section of the definition. A step/node can also define the next node to visit on failure of the node, if one is not provided we default to the fail node of the dag. You can also provide the maximum run time for a step or the entire dag in the definition. More information of all the features is available here .","title":"Traversal of the dag"},{"location":"getting_started/brief-concepts-input/#stepnode","text":"A Step/Node defines a single logical unit of work in the dag. In the example, we use three different type of nodes:","title":"Step/Node"},{"location":"getting_started/brief-concepts-input/#parameters","text":"Changed in v0.2 Initial parameters to the pipeline could be sent by sending in a parameters file during execution. The lambda expression, lambda x: {'x': int(x) + 1} , then can use the parameter and update it (in this case, x = x + 1 = 4) by returning a dictionary. The parameter space is updated with the key-value pair. Parameters can be passed to python functions using a similar fashion. Shell executions have access to the parameters too with key being prefixed by MAGNUS_PRM_. Any JSON serializable key-value pairs can be used. You can confirm this by searching for MAGNUS_PRM_ in data/data.txt . For larger content/files, please use the data catalog functionality. Note All parameter keys are case insensitive and the case is changed to lower to support Windows. Please read more information here .","title":"Parameters"},{"location":"getting_started/brief-concepts-input/#catalog","text":"Catalog is a way to pass data files across nodes and also serves as a way to track data used/generated as part of the execution. In the following instruction: step shell : type : task command_type : shell command : mkdir data ; env >> data/data.txt # For Linux/macOS #command: mkdir data next : success catalog : put : - \"*\" we are instructing magnus to create a data folder and echo the environmental variables into data.txt in the command section while asking magnus to put the files the catalog after execution. Logically, you can instruct magnus to: get files from the catalog before the execution to a specific compute data folder execute the command put the files from the compute data folder to the catalog. By default, magnus would look into data folder but you can over-ride this by providing compute_folder in the config. Glob patterns for file searching are allowed. Please read more about the catalog here .","title":"Catalog"},{"location":"getting_started/brief-concepts-output/","text":"Closer look at output \u00b6 While the dag defines the work that has to be done, it is only a piece of the whole puzzle. As clearly explained in this paper by Sculley et al. , the actual machine learning/data science related code is only fraction of all the systems that have to be in place to make it work. We implemented magnus with a clear understanding of the complexity while keeping the interface to the data scientists/ML researchers as simple as possible. Though the example pipeline we just ran did nothing useful, it helps in understanding the different systems in place. { \"run_id\" : \"20220118114608\" , \"dag_hash\" : \"ce0676d63e99c34848484f2df1744bab8d45e33a\" , \"use_cached\" : false , \"tag\" : null , \"original_run_id\" : \"\" , \"status\" : \"SUCCESS\" , \"steps\" :{ ... }, \"parameters\" : { \"x\" : 4 }, \"run_config\" : { \"executor\" : { \"type\" : \"local\" , \"config\" : {} }, \"run_log_store\" : { \"type\" : \"buffered\" , \"config\" : {} }, \"catalog\" : { \"type\" : \"file-system\" , \"config\" : {} }, \"secrets\" : { \"type\" : \"do-nothing\" , \"config\" : {} } } } } Run id \u00b6 Every run of magnus has a unique identifier called run_id. Magnus by default creates one based on timestamp but you can provide one at run time for better control. magnus execute --file getting-started.yaml --run-id my_first --x 3 Reproducibility \u00b6 All code breaks at some point and being able to replicate the exact cause of error is essential for a quick resolution. Magnus tracks four possible sources of changes that could have led to a different outcome of an experiment. dag: The dag_hash in the log is the SHA id of the actual dag. code: If the code is git versioned, magnus tracks the code commit id and modified files as part of the logs. If the run is containerized, magnus also tracks the docker image digest as part of the log. data: Any data generated as part of the nodes can be cataloged along with the SHA identity of the file. config: The run config used to make the run is also stored as part of the run logs. The run log structure of the output is exactly the same independent of where the actual run happens. This should enable to replicate a run that happened in an K8 environment, for example, in your local computer to debug. Step Log \u00b6 Every step of the dag, has a corresponding block in the run log. The name of the step is name of key in steps . Here is the step log for step shell of the example run \"steps\" : { ... , \"step shell\" : { \"name\" : \"step shell\" , \"internal_name\" : \"step shell\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"task\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"c5d2f4aa8dd354740d1b2f94b6ee5c904da5e63c\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"<INTENTIONALLY REMOVED>\" , \"code_identifier_message\" : \"<INTENTIONALLY REMOVED>\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-18 11:46:08.576522\" , \"end_time\" : \"2022-01-18 11:46:08.588158\" , \"duration\" : \"0:00:00.011636\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [ { \"name\" : \"data.txt\" , \"data_hash\" : \"8f25ba24e56f182c5125b9ede73cab6c16bf193e3ad36b75ba5145ff1b5db583\" , \"catalog_relative_path\" : \"20220118114608/data.txt\" , \"catalog_handler_location\" : \".catalog\" , \"stage\" : \"put\" } ] }, ... } Attempts \u00b6 As part of the attempt, we capture the start time, end time and the duration of the execution. Only task, success, fail and as-is nodes have this block as it refers to the actual compute time used. In case of failure, magnus tries to capture the exception message in the message block. Code identity \u00b6 The git SHA id of the code commit is captured, if the code is versioned using git. If the current branch was unclean, magnus will warn the user about the dependability of the code id and lists the files that are different from the commit. If the execution was in a container, magnus also adds the docker image digest as a code identity along with git sha id. Data catalog \u00b6 Step shell of the example run creates a file data.txt as part of the run in the data folder. As per the configuration of the pipeline, we have instructed magnus to store all (*) contents of the data folder for downstream steps using the catalog. The data catalog section of the step log captures the hash of the data and the metadata related to it. You can read more about catalog here .","title":"Explanation - output"},{"location":"getting_started/brief-concepts-output/#closer_look_at_output","text":"While the dag defines the work that has to be done, it is only a piece of the whole puzzle. As clearly explained in this paper by Sculley et al. , the actual machine learning/data science related code is only fraction of all the systems that have to be in place to make it work. We implemented magnus with a clear understanding of the complexity while keeping the interface to the data scientists/ML researchers as simple as possible. Though the example pipeline we just ran did nothing useful, it helps in understanding the different systems in place. { \"run_id\" : \"20220118114608\" , \"dag_hash\" : \"ce0676d63e99c34848484f2df1744bab8d45e33a\" , \"use_cached\" : false , \"tag\" : null , \"original_run_id\" : \"\" , \"status\" : \"SUCCESS\" , \"steps\" :{ ... }, \"parameters\" : { \"x\" : 4 }, \"run_config\" : { \"executor\" : { \"type\" : \"local\" , \"config\" : {} }, \"run_log_store\" : { \"type\" : \"buffered\" , \"config\" : {} }, \"catalog\" : { \"type\" : \"file-system\" , \"config\" : {} }, \"secrets\" : { \"type\" : \"do-nothing\" , \"config\" : {} } } } }","title":"Closer look at output"},{"location":"getting_started/brief-concepts-output/#run_id","text":"Every run of magnus has a unique identifier called run_id. Magnus by default creates one based on timestamp but you can provide one at run time for better control. magnus execute --file getting-started.yaml --run-id my_first --x 3","title":"Run id"},{"location":"getting_started/brief-concepts-output/#reproducibility","text":"All code breaks at some point and being able to replicate the exact cause of error is essential for a quick resolution. Magnus tracks four possible sources of changes that could have led to a different outcome of an experiment. dag: The dag_hash in the log is the SHA id of the actual dag. code: If the code is git versioned, magnus tracks the code commit id and modified files as part of the logs. If the run is containerized, magnus also tracks the docker image digest as part of the log. data: Any data generated as part of the nodes can be cataloged along with the SHA identity of the file. config: The run config used to make the run is also stored as part of the run logs. The run log structure of the output is exactly the same independent of where the actual run happens. This should enable to replicate a run that happened in an K8 environment, for example, in your local computer to debug.","title":"Reproducibility"},{"location":"getting_started/brief-concepts-output/#step_log","text":"Every step of the dag, has a corresponding block in the run log. The name of the step is name of key in steps . Here is the step log for step shell of the example run \"steps\" : { ... , \"step shell\" : { \"name\" : \"step shell\" , \"internal_name\" : \"step shell\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"task\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"c5d2f4aa8dd354740d1b2f94b6ee5c904da5e63c\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"<INTENTIONALLY REMOVED>\" , \"code_identifier_message\" : \"<INTENTIONALLY REMOVED>\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-18 11:46:08.576522\" , \"end_time\" : \"2022-01-18 11:46:08.588158\" , \"duration\" : \"0:00:00.011636\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [ { \"name\" : \"data.txt\" , \"data_hash\" : \"8f25ba24e56f182c5125b9ede73cab6c16bf193e3ad36b75ba5145ff1b5db583\" , \"catalog_relative_path\" : \"20220118114608/data.txt\" , \"catalog_handler_location\" : \".catalog\" , \"stage\" : \"put\" } ] }, ... }","title":"Step Log"},{"location":"getting_started/brief-concepts-output/#attempts","text":"As part of the attempt, we capture the start time, end time and the duration of the execution. Only task, success, fail and as-is nodes have this block as it refers to the actual compute time used. In case of failure, magnus tries to capture the exception message in the message block.","title":"Attempts"},{"location":"getting_started/brief-concepts-output/#code_identity","text":"The git SHA id of the code commit is captured, if the code is versioned using git. If the current branch was unclean, magnus will warn the user about the dependability of the code id and lists the files that are different from the commit. If the execution was in a container, magnus also adds the docker image digest as a code identity along with git sha id.","title":"Code identity"},{"location":"getting_started/brief-concepts-output/#data_catalog","text":"Step shell of the example run creates a file data.txt as part of the run in the data folder. As per the configuration of the pipeline, we have instructed magnus to store all (*) contents of the data folder for downstream steps using the catalog. The data catalog section of the step log captures the hash of the data and the metadata related to it. You can read more about catalog here .","title":"Data catalog"},{"location":"getting_started/example-deployment/","text":"Example Deployment \u00b6 While the previous two sections were about introducing magnus pipelines and different features, we can use the same trivial example to showcase the features of magnus in deployment patterns. To recap, here is the pipeline that we ran as an example: dag : description : Getting started start_at : step parameters steps : step parameters : type : task command_type : python-lambda command : \"lambda x: {'x': int(x) + 1}\" next : step shell step shell : type : task command_type : shell command : mkdir data ; env >> data/data.txt # For Linux/macOS next : success catalog : put : - \"*\" success : type : success fail : type : fail The pipeline is simple and demonstrates the core concepts of data catalog, dag traversal, passing data between nodes and task types. To demonstrate the strength of magnus, let us try to \"deploy\" the pipeline via a Bash shell script. This demonstration, though trivial, is very similar in process to translating a dag into something that argo or AWS step functions understands. Let us edit the getting-started.yaml file by adding these lines at the top: mode : type : demo-renderer run_log_store : type : file-system dag : description : Getting started start_at : step parameters steps : step parameters : type : task command_type : python-lambda command : \"lambda x: {'x': int(x) + 1}\" next : step shell step shell : type : task command_type : shell command : mkdir data ; env >> data/data.txt next : success catalog : put : - \"*\" success : type : success fail : type : fail Points to note : We have not changed the dag definition at all. We added a config variable at the top which modifies the execution type to \"demo-renderer\". Demo renderer translates the dag definition into a bash script. The buffered run log store that we have so far used in the example is not suitable anymore. File system run log store persists the logs on physical folder and therefore more suitable. There are other ways to change the configurations which are detailed here . Translation \u00b6 Changed in v0.2 We can execute the pipeline, just like we did it previously, by the following command. magnus execute --file getting-started.yaml --parameters-file parameters.yaml This run is different from the previous execution There is no output or run_id generated by magus. This is because the current execution only performs a translation of the dag into a bash script and not actual function calls. You should also notice a file called demo-bash.sh created in the working directory which is a translation of the dag into a bash script. Let us have a closer look at the contents of the demo-bash.sh . magnus execute_single_node $1 step%parameters --file getting-started.yaml exit_code = $? echo $exit_code if [ $exit_code -ne 0 ] ; then $( magnus execute_single_node $1 fail --file getting-started.yaml ) exit 1 fi magnus execute_single_node $1 step%shell --file getting-started.yaml exit_code = $? echo $exit_code if [ $exit_code -ne 0 ] ; then $( magnus execute_single_node $1 fail --file getting-started.yaml ) exit 1 fi magnus execute_single_node $1 success --file getting-started.yaml The shell script does the following Capture the command line arguments passed to the bash script as magnus parameters, i.e prefixed by MAGNUS_PRM_. Execute the first node and capture the exit code. If the exit code is successful, move to next node as defined in the dag. If the exit code is failure, move to the failure node of the dag. This is as per the dag definition. Execution \u00b6 We can execute the pipeline defined in the demo-bash.sh by chmod 755 demo-bash.sh ./demo-bash.sh my_first_bash Points to note run_id, my_first_bash, is no longer optional parameter and should be provided as the first positional parameter. 2). The parameters file was part of the translation step and is provided to the shell script. Since the run log store is file-system , there should be a directory, .run_log_store , created with a single run log in it by the name my_first_bash.json . Click to show the run log { \"run_id\" : \"my_first_bash\" , \"dag_hash\" : \"ce0676d63e99c34848484f2df1744bab8d45e33a\" , \"use_cached\" : false , \"tag\" : null , \"original_run_id\" : \"\" , \"status\" : \"SUCCESS\" , \"steps\" : { \"step parameters\" : { \"name\" : \"step parameters\" , \"internal_name\" : \"step parameters\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"task\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"493ae8c868fea18e50e6b6410f2c2290ab8d6734\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"<INTENTIONALLY REMOVED>\" , \"code_identifier_message\" : \"<INTENTIONALLY REMOVED>\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-19 08:23:46.720498\" , \"end_time\" : \"2022-01-19 08:23:46.720987\" , \"duration\" : \"0:00:00.000489\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] }, \"step shell\" : { \"name\" : \"step shell\" , \"internal_name\" : \"step shell\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"task\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"493ae8c868fea18e50e6b6410f2c2290ab8d6734\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"<INTENTIONALLY REMOVED>\" , \"code_identifier_message\" : \"<INTENTIONALLY REMOVED>\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-19 08:23:47.351849\" , \"end_time\" : \"2022-01-19 08:23:47.377000\" , \"duration\" : \"0:00:00.025151\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [ { \"name\" : \"data.txt\" , \"data_hash\" : \"011ba0c5de6693e544d838f7cd43f41ebe47b7a16053d17f3173f171c90579d6\" , \"catalog_relative_path\" : \"my_first_bash/data.txt\" , \"catalog_handler_location\" : \".catalog\" , \"stage\" : \"put\" } ] }, \"success\" : { \"name\" : \"success\" , \"internal_name\" : \"success\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"success\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"493ae8c868fea18e50e6b6410f2c2290ab8d6734\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"<INTENTIONALLY REMOVED>\" , \"code_identifier_message\" : \"<INTENTIONALLY REMOVED>\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-19 08:23:48.015055\" , \"end_time\" : \"2022-01-19 08:23:48.016062\" , \"duration\" : \"0:00:00.001007\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] } }, \"parameters\" : { \"x\" : 4 }, \"run_config\" : { \"executor\" : { \"type\" : \"demo-renderer\" , \"config\" : {} }, \"run_log_store\" : { \"type\" : \"file-system\" , \"config\" : {} }, \"catalog\" : { \"type\" : \"file-system\" , \"config\" : {} }, \"secrets\" : { \"type\" : \"do-nothing\" , \"config\" : {} } } } While the original run was in one single python process, the run via the bash uses a different python process for each step of the dag. To extrapolate the idea, this is very similar to AWS step function execution or Argo dag execution that every step of the pipeline executes either a AWS compute or a container. Even though the process of execution of the nodes is different, the structure of run log/catalog is exactly identical to local execution. This feature should enable you to debug/re-run a failed run in any other environments in local environments.","title":"Example Deployment"},{"location":"getting_started/example-deployment/#example_deployment","text":"While the previous two sections were about introducing magnus pipelines and different features, we can use the same trivial example to showcase the features of magnus in deployment patterns. To recap, here is the pipeline that we ran as an example: dag : description : Getting started start_at : step parameters steps : step parameters : type : task command_type : python-lambda command : \"lambda x: {'x': int(x) + 1}\" next : step shell step shell : type : task command_type : shell command : mkdir data ; env >> data/data.txt # For Linux/macOS next : success catalog : put : - \"*\" success : type : success fail : type : fail The pipeline is simple and demonstrates the core concepts of data catalog, dag traversal, passing data between nodes and task types. To demonstrate the strength of magnus, let us try to \"deploy\" the pipeline via a Bash shell script. This demonstration, though trivial, is very similar in process to translating a dag into something that argo or AWS step functions understands. Let us edit the getting-started.yaml file by adding these lines at the top: mode : type : demo-renderer run_log_store : type : file-system dag : description : Getting started start_at : step parameters steps : step parameters : type : task command_type : python-lambda command : \"lambda x: {'x': int(x) + 1}\" next : step shell step shell : type : task command_type : shell command : mkdir data ; env >> data/data.txt next : success catalog : put : - \"*\" success : type : success fail : type : fail Points to note : We have not changed the dag definition at all. We added a config variable at the top which modifies the execution type to \"demo-renderer\". Demo renderer translates the dag definition into a bash script. The buffered run log store that we have so far used in the example is not suitable anymore. File system run log store persists the logs on physical folder and therefore more suitable. There are other ways to change the configurations which are detailed here .","title":"Example Deployment"},{"location":"getting_started/example-deployment/#translation","text":"Changed in v0.2 We can execute the pipeline, just like we did it previously, by the following command. magnus execute --file getting-started.yaml --parameters-file parameters.yaml This run is different from the previous execution There is no output or run_id generated by magus. This is because the current execution only performs a translation of the dag into a bash script and not actual function calls. You should also notice a file called demo-bash.sh created in the working directory which is a translation of the dag into a bash script. Let us have a closer look at the contents of the demo-bash.sh . magnus execute_single_node $1 step%parameters --file getting-started.yaml exit_code = $? echo $exit_code if [ $exit_code -ne 0 ] ; then $( magnus execute_single_node $1 fail --file getting-started.yaml ) exit 1 fi magnus execute_single_node $1 step%shell --file getting-started.yaml exit_code = $? echo $exit_code if [ $exit_code -ne 0 ] ; then $( magnus execute_single_node $1 fail --file getting-started.yaml ) exit 1 fi magnus execute_single_node $1 success --file getting-started.yaml The shell script does the following Capture the command line arguments passed to the bash script as magnus parameters, i.e prefixed by MAGNUS_PRM_. Execute the first node and capture the exit code. If the exit code is successful, move to next node as defined in the dag. If the exit code is failure, move to the failure node of the dag. This is as per the dag definition.","title":"Translation"},{"location":"getting_started/example-deployment/#execution","text":"We can execute the pipeline defined in the demo-bash.sh by chmod 755 demo-bash.sh ./demo-bash.sh my_first_bash Points to note run_id, my_first_bash, is no longer optional parameter and should be provided as the first positional parameter. 2). The parameters file was part of the translation step and is provided to the shell script. Since the run log store is file-system , there should be a directory, .run_log_store , created with a single run log in it by the name my_first_bash.json . Click to show the run log { \"run_id\" : \"my_first_bash\" , \"dag_hash\" : \"ce0676d63e99c34848484f2df1744bab8d45e33a\" , \"use_cached\" : false , \"tag\" : null , \"original_run_id\" : \"\" , \"status\" : \"SUCCESS\" , \"steps\" : { \"step parameters\" : { \"name\" : \"step parameters\" , \"internal_name\" : \"step parameters\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"task\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"493ae8c868fea18e50e6b6410f2c2290ab8d6734\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"<INTENTIONALLY REMOVED>\" , \"code_identifier_message\" : \"<INTENTIONALLY REMOVED>\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-19 08:23:46.720498\" , \"end_time\" : \"2022-01-19 08:23:46.720987\" , \"duration\" : \"0:00:00.000489\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] }, \"step shell\" : { \"name\" : \"step shell\" , \"internal_name\" : \"step shell\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"task\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"493ae8c868fea18e50e6b6410f2c2290ab8d6734\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"<INTENTIONALLY REMOVED>\" , \"code_identifier_message\" : \"<INTENTIONALLY REMOVED>\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-19 08:23:47.351849\" , \"end_time\" : \"2022-01-19 08:23:47.377000\" , \"duration\" : \"0:00:00.025151\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [ { \"name\" : \"data.txt\" , \"data_hash\" : \"011ba0c5de6693e544d838f7cd43f41ebe47b7a16053d17f3173f171c90579d6\" , \"catalog_relative_path\" : \"my_first_bash/data.txt\" , \"catalog_handler_location\" : \".catalog\" , \"stage\" : \"put\" } ] }, \"success\" : { \"name\" : \"success\" , \"internal_name\" : \"success\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"success\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"493ae8c868fea18e50e6b6410f2c2290ab8d6734\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"<INTENTIONALLY REMOVED>\" , \"code_identifier_message\" : \"<INTENTIONALLY REMOVED>\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-19 08:23:48.015055\" , \"end_time\" : \"2022-01-19 08:23:48.016062\" , \"duration\" : \"0:00:00.001007\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] } }, \"parameters\" : { \"x\" : 4 }, \"run_config\" : { \"executor\" : { \"type\" : \"demo-renderer\" , \"config\" : {} }, \"run_log_store\" : { \"type\" : \"file-system\" , \"config\" : {} }, \"catalog\" : { \"type\" : \"file-system\" , \"config\" : {} }, \"secrets\" : { \"type\" : \"do-nothing\" , \"config\" : {} } } } While the original run was in one single python process, the run via the bash uses a different python process for each step of the dag. To extrapolate the idea, this is very similar to AWS step function execution or Argo dag execution that every step of the pipeline executes either a AWS compute or a container. Even though the process of execution of the nodes is different, the structure of run log/catalog is exactly identical to local execution. This feature should enable you to debug/re-run a failed run in any other environments in local environments.","title":"Execution"},{"location":"getting_started/example/","text":"Example Run \u00b6 Note It is assumed that you have gone through installation and magnus command line works. To give you a flavour of how magnus works, lets create a simple pipeline. Copy the contents of this yaml into getting-started.yaml. Note The below execution would create a folder called 'data' in the current working directory. The command as given should work in linux/macOS but for windows, please change accordingly. dag : description : Getting started start_at : step parameters steps : step parameters : type : task command_type : python-lambda command : \"lambda x: {'x': int(x) + 1}\" next : step shell step shell : type : task command_type : shell command : mkdir data ; env >> data/data.txt # For Linux/macOS next : success catalog : put : - \"*\" success : type : success fail : type : fail Since the pipeline expects a parameter x , lets provide that using parameters.yaml x : 3 And let's run the pipeline using: magnus execute --file getting-started.yaml --parameters-file parameters.yaml You should see a list of warnings but your terminal output should look something similar to this: { \"run_id\" : \"20220118114608\" , \"dag_hash\" : \"ce0676d63e99c34848484f2df1744bab8d45e33a\" , \"use_cached\" : false , \"tag\" : null , \"original_run_id\" : \"\" , \"status\" : \"SUCCESS\" , \"steps\" : { \"step parameters\" : { \"name\" : \"step parameters\" , \"internal_name\" : \"step parameters\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"task\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"c5d2f4aa8dd354740d1b2f94b6ee5c904da5e63c\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"<INTENTIONALLY REMOVED>\" , \"code_identifier_message\" : \"<INTENTIONALLY REMOVED>\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-18 11:46:08.530138\" , \"end_time\" : \"2022-01-18 11:46:08.530561\" , \"duration\" : \"0:00:00.000423\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] }, \"step shell\" : { \"name\" : \"step shell\" , \"internal_name\" : \"step shell\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"task\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"c5d2f4aa8dd354740d1b2f94b6ee5c904da5e63c\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"<INTENTIONALLY REMOVED>\" , \"code_identifier_message\" : \"<INTENTIONALLY REMOVED>\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-18 11:46:08.576522\" , \"end_time\" : \"2022-01-18 11:46:08.588158\" , \"duration\" : \"0:00:00.011636\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [ { \"name\" : \"data.txt\" , \"data_hash\" : \"8f25ba24e56f182c5125b9ede73cab6c16bf193e3ad36b75ba5145ff1b5db583\" , \"catalog_relative_path\" : \"20220118114608/data.txt\" , \"catalog_handler_location\" : \".catalog\" , \"stage\" : \"put\" } ] }, \"success\" : { \"name\" : \"success\" , \"internal_name\" : \"success\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"success\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"c5d2f4aa8dd354740d1b2f94b6ee5c904da5e63c\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"<INTENTIONALLY REMOVED>\" , \"code_identifier_message\" : \"<INTENTIONALLY REMOVED>\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-18 11:46:08.639563\" , \"end_time\" : \"2022-01-18 11:46:08.639680\" , \"duration\" : \"0:00:00.000117\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] } }, \"parameters\" : { \"x\" : 4 }, \"run_config\" : { \"executor\" : { \"type\" : \"local\" , \"config\" : {} }, \"run_log_store\" : { \"type\" : \"buffered\" , \"config\" : {} }, \"catalog\" : { \"type\" : \"file-system\" , \"config\" : {} }, \"secrets\" : { \"type\" : \"do-nothing\" , \"config\" : {} } } } You should see that data folder being created with a file called data.txt in it. This is according to the command in step shell . You should also see a folder .catalog being created with a single folder corresponding to the run_id of this run. Let's take a closer look at the input and output in the next sections.","title":"Example Run"},{"location":"getting_started/example/#example_run","text":"Note It is assumed that you have gone through installation and magnus command line works. To give you a flavour of how magnus works, lets create a simple pipeline. Copy the contents of this yaml into getting-started.yaml. Note The below execution would create a folder called 'data' in the current working directory. The command as given should work in linux/macOS but for windows, please change accordingly. dag : description : Getting started start_at : step parameters steps : step parameters : type : task command_type : python-lambda command : \"lambda x: {'x': int(x) + 1}\" next : step shell step shell : type : task command_type : shell command : mkdir data ; env >> data/data.txt # For Linux/macOS next : success catalog : put : - \"*\" success : type : success fail : type : fail Since the pipeline expects a parameter x , lets provide that using parameters.yaml x : 3 And let's run the pipeline using: magnus execute --file getting-started.yaml --parameters-file parameters.yaml You should see a list of warnings but your terminal output should look something similar to this: { \"run_id\" : \"20220118114608\" , \"dag_hash\" : \"ce0676d63e99c34848484f2df1744bab8d45e33a\" , \"use_cached\" : false , \"tag\" : null , \"original_run_id\" : \"\" , \"status\" : \"SUCCESS\" , \"steps\" : { \"step parameters\" : { \"name\" : \"step parameters\" , \"internal_name\" : \"step parameters\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"task\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"c5d2f4aa8dd354740d1b2f94b6ee5c904da5e63c\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"<INTENTIONALLY REMOVED>\" , \"code_identifier_message\" : \"<INTENTIONALLY REMOVED>\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-18 11:46:08.530138\" , \"end_time\" : \"2022-01-18 11:46:08.530561\" , \"duration\" : \"0:00:00.000423\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] }, \"step shell\" : { \"name\" : \"step shell\" , \"internal_name\" : \"step shell\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"task\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"c5d2f4aa8dd354740d1b2f94b6ee5c904da5e63c\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"<INTENTIONALLY REMOVED>\" , \"code_identifier_message\" : \"<INTENTIONALLY REMOVED>\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-18 11:46:08.576522\" , \"end_time\" : \"2022-01-18 11:46:08.588158\" , \"duration\" : \"0:00:00.011636\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [ { \"name\" : \"data.txt\" , \"data_hash\" : \"8f25ba24e56f182c5125b9ede73cab6c16bf193e3ad36b75ba5145ff1b5db583\" , \"catalog_relative_path\" : \"20220118114608/data.txt\" , \"catalog_handler_location\" : \".catalog\" , \"stage\" : \"put\" } ] }, \"success\" : { \"name\" : \"success\" , \"internal_name\" : \"success\" , \"status\" : \"SUCCESS\" , \"step_type\" : \"success\" , \"message\" : \"\" , \"mock\" : false , \"code_identities\" : [ { \"code_identifier\" : \"c5d2f4aa8dd354740d1b2f94b6ee5c904da5e63c\" , \"code_identifier_type\" : \"git\" , \"code_identifier_dependable\" : false , \"code_identifier_url\" : \"<INTENTIONALLY REMOVED>\" , \"code_identifier_message\" : \"<INTENTIONALLY REMOVED>\" } ], \"attempts\" : [ { \"attempt_number\" : 0 , \"start_time\" : \"2022-01-18 11:46:08.639563\" , \"end_time\" : \"2022-01-18 11:46:08.639680\" , \"duration\" : \"0:00:00.000117\" , \"status\" : \"SUCCESS\" , \"message\" : \"\" } ], \"user_defined_metrics\" : {}, \"branches\" : {}, \"data_catalog\" : [] } }, \"parameters\" : { \"x\" : 4 }, \"run_config\" : { \"executor\" : { \"type\" : \"local\" , \"config\" : {} }, \"run_log_store\" : { \"type\" : \"buffered\" , \"config\" : {} }, \"catalog\" : { \"type\" : \"file-system\" , \"config\" : {} }, \"secrets\" : { \"type\" : \"do-nothing\" , \"config\" : {} } } } You should see that data folder being created with a file called data.txt in it. This is according to the command in step shell . You should also see a folder .catalog being created with a single folder corresponding to the run_id of this run. Let's take a closer look at the input and output in the next sections.","title":"Example Run"},{"location":"getting_started/installation/","text":"Installation \u00b6 pip \u00b6 magnus is a python package and should be installed as any other. pip install magnus We recommend that you install magnus in a virtual environment specific to the project and also poetry for your application development. The command to install in a poetry managed virtual environment poetry add magnus","title":"Installation"},{"location":"getting_started/installation/#installation","text":"","title":"Installation"},{"location":"getting_started/installation/#pip","text":"magnus is a python package and should be installed as any other. pip install magnus We recommend that you install magnus in a virtual environment specific to the project and also poetry for your application development. The command to install in a poetry managed virtual environment poetry add magnus","title":"pip"},{"location":"getting_started/why-magnus/","text":"Why Magnus \u00b6 Magnus is never set out to replace production grade orchestrators like AWS Step functions or argo. These orchestrators are proven to be robust and are constantly improved to align to best practices. We agree that, we should always use these tools for production grade deployments. But the same tools, seem to over-engineered and extremely complex for experiments and local development where the actual data science teams thrive. The farther the operational world is from the developers, the longer it takes to operationalize projects - lesson learnt from DevOps. Magnus was developed to bring the data science team closer to the production infrastructure and practices while abstracting a lof of underlying complexity. Magnus treats the dag definition as a contract between the data science team and the engineering team. While the dag could be run on local computers or in cloud by the data science team during the development/experiment phase, the dag is translated to chosen orchestrators language during deployment by the engineering team. This also enables the data science team to think along the lines of pipelines and orchestration without infrastructure complexities. We also found that, a few implementations in magnus to be more convenient than the counterparts it tries to emulate. For example: passing variables between steps in AWS Step function is complex and not even possible when using containers as one of the steps. The same step when wrapped around magnus before step function makes it easier. Here are some of the key points on choosing magnus. Reproducibility of Experiments \u00b6 Data science experiments and projects are notorious for being difficult to replicate. In our opinion, a data science experiment is code + data + configuration. Magnus tracks all three of them in the run logs and makes it easier to reproduce/highlight differences between experiments. If the default tracking provided by Magnus is not suitable, you can easily integrate your application with one of your liking or extend magnus to fit your needs. Easy re-run \u00b6 Along the same lines as reproducibility, a pipeline run with magnus can be re-run on any other environment as long as the run log/catalog are available. Magnus would skip the steps that were successfully executed in the older run and start execution from the point of failure. Extensibility \u00b6 A lot of design principles while writing magnus was to promote extensibility . Its easy to write extensions to include new compute environments (k8s, on-prem clusters etc) run log store (databases, file systems etc) data cataloging (feature stores, object storage etc) secret managers (vault, azure secrets) Near Zero code change from local to production \u00b6 Magnus was designed to make data science teams closer to operational world. The code and orchestration are ready to be productionized as soon as you are ready. The only change to enable that would be a config . Easy switch \u00b6 The technological decisions made today for your project may not be correct one in a few months for a lot of varied reasons. You might want to change your cloud provider or orchestrating tools or secrets manager and it should be easy to do so. With magnus, you can easily switch without touching your project code/practices. Since the configuration could also be parameterized, switching might be as simple as changing one file. For more details check here. And one of the design principles in magnus was to limit needing to import magnus to achieve functionality. This also means that you can move away from magnus if its no longer supporting you. :-)","title":"Why magnus"},{"location":"getting_started/why-magnus/#why_magnus","text":"Magnus is never set out to replace production grade orchestrators like AWS Step functions or argo. These orchestrators are proven to be robust and are constantly improved to align to best practices. We agree that, we should always use these tools for production grade deployments. But the same tools, seem to over-engineered and extremely complex for experiments and local development where the actual data science teams thrive. The farther the operational world is from the developers, the longer it takes to operationalize projects - lesson learnt from DevOps. Magnus was developed to bring the data science team closer to the production infrastructure and practices while abstracting a lof of underlying complexity. Magnus treats the dag definition as a contract between the data science team and the engineering team. While the dag could be run on local computers or in cloud by the data science team during the development/experiment phase, the dag is translated to chosen orchestrators language during deployment by the engineering team. This also enables the data science team to think along the lines of pipelines and orchestration without infrastructure complexities. We also found that, a few implementations in magnus to be more convenient than the counterparts it tries to emulate. For example: passing variables between steps in AWS Step function is complex and not even possible when using containers as one of the steps. The same step when wrapped around magnus before step function makes it easier. Here are some of the key points on choosing magnus.","title":"Why Magnus"},{"location":"getting_started/why-magnus/#reproducibility_of_experiments","text":"Data science experiments and projects are notorious for being difficult to replicate. In our opinion, a data science experiment is code + data + configuration. Magnus tracks all three of them in the run logs and makes it easier to reproduce/highlight differences between experiments. If the default tracking provided by Magnus is not suitable, you can easily integrate your application with one of your liking or extend magnus to fit your needs.","title":"Reproducibility of Experiments"},{"location":"getting_started/why-magnus/#easy_re-run","text":"Along the same lines as reproducibility, a pipeline run with magnus can be re-run on any other environment as long as the run log/catalog are available. Magnus would skip the steps that were successfully executed in the older run and start execution from the point of failure.","title":"Easy re-run"},{"location":"getting_started/why-magnus/#extensibility","text":"A lot of design principles while writing magnus was to promote extensibility . Its easy to write extensions to include new compute environments (k8s, on-prem clusters etc) run log store (databases, file systems etc) data cataloging (feature stores, object storage etc) secret managers (vault, azure secrets)","title":"Extensibility"},{"location":"getting_started/why-magnus/#near_zero_code_change_from_local_to_production","text":"Magnus was designed to make data science teams closer to operational world. The code and orchestration are ready to be productionized as soon as you are ready. The only change to enable that would be a config .","title":"Near Zero code change from local to production"},{"location":"getting_started/why-magnus/#easy_switch","text":"The technological decisions made today for your project may not be correct one in a few months for a lot of varied reasons. You might want to change your cloud provider or orchestrating tools or secrets manager and it should be easy to do so. With magnus, you can easily switch without touching your project code/practices. Since the configuration could also be parameterized, switching might be as simple as changing one file. For more details check here. And one of the design principles in magnus was to limit needing to import magnus to achieve functionality. This also means that you can move away from magnus if its no longer supporting you. :-)","title":"Easy switch"},{"location":"getting_started/wrap-up/","text":"Wrapping up \u00b6 To summarize the journey so-far, We have defined a simple pipeline to show the features of magnus. DAG definition and traversal rules. Passing data between nodes. Basic task types (task, success, fail) and execution environments (shell, python-lambda). Data catalogs. We have executed the pipeline in local environment to demonstrate The run log structure and its relation to the dag steps. The config identities (run_config, dag hash). The code identities (code commits). The data catalog and data identity (data hash). We also \"deployed\" the pipeline as a bash script to demonstrate Translation of the dag definition into language that compute environments understands. proven the identical structure of run log/catalog independent of the environment. proven the only change required to deploy is a config i.e no change in code/dag definition. Design \u00b6 The design thought behind magnus has always been to not disturb the coding/engineering practices of the data teams or the infrastructure teams. We found the right abstraction layer to make the communication between these teams to be the DAG definition i.e The data teams should focus on delivering and proving the correctness of the dag in environments that are friendly to them. These could be local or any other environments that are experiment-friendly. The infrastructure teams should focus on deploying the dag definition in production grade environments as per their team practices or capabilities. While both teams are looking at the same dag definition, their interpretation of it is different and should be decoupled. While the example shown is trivial, the rationale and the process of translating dag definitions is not very far away from real world examples. Testing \u00b6 We also agree with dagster's observation of \"Data applications are notoriously difficult to test and are therefore often un- or under-tested.\" In magnus, python commands are just regular functions that can be unit tested as the data teams chose to. Magnus itself is unit tested with a test coverage closer to 80% and with a lot of scenarios tested where we have noticed failures in the past. Conclusion \u00b6 We hope you got a good introduction to magnus and its features. We did not complicate the pipeline to keep it simple but there are many features that are interesting and might be of use to you in writing a robust pipeline. You can read about them in concepts or see examples . You can even write extensions to magnus to see a feature that we have not implemented.","title":"Wrap up"},{"location":"getting_started/wrap-up/#wrapping_up","text":"To summarize the journey so-far, We have defined a simple pipeline to show the features of magnus. DAG definition and traversal rules. Passing data between nodes. Basic task types (task, success, fail) and execution environments (shell, python-lambda). Data catalogs. We have executed the pipeline in local environment to demonstrate The run log structure and its relation to the dag steps. The config identities (run_config, dag hash). The code identities (code commits). The data catalog and data identity (data hash). We also \"deployed\" the pipeline as a bash script to demonstrate Translation of the dag definition into language that compute environments understands. proven the identical structure of run log/catalog independent of the environment. proven the only change required to deploy is a config i.e no change in code/dag definition.","title":"Wrapping up"},{"location":"getting_started/wrap-up/#design","text":"The design thought behind magnus has always been to not disturb the coding/engineering practices of the data teams or the infrastructure teams. We found the right abstraction layer to make the communication between these teams to be the DAG definition i.e The data teams should focus on delivering and proving the correctness of the dag in environments that are friendly to them. These could be local or any other environments that are experiment-friendly. The infrastructure teams should focus on deploying the dag definition in production grade environments as per their team practices or capabilities. While both teams are looking at the same dag definition, their interpretation of it is different and should be decoupled. While the example shown is trivial, the rationale and the process of translating dag definitions is not very far away from real world examples.","title":"Design"},{"location":"getting_started/wrap-up/#testing","text":"We also agree with dagster's observation of \"Data applications are notoriously difficult to test and are therefore often un- or under-tested.\" In magnus, python commands are just regular functions that can be unit tested as the data teams chose to. Magnus itself is unit tested with a test coverage closer to 80% and with a lot of scenarios tested where we have noticed failures in the past.","title":"Testing"},{"location":"getting_started/wrap-up/#conclusion","text":"We hope you got a good introduction to magnus and its features. We did not complicate the pipeline to keep it simple but there are many features that are interesting and might be of use to you in writing a robust pipeline. You can read about them in concepts or see examples . You can even write extensions to magnus to see a feature that we have not implemented.","title":"Conclusion"}]}